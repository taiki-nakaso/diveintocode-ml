{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint\n",
    "## 機械学習スクラッチ ロジスティック回帰\n",
    "スクラッチでロジスティック回帰を実装した後、学習と検証を行なっていきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ロジスティック回帰のクラスをスクラッチで作成していきます。NumPyなど最低限のライブラリのみを使いアルゴリズムを実装していきます。\n",
    "\n",
    "以下に雛形を用意してあります。このScratchLogisticRegressionクラスにコードを書き加えていってください。推定関係のメソッドは線形回帰と異なり、ラベルを出力するpredictメソッドと、確率を出力するpredict_probaメソッドの2種類を作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "class ScratchLogisticRegression():\n",
    "    \"\"\"\n",
    "    ロジスティック回帰のスクラッチ実装\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_iter : int\n",
    "      イテレーション数\n",
    "    lr : float\n",
    "      学習率\n",
    "    C : int\n",
    "      正則化パラメータ\n",
    "    no_bias : bool\n",
    "      バイアス項を入れない場合はTrue\n",
    "    verbose : bool\n",
    "      学習過程を出力する場合はTrue\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    self.coef_ : 次の形のndarray, shape (n_features,)\n",
    "      パラメータ\n",
    "    self.loss : 次の形のndarray, shape (self.iter,)\n",
    "      訓練データに対する損失の記録\n",
    "    self.val_loss : 次の形のndarray, shape (self.iter,)\n",
    "      検証データに対する損失の記録\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, num_iter, lr, C, no_bias, verbose):\n",
    "        # ハイパーパラメータを属性として記録\n",
    "        self.iter = num_iter\n",
    "        self.lr = lr\n",
    "        self.C = C\n",
    "        self.no_bias = no_bias\n",
    "        self.verbose = verbose\n",
    "        # 損失を記録する配列を用意\n",
    "        self.loss = np.zeros(self.iter)\n",
    "        self.val_loss = np.zeros(self.iter)\n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        ロジスティック回帰を学習する。検証データが入力された場合はそれに対する損失と精度もイテレーションごとに計算する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証データの正解値\n",
    "        \"\"\"\n",
    "        # preparing\n",
    "        ## check bias \n",
    "        X_biased = self._check_bias(X)\n",
    "        ## check val_bias\n",
    "        if X_val is not None:\n",
    "            X_val_biased = self._check_bias(X_val)\n",
    "\n",
    "        ## initial theta = [[0], [0], ..., [0]]\n",
    "        self.coef_ = np.random.randn(X_biased.shape[1], 1)\n",
    "\n",
    "        \n",
    "        if self.verbose:\n",
    "            #verboseをTrueにした際は学習過程を出力\n",
    "            print('start learning with process')\n",
    "        \n",
    "        start_time = time.time()\n",
    "\n",
    "        # loop learning\n",
    "        for i in range(self.iter):\n",
    "            ## calc hypothesis\n",
    "            hypothesis = self._logistic_hypothesis(X_biased)\n",
    "\n",
    "            ## add loss\n",
    "            self.loss[i] = self._get_loss(hypothesis, y)\n",
    "\n",
    "            ## calc error\n",
    "            error = self._get_error(hypothesis, y)\n",
    "\n",
    "            ## gradient descent\n",
    "            self.coef_ = self._gradient_descent(X_biased, error)\n",
    "\n",
    "            ## validation\n",
    "            if (X_val is not None) and (y_val is not None):\n",
    "                ## calc val_hypothesis\n",
    "                hypothesis_val = self._logistic_hypothesis(X_val_biased)\n",
    "\n",
    "                ### add val_loss\n",
    "                self.val_loss[i] = self._get_loss(hypothesis_val, y_val)\n",
    "            \n",
    "            ## output process\n",
    "            if self.verbose:\n",
    "                print(f'{i+1} loss : train: {self.loss[i]}, valid: {self.val_loss[i]}')\n",
    "        \n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "        print(f'Done! elapsed time: {elapsed_time:.5f}s')\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        ロジスティック回帰を使いラベルを推定する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            ロジスティック回帰による推定結果\n",
    "        \"\"\"\n",
    "        # preparing\n",
    "        ## check bias \n",
    "        X_biased = self._check_bias(X)\n",
    "\n",
    "        # predict probability of y\n",
    "        pred_proba =  self._logistic_hypothesis(X_biased)\n",
    "        # round to predict y\n",
    "        pred = (lambda x:(x*2+1)//2)(pred_proba)\n",
    "\n",
    "        return pred\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        ロジスティック回帰を使い確率を推定する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            ロジスティック回帰による推定結果\n",
    "        \"\"\"\n",
    "        # preparing\n",
    "        ## check bias \n",
    "        X_biased = self._check_bias(X)\n",
    "\n",
    "        # predict probability of y\n",
    "        pred_proba = self._logistic_hypothesis(X_biased)\n",
    "        \n",
    "        return pred_proba\n",
    "\n",
    "    def _logistic_hypothesis(self, X):\n",
    "        \"\"\"\n",
    "        ロジスティック回帰の仮定関数を計算する\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データ\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            ロジスティック回帰の仮定関数による推定結果\n",
    "\n",
    "        \"\"\"\n",
    "        linear_h = np.matmul(X, self.coef_)\n",
    "\n",
    "        h = self._sigmoid(linear_h)\n",
    "\n",
    "        return h\n",
    "\n",
    "    def _sigmoid(self, X):\n",
    "        \"\"\"\n",
    "        シグモイド関数の計算結果を返す\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, 1)\n",
    "            線形回帰における仮定関数の計算結果\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            シグモイド関数の計算結果\n",
    "        \"\"\"\n",
    "        exp_term = np.exp(-X)\n",
    "\n",
    "        s = 1 / (1+exp_term)\n",
    "\n",
    "        return s\n",
    "\n",
    "    def _gradient_descent(self, X, error):\n",
    "        \"\"\"\n",
    "        パラメータベクトルの値を更新する\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データ\n",
    "        error : 次の形のndarray, shape (n_samples, 1)\n",
    "            予測値と正解値の差\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        new_coef_ : 次の形のndarray, shape (n_samples, 1)\n",
    "            更新された新たなパラメータベクトル\n",
    "        \"\"\"\n",
    "        # m : number of samples\n",
    "        m = X.shape[0]\n",
    "\n",
    "        # calc\n",
    "        ## Regularization term\n",
    "        reg_term = np.concatenate((np.zeros((1, 1)), ((self.C * self.coef_[1:]) / m)), axis=0)\n",
    "        ## new param\n",
    "        new_coef_ = self.coef_ - (((self.lr * np.matmul(X.T, error)) / m) + reg_term) \n",
    "        \n",
    "        return new_coef_\n",
    "\n",
    "    def _get_error(self, hypothesis, y):\n",
    "        \"\"\"\n",
    "        誤差ベクトルを求める\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        hypothesis : 次の形のndarray, shape (n_samples, 1)\n",
    "            予測値ベクトル\n",
    "        y : 次の形のndarray, shape (n_samples, 1)\n",
    "            パラメータベクトル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        error: 次の形のndarray, shape (n_samples, 1)\n",
    "            誤差ベクトル\n",
    "        \"\"\"\n",
    "        error = hypothesis - y\n",
    "\n",
    "        return error\n",
    "\n",
    "    def _check_bias(self, v):\n",
    "        \"\"\"\n",
    "        no_biasフラグに従い、ベクトルにバイアス項をよしなにする\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        v : 次の形のndarray, shape (*, n_features)\n",
    "            ベクトル\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        v_biased : 次の形のndarray, shape (*, n_features+1)\n",
    "            バイアス項を追加したりしなかったりしたベクトル\n",
    "        \"\"\"\n",
    "        m = v.shape[0]\n",
    "\n",
    "        if not self.no_bias:\n",
    "            v_biased = np.concatenate((np.ones((m, 1)), v), axis=1)\n",
    "        else:\n",
    "            v_biased = v\n",
    "\n",
    "        return v_biased\n",
    "\n",
    "    def _get_loss(self, y_pred, y):\n",
    "        \"\"\"\n",
    "        目的関数（損失関数）の計算\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_pred : 次の形のndarray, shape (n_samples,)\n",
    "            推定した値\n",
    "        y : 次の形のndarray, shape (n_samples,)\n",
    "            正解値\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        loss : numpy.float\n",
    "            目的関数（損失関数）\n",
    "        \"\"\"\n",
    "        # m : number of samples\n",
    "        m = y.shape[0]\n",
    "\n",
    "        # calc\n",
    "        ## y_true = 1\n",
    "        log_positive_sum = np.sum(y * np.log(y_pred))\n",
    "        ## y_true = 0\n",
    "        log_negative_sum = np.sum((1-y) * np.log(y_pred))\n",
    "        ## Regularization term (except theta_0)\n",
    "        reg_term_sum = np.sum(self.coef_[1:]**2)\n",
    "\n",
    "        loss = ((-log_positive_sum-log_positive_sum) / m) + ((self.C*reg_term_sum) / (2*m))\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題1】\n",
    "## 仮定関数\n",
    "ロジスティック回帰の仮定関数のメソッドをScratchLogisticRegressionクラスに実装してください。\n",
    "\n",
    "ロジスティック回帰の仮定関数は、線形回帰の仮定関数を **シグモイド関数** に通したものです。シグモイド関数は以下の式で表されます。\n",
    "\n",
    "$$\n",
    "g(z) = \\frac{1}{1+e^{−z}}\n",
    "$$\n",
    "\n",
    "線形回帰の仮定関数は次の式でした。\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\theta^T \\cdot x\n",
    "$$\n",
    "\n",
    "まとめて書くと、ロジスティック回帰の仮定関数は次のようになります。\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\frac{1}{1+e^{−\\theta^T \\cdot x}}\n",
    "$$\n",
    "\n",
    "$x$: 特徴量ベクトル\n",
    "\n",
    "$θ$: パラメータ（重み）ベクトル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題2】\n",
    "## 最急降下法\n",
    "最急降下法により学習させる実装を行なってください。以下の式で表されるパラメータの更新式のメソッド_gradient_descentを追加し、fit\n",
    "メソッドから呼び出すようにしてください。\n",
    "\n",
    "$$\n",
    "\\theta_j := \\theta_j - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_j}\\\\\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_0} = \\frac{1}{m}  \\sum_{i=1}^{m}(h_θ(x^{(i)}) − y^{(i)})x_j^{(i)}  ,j = 0\\\\\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\biggl(\\frac{1}{m}  \\sum_{i=1}^{m}(h_θ(x^{(i)}) − y^{(i)})x_j^{(i)} \\biggr) + \\frac{λ}{m}\\theta_j　 ,j\\geq 1\n",
    "$$\n",
    "\n",
    "$α$: 学習率\n",
    "\n",
    "$i$: サンプルのインデックス\n",
    "\n",
    "$j$: 特徴量のインデックス\n",
    "\n",
    "$m$: 入力されるデータの数\n",
    "\n",
    "$h_θ()$: 仮定関数\n",
    "\n",
    "$x$: 特徴量ベクトル\n",
    "\n",
    "$θ$: パラメータ（重み）ベクトル\n",
    "\n",
    "$x(i)$: i番目のサンプルの特徴量ベクトル\n",
    "\n",
    "$y(i)$: i番目のサンプルの正解ラベル\n",
    "\n",
    "$θ_j$: j番目のパラメータ（重み）\n",
    "\n",
    "$λ$: 正則化パラメータ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題3】\n",
    "## 推定\n",
    "推定する仕組みを実装してください。ScratchLogisticRegressionクラスの雛形に含まれるpredictメソッドとpredict_probaメソッドに書き加えてください。\n",
    "\n",
    "仮定関数$h_θ(x)$の出力がpredict_probaの返り値、さらにその値に閾値を設けて1と0のラベルとしたものがpredictの返り値となります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題4】\n",
    "## 目的関数\n",
    "以下の数式で表されるロジスティック回帰の 目的関数（損失関数） を実装してください。そして、これをself.loss, self.val_lossに記録するようにしてください。\n",
    "\n",
    "なお、この数式には正則化項が含まれています。\n",
    "\n",
    "$$\n",
    "J(\\theta)=  \\frac{1}{m}  \\sum_{i=1}^{m}[−y^{(i)} log(h_θ(x^{(i)})) − (1−y^{(i)}) log(1−h_θ(x^{(i)}))] +\n",
    "\\frac{λ}{2m}\\sum_{j=1}^n\n",
    "θ^2_j.\\\\\n",
    "$$\n",
    "\n",
    "$m$: 入力されるデータの数\n",
    "\n",
    "$h_θ()$: 仮定関数\n",
    "\n",
    "$x$: 特徴量ベクトル\n",
    "\n",
    "$θ$: パラメータ（重み）ベクトル\n",
    "\n",
    "$x(i)$: i番目のサンプルの特徴量ベクトル\n",
    "\n",
    "$y(i)$: i番目のサンプルの正解ラベル\n",
    "\n",
    "$θ_j$: j番目のパラメータ（重み）\n",
    "\n",
    "$n$: 特徴量の数\n",
    "\n",
    "$λ$: 正則化パラメータ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題5】\n",
    "## 学習と推定\n",
    "機械学習スクラッチ入門のSprintで用意したirisデータセットのvirgicolorとvirginicaの2値分類に対してスクラッチ実装の学習と推定を行なってください。\n",
    "\n",
    "scikit-learnによる実装と比べ、正しく動いているかを確認してください。\n",
    "\n",
    "AccuracyやPrecision、Recallなどの指標値はscikit-learnを使用してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特徴量についてはまず'sepal_length', 'petal_length'を用いて分類する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "     sepal_length  petal_length  Species\n50            7.0           4.7        1\n51            6.4           4.5        1\n52            6.9           4.9        1\n53            5.5           4.0        1\n54            6.5           4.6        1\n..            ...           ...      ...\n145           6.7           5.2        2\n146           6.3           5.0        2\n147           6.5           5.2        2\n148           6.2           5.4        2\n149           5.9           5.1        2\n\n[100 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sepal_length</th>\n      <th>petal_length</th>\n      <th>Species</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>50</th>\n      <td>7.0</td>\n      <td>4.7</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>6.4</td>\n      <td>4.5</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>52</th>\n      <td>6.9</td>\n      <td>4.9</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>53</th>\n      <td>5.5</td>\n      <td>4.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>54</th>\n      <td>6.5</td>\n      <td>4.6</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>6.7</td>\n      <td>5.2</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>146</th>\n      <td>6.3</td>\n      <td>5.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>6.5</td>\n      <td>5.2</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>148</th>\n      <td>6.2</td>\n      <td>5.4</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>149</th>\n      <td>5.9</td>\n      <td>5.1</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 3 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()\n",
    "\n",
    "# 説明変数\n",
    "X = pd.DataFrame(data=data.get('data'), \n",
    "    columns=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'])\n",
    "\n",
    "# 目的変数\n",
    "Y = pd.DataFrame(data=data.get('target'),\n",
    "    columns=['Species'])\n",
    "\n",
    "# 列の抽出\n",
    "df_iris = pd.concat((X, Y), axis=1)[['sepal_length', 'petal_length', 'Species']]\n",
    "# Species: virgicolor = 1, verginica = 2\n",
    "df_iris = df_iris[df_iris['Species'] != 0]\n",
    "\n",
    "display(df_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# get numpy array\n",
    "X_values = df_iris[['sepal_length', 'petal_length']].values\n",
    "# get numpy array and replace Species: virgicolor = 0, verginica = 1\n",
    "y_values = df_iris['Species'].values.reshape(-1, 1)-1\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_values, y_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[-5.52203182e-01  2.43731002e-01]\n [ 2.26090737e+00  2.45203203e+00]\n [ 2.10462345e+00  2.08398186e+00]\n [ 3.85500335e-01 -3.69685950e-01]\n [-8.64771021e-01 -1.71920324e+00]\n [-1.02105494e+00 -1.22846968e+00]\n [-1.95875846e+00 -1.96457002e+00]\n [-2.39635343e-01 -2.47002559e-01]\n [-8.33514237e-02 -4.92369340e-01]\n [ 2.29216415e-01  7.34464563e-01]\n [-3.95919263e-01 -1.10578629e+00]\n [ 7.29324957e-02  1.34788152e+00]\n [-1.64619062e+00 -1.22846968e+00]\n [-8.64771021e-01 -8.60419511e-01]\n [ 7.29324957e-02 -1.63577854e-03]\n [-2.39635343e-01 -3.69685950e-01]\n [-2.11504238e+00 -4.92369340e-01]\n [ 1.01063601e+00  9.79831344e-01]\n [-8.64771021e-01 -4.92369340e-01]\n [-1.17733886e+00 -6.15052730e-01]\n [ 1.32320385e+00  1.22519812e+00]\n [ 1.01063601e+00  2.43731002e-01]\n [-1.02105494e+00 -4.92369340e-01]\n [-1.17733886e+00 -1.10578629e+00]\n [ 2.29216415e-01 -4.92369340e-01]\n [ 6.98068174e-01  3.66414392e-01]\n [ 5.41784254e-01 -6.15052730e-01]\n [-1.17733886e+00 -1.47383646e+00]\n [ 1.63577169e+00  1.71593169e+00]\n [-3.95919263e-01 -4.92369340e-01]\n [ 6.98068174e-01  8.57147954e-01]\n [ 1.16691993e+00 -2.47002559e-01]\n [-8.64771021e-01 -9.83102901e-01]\n [-5.52203182e-01 -8.60419511e-01]\n [ 2.29216415e-01  8.57147954e-01]\n [ 3.85500335e-01  2.43731002e-01]\n [ 7.29324957e-02  1.21047612e-01]\n [-7.08487102e-01 -1.22846968e+00]\n [ 6.98068174e-01  1.10251473e+00]\n [-2.11504238e+00 -1.96457002e+00]\n [ 2.26090737e+00  2.20666525e+00]\n [-7.08487102e-01 -9.83102901e-01]\n [ 6.98068174e-01 -2.47002559e-01]\n [ 1.01063601e+00  6.11781173e-01]\n [ 6.98068174e-01  9.79831344e-01]\n [ 6.98068174e-01  9.79831344e-01]\n [-1.17733886e+00 -1.35115307e+00]\n [-3.95919263e-01  2.43731002e-01]\n [ 7.29324957e-02 -6.15052730e-01]\n [-2.39635343e-01  8.57147954e-01]\n [-3.95919263e-01 -1.24319169e-01]\n [-2.39635343e-01 -1.63577854e-03]\n [-1.02105494e+00 -1.63577854e-03]\n [ 7.29324957e-02  8.57147954e-01]\n [ 7.29324957e-02  8.57147954e-01]\n [ 8.54352093e-01 -1.24319169e-01]\n [-1.02105494e+00 -1.59651985e+00]\n [-3.95919263e-01  1.21047612e-01]\n [ 1.47948777e+00  1.47056491e+00]\n [-7.08487102e-01  2.43731002e-01]\n [ 2.57347521e+00  1.83861508e+00]\n [ 3.85500335e-01  1.10251473e+00]\n [ 5.41784254e-01 -3.69685950e-01]\n [ 8.54352093e-01  1.22519812e+00]\n [ 7.29324957e-02 -1.63577854e-03]\n [-3.95919263e-01 -4.92369340e-01]\n [ 1.47948777e+00  1.10251473e+00]\n [ 6.98068174e-01 -6.15052730e-01]\n [-1.02105494e+00 -8.60419511e-01]\n [ 3.85500335e-01  7.34464563e-01]\n [-5.52203182e-01 -1.24319169e-01]\n [-8.33514237e-02 -7.37736121e-01]\n [-8.33514237e-02  6.11781173e-01]\n [-8.33514237e-02 -1.24319169e-01]\n [-1.17733886e+00 -1.10578629e+00]]\n[[ 1.01063601e+00 -1.63577854e-03]\n [ 2.29216415e-01 -7.37736121e-01]\n [-2.39635343e-01 -1.10578629e+00]\n [-2.39635343e-01 -2.47002559e-01]\n [ 8.54352093e-01  7.34464563e-01]\n [ 2.29216415e-01  4.89097783e-01]\n [-1.95875846e+00 -1.71920324e+00]\n [ 2.26090737e+00  1.47056491e+00]\n [-1.02105494e+00 -9.83102901e-01]\n [-7.08487102e-01  2.43731002e-01]\n [ 2.29216415e-01  8.57147954e-01]\n [ 6.98068174e-01  1.21047612e-01]\n [-7.08487102e-01 -1.10578629e+00]\n [-8.64771021e-01 -8.60419511e-01]\n [ 7.29324957e-02  2.43731002e-01]\n [ 3.85500335e-01  3.66414392e-01]\n [ 1.79205561e+00  1.47056491e+00]\n [ 2.26090737e+00  2.20666525e+00]\n [ 1.47948777e+00  1.34788152e+00]\n [ 7.29324957e-02 -2.47002559e-01]\n [-8.64771021e-01  1.21047612e-01]\n [-1.33362278e+00 -4.92369340e-01]\n [-1.80247454e+00 -2.33262019e+00]\n [ 2.29216415e-01  4.89097783e-01]\n [-7.08487102e-01  2.43731002e-01]]\n"
    }
   ],
   "source": [
    "# Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_std = scaler.transform(X_train)\n",
    "X_val_std = scaler.transform(X_val)\n",
    "\n",
    "print(X_train_std)\n",
    "print(X_val_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "6677260352\n9693 loss : train: 0.5882354302124235, valid: 0.6175846677260304\n9694 loss : train: 0.5882354302124189, valid: 0.6175846677260256\n9695 loss : train: 0.5882354302124144, valid: 0.6175846677260209\n9696 loss : train: 0.5882354302124098, valid: 0.6175846677260162\n9697 loss : train: 0.5882354302124054, valid: 0.6175846677260115\n9698 loss : train: 0.588235430212401, valid: 0.6175846677260068\n9699 loss : train: 0.5882354302123964, valid: 0.6175846677260022\n9700 loss : train: 0.588235430212392, valid: 0.6175846677259974\n9701 loss : train: 0.5882354302123874, valid: 0.6175846677259929\n9702 loss : train: 0.588235430212383, valid: 0.6175846677259881\n9703 loss : train: 0.5882354302123786, valid: 0.6175846677259834\n9704 loss : train: 0.5882354302123742, valid: 0.6175846677259789\n9705 loss : train: 0.5882354302123697, valid: 0.6175846677259743\n9706 loss : train: 0.5882354302123655, valid: 0.6175846677259695\n9707 loss : train: 0.5882354302123609, valid: 0.617584667725965\n9708 loss : train: 0.5882354302123565, valid: 0.6175846677259604\n9709 loss : train: 0.5882354302123523, valid: 0.6175846677259558\n9710 loss : train: 0.5882354302123479, valid: 0.6175846677259512\n9711 loss : train: 0.5882354302123435, valid: 0.6175846677259467\n9712 loss : train: 0.5882354302123391, valid: 0.6175846677259421\n9713 loss : train: 0.5882354302123348, valid: 0.6175846677259377\n9714 loss : train: 0.5882354302123306, valid: 0.6175846677259331\n9715 loss : train: 0.5882354302123262, valid: 0.6175846677259286\n9716 loss : train: 0.5882354302123219, valid: 0.6175846677259239\n9717 loss : train: 0.5882354302123176, valid: 0.6175846677259196\n9718 loss : train: 0.5882354302123133, valid: 0.6175846677259151\n9719 loss : train: 0.5882354302123091, valid: 0.6175846677259106\n9720 loss : train: 0.5882354302123048, valid: 0.6175846677259061\n9721 loss : train: 0.5882354302123005, valid: 0.6175846677259017\n9722 loss : train: 0.5882354302122963, valid: 0.6175846677258973\n9723 loss : train: 0.5882354302122922, valid: 0.6175846677258928\n9724 loss : train: 0.5882354302122879, valid: 0.6175846677258884\n9725 loss : train: 0.5882354302122836, valid: 0.6175846677258839\n9726 loss : train: 0.5882354302122795, valid: 0.6175846677258796\n9727 loss : train: 0.5882354302122753, valid: 0.6175846677258752\n9728 loss : train: 0.5882354302122711, valid: 0.617584667725871\n9729 loss : train: 0.5882354302122669, valid: 0.6175846677258664\n9730 loss : train: 0.5882354302122628, valid: 0.6175846677258621\n9731 loss : train: 0.5882354302122587, valid: 0.6175846677258577\n9732 loss : train: 0.5882354302122544, valid: 0.6175846677258534\n9733 loss : train: 0.5882354302122503, valid: 0.6175846677258491\n9734 loss : train: 0.5882354302122463, valid: 0.6175846677258449\n9735 loss : train: 0.5882354302122422, valid: 0.6175846677258404\n9736 loss : train: 0.588235430212238, valid: 0.6175846677258362\n9737 loss : train: 0.5882354302122339, valid: 0.6175846677258319\n9738 loss : train: 0.5882354302122299, valid: 0.6175846677258277\n9739 loss : train: 0.5882354302122258, valid: 0.6175846677258234\n9740 loss : train: 0.5882354302122219, valid: 0.6175846677258192\n9741 loss : train: 0.5882354302122176, valid: 0.6175846677258149\n9742 loss : train: 0.5882354302122137, valid: 0.6175846677258107\n9743 loss : train: 0.5882354302122097, valid: 0.6175846677258064\n9744 loss : train: 0.5882354302122057, valid: 0.6175846677258022\n9745 loss : train: 0.5882354302122016, valid: 0.617584667725798\n9746 loss : train: 0.5882354302121977, valid: 0.6175846677257938\n9747 loss : train: 0.5882354302121936, valid: 0.6175846677257896\n9748 loss : train: 0.5882354302121897, valid: 0.6175846677257856\n9749 loss : train: 0.5882354302121858, valid: 0.6175846677257815\n9750 loss : train: 0.5882354302121817, valid: 0.6175846677257771\n9751 loss : train: 0.5882354302121777, valid: 0.617584667725773\n9752 loss : train: 0.5882354302121738, valid: 0.617584667725769\n9753 loss : train: 0.58823543021217, valid: 0.6175846677257649\n9754 loss : train: 0.588235430212166, valid: 0.6175846677257606\n9755 loss : train: 0.588235430212162, valid: 0.6175846677257566\n9756 loss : train: 0.5882354302121582, valid: 0.6175846677257525\n9757 loss : train: 0.5882354302121543, valid: 0.6175846677257485\n9758 loss : train: 0.5882354302121504, valid: 0.6175846677257444\n9759 loss : train: 0.5882354302121465, valid: 0.6175846677257403\n9760 loss : train: 0.5882354302121426, valid: 0.6175846677257363\n9761 loss : train: 0.5882354302121388, valid: 0.6175846677257323\n9762 loss : train: 0.588235430212135, valid: 0.6175846677257283\n9763 loss : train: 0.5882354302121312, valid: 0.6175846677257243\n9764 loss : train: 0.5882354302121273, valid: 0.6175846677257201\n9765 loss : train: 0.5882354302121235, valid: 0.6175846677257162\n9766 loss : train: 0.5882354302121197, valid: 0.6175846677257122\n9767 loss : train: 0.5882354302121159, valid: 0.6175846677257082\n9768 loss : train: 0.588235430212112, valid: 0.6175846677257042\n9769 loss : train: 0.5882354302121083, valid: 0.6175846677257003\n9770 loss : train: 0.5882354302121046, valid: 0.6175846677256963\n9771 loss : train: 0.5882354302121009, valid: 0.6175846677256923\n9772 loss : train: 0.5882354302120969, valid: 0.6175846677256885\n9773 loss : train: 0.5882354302120932, valid: 0.6175846677256845\n9774 loss : train: 0.5882354302120897, valid: 0.6175846677256807\n9775 loss : train: 0.5882354302120858, valid: 0.6175846677256768\n9776 loss : train: 0.588235430212082, valid: 0.6175846677256728\n9777 loss : train: 0.5882354302120784, valid: 0.617584667725669\n9778 loss : train: 0.5882354302120748, valid: 0.617584667725665\n9779 loss : train: 0.5882354302120711, valid: 0.6175846677256611\n9780 loss : train: 0.5882354302120675, valid: 0.6175846677256573\n9781 loss : train: 0.5882354302120636, valid: 0.6175846677256536\n9782 loss : train: 0.5882354302120599, valid: 0.6175846677256497\n9783 loss : train: 0.5882354302120564, valid: 0.6175846677256459\n9784 loss : train: 0.5882354302120528, valid: 0.617584667725642\n9785 loss : train: 0.5882354302120492, valid: 0.6175846677256382\n9786 loss : train: 0.5882354302120455, valid: 0.6175846677256345\n9787 loss : train: 0.5882354302120418, valid: 0.6175846677256306\n9788 loss : train: 0.5882354302120383, valid: 0.6175846677256268\n9789 loss : train: 0.5882354302120347, valid: 0.6175846677256231\n9790 loss : train: 0.588235430212031, valid: 0.6175846677256193\n9791 loss : train: 0.5882354302120274, valid: 0.6175846677256155\n9792 loss : train: 0.5882354302120238, valid: 0.6175846677256118\n9793 loss : train: 0.5882354302120203, valid: 0.617584667725608\n9794 loss : train: 0.5882354302120169, valid: 0.6175846677256044\n9795 loss : train: 0.5882354302120133, valid: 0.6175846677256006\n9796 loss : train: 0.5882354302120095, valid: 0.617584667725597\n9797 loss : train: 0.5882354302120062, valid: 0.6175846677255933\n9798 loss : train: 0.5882354302120025, valid: 0.6175846677255895\n9799 loss : train: 0.5882354302119992, valid: 0.617584667725586\n9800 loss : train: 0.5882354302119955, valid: 0.6175846677255822\n9801 loss : train: 0.5882354302119921, valid: 0.6175846677255785\n9802 loss : train: 0.5882354302119885, valid: 0.6175846677255749\n9803 loss : train: 0.5882354302119852, valid: 0.6175846677255713\n9804 loss : train: 0.5882354302119815, valid: 0.6175846677255676\n9805 loss : train: 0.5882354302119781, valid: 0.617584667725564\n9806 loss : train: 0.5882354302119747, valid: 0.6175846677255602\n9807 loss : train: 0.5882354302119713, valid: 0.6175846677255566\n9808 loss : train: 0.5882354302119679, valid: 0.6175846677255531\n9809 loss : train: 0.5882354302119643, valid: 0.6175846677255495\n9810 loss : train: 0.588235430211961, valid: 0.617584667725546\n9811 loss : train: 0.5882354302119576, valid: 0.6175846677255424\n9812 loss : train: 0.5882354302119541, valid: 0.6175846677255388\n9813 loss : train: 0.5882354302119508, valid: 0.6175846677255352\n9814 loss : train: 0.5882354302119475, valid: 0.6175846677255317\n9815 loss : train: 0.5882354302119441, valid: 0.6175846677255281\n9816 loss : train: 0.5882354302119407, valid: 0.6175846677255247\n9817 loss : train: 0.5882354302119372, valid: 0.617584667725521\n9818 loss : train: 0.5882354302119339, valid: 0.6175846677255175\n9819 loss : train: 0.5882354302119305, valid: 0.617584667725514\n9820 loss : train: 0.5882354302119271, valid: 0.6175846677255105\n9821 loss : train: 0.5882354302119239, valid: 0.6175846677255071\n9822 loss : train: 0.5882354302119206, valid: 0.6175846677255036\n9823 loss : train: 0.5882354302119173, valid: 0.6175846677255\n9824 loss : train: 0.5882354302119139, valid: 0.6175846677254967\n9825 loss : train: 0.5882354302119107, valid: 0.6175846677254931\n9826 loss : train: 0.5882354302119074, valid: 0.6175846677254897\n9827 loss : train: 0.5882354302119043, valid: 0.6175846677254863\n9828 loss : train: 0.5882354302119007, valid: 0.6175846677254828\n9829 loss : train: 0.5882354302118975, valid: 0.6175846677254794\n9830 loss : train: 0.5882354302118943, valid: 0.617584667725476\n9831 loss : train: 0.5882354302118911, valid: 0.6175846677254726\n9832 loss : train: 0.5882354302118878, valid: 0.6175846677254693\n9833 loss : train: 0.5882354302118846, valid: 0.6175846677254658\n9834 loss : train: 0.5882354302118814, valid: 0.6175846677254625\n9835 loss : train: 0.5882354302118782, valid: 0.6175846677254592\n9836 loss : train: 0.588235430211875, valid: 0.6175846677254556\n9837 loss : train: 0.5882354302118717, valid: 0.6175846677254524\n9838 loss : train: 0.5882354302118685, valid: 0.6175846677254492\n9839 loss : train: 0.5882354302118654, valid: 0.6175846677254456\n9840 loss : train: 0.5882354302118622, valid: 0.6175846677254423\n9841 loss : train: 0.588235430211859, valid: 0.617584667725439\n9842 loss : train: 0.5882354302118559, valid: 0.6175846677254357\n9843 loss : train: 0.5882354302118525, valid: 0.6175846677254323\n9844 loss : train: 0.5882354302118494, valid: 0.617584667725429\n9845 loss : train: 0.5882354302118462, valid: 0.6175846677254259\n9846 loss : train: 0.5882354302118431, valid: 0.6175846677254225\n9847 loss : train: 0.5882354302118401, valid: 0.6175846677254193\n9848 loss : train: 0.5882354302118371, valid: 0.6175846677254159\n9849 loss : train: 0.5882354302118339, valid: 0.6175846677254128\n9850 loss : train: 0.5882354302118307, valid: 0.6175846677254094\n9851 loss : train: 0.5882354302118276, valid: 0.6175846677254063\n9852 loss : train: 0.5882354302118246, valid: 0.6175846677254031\n9853 loss : train: 0.5882354302118215, valid: 0.6175846677253998\n9854 loss : train: 0.5882354302118183, valid: 0.6175846677253966\n9855 loss : train: 0.5882354302118153, valid: 0.6175846677253934\n9856 loss : train: 0.5882354302118122, valid: 0.6175846677253901\n9857 loss : train: 0.5882354302118092, valid: 0.617584667725387\n9858 loss : train: 0.5882354302118061, valid: 0.6175846677253838\n9859 loss : train: 0.5882354302118031, valid: 0.6175846677253806\n9860 loss : train: 0.5882354302118001, valid: 0.6175846677253775\n9861 loss : train: 0.588235430211797, valid: 0.6175846677253743\n9862 loss : train: 0.5882354302117941, valid: 0.6175846677253712\n9863 loss : train: 0.5882354302117911, valid: 0.6175846677253678\n9864 loss : train: 0.588235430211788, valid: 0.6175846677253647\n9865 loss : train: 0.588235430211785, valid: 0.6175846677253617\n9866 loss : train: 0.5882354302117822, valid: 0.6175846677253585\n9867 loss : train: 0.5882354302117792, valid: 0.6175846677253554\n9868 loss : train: 0.5882354302117762, valid: 0.6175846677253523\n9869 loss : train: 0.5882354302117732, valid: 0.617584667725349\n9870 loss : train: 0.5882354302117702, valid: 0.617584667725346\n9871 loss : train: 0.5882354302117673, valid: 0.6175846677253429\n9872 loss : train: 0.5882354302117644, valid: 0.6175846677253399\n9873 loss : train: 0.5882354302117615, valid: 0.6175846677253368\n9874 loss : train: 0.5882354302117584, valid: 0.6175846677253338\n9875 loss : train: 0.5882354302117555, valid: 0.6175846677253306\n9876 loss : train: 0.5882354302117525, valid: 0.6175846677253276\n9877 loss : train: 0.5882354302117497, valid: 0.6175846677253246\n9878 loss : train: 0.5882354302117466, valid: 0.6175846677253216\n9879 loss : train: 0.5882354302117438, valid: 0.6175846677253185\n9880 loss : train: 0.5882354302117411, valid: 0.6175846677253155\n9881 loss : train: 0.5882354302117382, valid: 0.6175846677253125\n9882 loss : train: 0.5882354302117354, valid: 0.6175846677253094\n9883 loss : train: 0.5882354302117324, valid: 0.6175846677253065\n9884 loss : train: 0.5882354302117296, valid: 0.6175846677253035\n9885 loss : train: 0.5882354302117268, valid: 0.6175846677253005\n9886 loss : train: 0.5882354302117238, valid: 0.6175846677252975\n9887 loss : train: 0.5882354302117211, valid: 0.6175846677252946\n9888 loss : train: 0.5882354302117182, valid: 0.6175846677252916\n9889 loss : train: 0.5882354302117153, valid: 0.6175846677252885\n9890 loss : train: 0.5882354302117127, valid: 0.6175846677252856\n9891 loss : train: 0.5882354302117098, valid: 0.6175846677252828\n9892 loss : train: 0.588235430211707, valid: 0.6175846677252798\n9893 loss : train: 0.5882354302117041, valid: 0.6175846677252769\n9894 loss : train: 0.5882354302117013, valid: 0.6175846677252739\n9895 loss : train: 0.5882354302116986, valid: 0.6175846677252711\n9896 loss : train: 0.5882354302116958, valid: 0.6175846677252681\n9897 loss : train: 0.588235430211693, valid: 0.6175846677252652\n9898 loss : train: 0.5882354302116903, valid: 0.6175846677252622\n9899 loss : train: 0.5882354302116874, valid: 0.6175846677252594\n9900 loss : train: 0.5882354302116847, valid: 0.6175846677252566\n9901 loss : train: 0.588235430211682, valid: 0.6175846677252537\n9902 loss : train: 0.5882354302116795, valid: 0.6175846677252508\n9903 loss : train: 0.5882354302116766, valid: 0.617584667725248\n9904 loss : train: 0.5882354302116737, valid: 0.6175846677252451\n9905 loss : train: 0.5882354302116711, valid: 0.6175846677252422\n9906 loss : train: 0.5882354302116685, valid: 0.6175846677252393\n9907 loss : train: 0.5882354302116658, valid: 0.6175846677252366\n9908 loss : train: 0.5882354302116631, valid: 0.6175846677252337\n9909 loss : train: 0.5882354302116604, valid: 0.6175846677252309\n9910 loss : train: 0.5882354302116576, valid: 0.6175846677252281\n9911 loss : train: 0.5882354302116549, valid: 0.6175846677252254\n9912 loss : train: 0.5882354302116523, valid: 0.6175846677252226\n9913 loss : train: 0.5882354302116497, valid: 0.6175846677252198\n9914 loss : train: 0.5882354302116469, valid: 0.617584667725217\n9915 loss : train: 0.5882354302116444, valid: 0.6175846677252141\n9916 loss : train: 0.5882354302116417, valid: 0.6175846677252114\n9917 loss : train: 0.588235430211639, valid: 0.6175846677252087\n9918 loss : train: 0.5882354302116365, valid: 0.6175846677252059\n9919 loss : train: 0.5882354302116338, valid: 0.6175846677252032\n9920 loss : train: 0.5882354302116312, valid: 0.6175846677252004\n9921 loss : train: 0.5882354302116285, valid: 0.6175846677251977\n9922 loss : train: 0.5882354302116259, valid: 0.617584667725195\n9923 loss : train: 0.5882354302116234, valid: 0.6175846677251922\n9924 loss : train: 0.5882354302116207, valid: 0.6175846677251895\n9925 loss : train: 0.5882354302116181, valid: 0.6175846677251868\n9926 loss : train: 0.5882354302116155, valid: 0.6175846677251841\n9927 loss : train: 0.5882354302116131, valid: 0.6175846677251815\n9928 loss : train: 0.5882354302116104, valid: 0.6175846677251787\n9929 loss : train: 0.5882354302116077, valid: 0.6175846677251761\n9930 loss : train: 0.5882354302116054, valid: 0.6175846677251734\n9931 loss : train: 0.5882354302116028, valid: 0.6175846677251706\n9932 loss : train: 0.5882354302116002, valid: 0.6175846677251678\n9933 loss : train: 0.5882354302115977, valid: 0.6175846677251653\n9934 loss : train: 0.5882354302115951, valid: 0.6175846677251626\n9935 loss : train: 0.5882354302115926, valid: 0.61758466772516\n9936 loss : train: 0.5882354302115902, valid: 0.6175846677251574\n9937 loss : train: 0.5882354302115878, valid: 0.6175846677251549\n9938 loss : train: 0.5882354302115851, valid: 0.6175846677251522\n9939 loss : train: 0.5882354302115825, valid: 0.6175846677251495\n9940 loss : train: 0.58823543021158, valid: 0.6175846677251469\n9941 loss : train: 0.5882354302115775, valid: 0.6175846677251443\n9942 loss : train: 0.5882354302115752, valid: 0.6175846677251418\n9943 loss : train: 0.5882354302115727, valid: 0.6175846677251391\n9944 loss : train: 0.5882354302115702, valid: 0.6175846677251365\n9945 loss : train: 0.5882354302115678, valid: 0.6175846677251339\n9946 loss : train: 0.5882354302115652, valid: 0.6175846677251314\n9947 loss : train: 0.5882354302115628, valid: 0.6175846677251285\n9948 loss : train: 0.5882354302115604, valid: 0.6175846677251262\n9949 loss : train: 0.5882354302115579, valid: 0.6175846677251237\n9950 loss : train: 0.5882354302115554, valid: 0.6175846677251211\n9951 loss : train: 0.5882354302115529, valid: 0.6175846677251187\n9952 loss : train: 0.5882354302115508, valid: 0.617584667725116\n9953 loss : train: 0.5882354302115482, valid: 0.6175846677251134\n9954 loss : train: 0.5882354302115458, valid: 0.6175846677251108\n9955 loss : train: 0.5882354302115433, valid: 0.6175846677251084\n9956 loss : train: 0.588235430211541, valid: 0.6175846677251059\n9957 loss : train: 0.5882354302115387, valid: 0.6175846677251035\n9958 loss : train: 0.5882354302115362, valid: 0.6175846677251009\n9959 loss : train: 0.5882354302115339, valid: 0.6175846677250985\n9960 loss : train: 0.5882354302115316, valid: 0.6175846677250958\n9961 loss : train: 0.588235430211529, valid: 0.6175846677250934\n9962 loss : train: 0.5882354302115267, valid: 0.6175846677250909\n9963 loss : train: 0.5882354302115245, valid: 0.6175846677250885\n9964 loss : train: 0.5882354302115219, valid: 0.6175846677250861\n9965 loss : train: 0.5882354302115197, valid: 0.6175846677250836\n9966 loss : train: 0.5882354302115174, valid: 0.6175846677250811\n9967 loss : train: 0.5882354302115149, valid: 0.6175846677250787\n9968 loss : train: 0.5882354302115127, valid: 0.6175846677250761\n9969 loss : train: 0.5882354302115103, valid: 0.6175846677250737\n9970 loss : train: 0.588235430211508, valid: 0.6175846677250714\n9971 loss : train: 0.5882354302115057, valid: 0.6175846677250688\n9972 loss : train: 0.5882354302115034, valid: 0.6175846677250665\n9973 loss : train: 0.5882354302115012, valid: 0.6175846677250642\n9974 loss : train: 0.5882354302114987, valid: 0.6175846677250617\n9975 loss : train: 0.5882354302114966, valid: 0.6175846677250593\n9976 loss : train: 0.5882354302114943, valid: 0.6175846677250569\n9977 loss : train: 0.5882354302114919, valid: 0.6175846677250546\n9978 loss : train: 0.5882354302114896, valid: 0.6175846677250523\n9979 loss : train: 0.5882354302114876, valid: 0.6175846677250498\n9980 loss : train: 0.588235430211485, valid: 0.6175846677250474\n9981 loss : train: 0.5882354302114828, valid: 0.6175846677250451\n9982 loss : train: 0.5882354302114805, valid: 0.6175846677250426\n9983 loss : train: 0.5882354302114785, valid: 0.6175846677250403\n9984 loss : train: 0.5882354302114762, valid: 0.617584667725038\n9985 loss : train: 0.588235430211474, valid: 0.6175846677250356\n9986 loss : train: 0.5882354302114716, valid: 0.6175846677250333\n9987 loss : train: 0.5882354302114695, valid: 0.6175846677250311\n9988 loss : train: 0.5882354302114672, valid: 0.6175846677250286\n9989 loss : train: 0.5882354302114651, valid: 0.6175846677250263\n9990 loss : train: 0.5882354302114629, valid: 0.6175846677250241\n9991 loss : train: 0.5882354302114605, valid: 0.6175846677250219\n9992 loss : train: 0.5882354302114585, valid: 0.6175846677250194\n9993 loss : train: 0.5882354302114561, valid: 0.6175846677250171\n9994 loss : train: 0.5882354302114541, valid: 0.6175846677250147\n9995 loss : train: 0.5882354302114519, valid: 0.6175846677250125\n9996 loss : train: 0.5882354302114495, valid: 0.6175846677250102\n9997 loss : train: 0.5882354302114474, valid: 0.6175846677250078\n9998 loss : train: 0.5882354302114454, valid: 0.6175846677250056\n9999 loss : train: 0.5882354302114431, valid: 0.6175846677250034\n10000 loss : train: 0.588235430211441, valid: 0.6175846677250012\nDone! elapsed time: 2.35278s\n"
    }
   ],
   "source": [
    "# learn and predict\n",
    "reg = ScratchLogisticRegression(num_iter=10000, lr=0.01, C=1, no_bias=False, verbose=True)\n",
    "reg.fit(X_train_std, y_train, X_val_std, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題6】\n",
    "## 学習曲線のプロット\n",
    "学習曲線を見て損失が適切に下がっているかどうか確認してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題7】\n",
    "## 決定領域の可視化\n",
    "決定領域を可視化してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題8】（アドバンス課題）\n",
    "## 重みの保存\n",
    "検証が容易になるように、学習した重みを保存および読み込みができるようにしましょう。pickleモジュールやNumPyのnp.savezを利用します。\n",
    "\n",
    "[pickle — Python オブジェクトの直列化 — Python 3.7.4 ドキュメント](https://docs.python.org/ja/3/library/pickle.html)\n",
    "\n",
    "[numpy.savez — NumPy v1.17 Manual](https://docs.scipy.org/doc/numpy/reference/generated/numpy.savez.html)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}