{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint\n",
    "## 深層学習スクラッチ ディープニューラルネットワーク\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.このSprintについて\n",
    "\n",
    "### Sprintの目的\n",
    "- スクラッチを通してニューラルネットワークの発展的内容を理解する\n",
    "\n",
    "### どのように学ぶか\n",
    "スクラッチで作成したニューラルネットワークの実装を拡張していきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.ディープニューラルネットワークスクラッチ\n",
    "\n",
    "前回は3層のニューラルネットワークを作成しましたが、今回はこれを任意の層数に拡張しやすいものに書き換えていきます。その上で、活性化関数や初期値、最適化手法について発展的なものを扱えるようにしていきます。\n",
    "\n",
    "\n",
    "このようなスクラッチを行うことで、今後各種フレームワークを利用していくにあたり、内部の動きが想像できることを目指します。\n",
    "\n",
    "\n",
    "名前は新しくScratchDeepNeuralNetrowkClassifierクラスとしてください。\n",
    "\n",
    "\n",
    "### 層などのクラス化\n",
    "クラスにまとめて行くことで、構成を変更しやすい実装にしていきます。\n",
    "\n",
    "\n",
    "**手を加える箇所**\n",
    "\n",
    "\n",
    "- 層の数\n",
    "- 層の種類（今後畳み込み層など他のタイプの層が登場する）\n",
    "- 活性化関数の種類\n",
    "- 重みやバイアスの初期化方法\n",
    "- 最適化手法\n",
    "\n",
    "そのために、全結合層、各種活性化関数、重みやバイアスの初期化、最適化手法それぞれのクラスを作成します。\n",
    "\n",
    "\n",
    "実装方法は自由ですが、簡単な例を紹介します。サンプルコード1のように全結合層と活性化関数のインスタンスを作成し、サンプルコード2,3のようにして使用します。それぞれのクラスについてはこのあと解説します。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**《サンプルコード1》**\n",
    "\n",
    "ScratchDeepNeuralNetrowkClassifierのfitメソッド内\n",
    "\n",
    "```python\n",
    "# self.sigma : ガウス分布の標準偏差\n",
    "# self.lr : 学習率\n",
    "# self.n_nodes1 : 1層目のノード数\n",
    "# self.n_nodes2 : 2層目のノード数\n",
    "# self.n_output : 出力層のノード数\n",
    "optimizer = SGD(self.lr)\n",
    "self.FC1 = FC(self.n_features, self.n_nodes1, SimpleInitializer(self.sigma), optimizer)\n",
    "self.activation1 = Tanh()\n",
    "self.FC2 = FC(self.n_nodes1, self.n_nodes2, SimpleInitializer(self.sigma), optimizer)\n",
    "self.activation2 = Tanh()\n",
    "self.FC3 = FC(self.n_nodes2, self.n_output, SimpleInitializer(self.sigma), optimizer)\n",
    "self.activation3 = Softmax()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**《サンプルコード2》**\n",
    "\n",
    "イテレーションごとのフォワード\n",
    "\n",
    "```python\n",
    "A1 = self.FC1.forward(X)\n",
    "Z1 = self.activation1.forward(A1)\n",
    "A2 = self.FC2.forward(Z1)\n",
    "Z2 = self.activation2.forward(A2)\n",
    "A3 = self.FC3.forward(Z2)\n",
    "Z3 = self.activation3.forward(A3)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**《サンプルコード3》**\n",
    "\n",
    "イテレーションごとのバックワード\n",
    "\n",
    "```python\n",
    "dA3 = self.activation3.backward(Z3, Y) # 交差エントロピー誤差とソフトマックスを合わせている\n",
    "dZ2 = self.FC3.backward(dA3)\n",
    "dA2 = self.activation2.backward(dZ2)\n",
    "dZ1 = self.FC2.backward(dA2)\n",
    "dA1 = self.activation1.backward(dZ1)\n",
    "dZ0 = self.FC1.backward(dA1) # dZ0は使用しない\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題1】\n",
    "#### 全結合層のクラス化\n",
    "全結合層のクラス化を行なってください。\n",
    "\n",
    "\n",
    "以下に雛形を載せました。コンストラクタで重みやバイアスの初期化をして、あとはフォワードとバックワードのメソッドを用意します。重みW、バイアスB、およびフォワード時の入力Xをインスタンス変数として保持しておくことで、煩雑な入出力は不要になります。\n",
    "\n",
    "\n",
    "なお、インスタンスも引数として渡すことができます。そのため、初期化方法のインスタンスinitializerをコンストラクタで受け取れば、それにより初期化が行われます。渡すインスタンスを変えれば、初期化方法が変えられます。\n",
    "\n",
    "\n",
    "また、引数として自身のインスタンス`self`を渡すこともできます。これを利用して`self.optimizer.update(self)`という風に層の重みの更新が可能です。更新に必要な値は複数ありますが、全て全結合層が持つインスタンス変数にすることができます。\n",
    "\n",
    "\n",
    "初期化方法と最適化手法のクラスについては後述します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "\n",
    "    Attribute\n",
    "    ---------\n",
    "    self.W : ndarray(n_nodes1, n_nodes2)\n",
    "      重み\n",
    "    self.B : ndarray(n_node2,)\n",
    "      バイアス\n",
    "    self.H : float\n",
    "      前イテレーションまでの勾配の二乗和\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
    "        self.B = initializer.B(n_nodes2)\n",
    "        self.HW = np.zeros(self.W.shape)\n",
    "        self.HB = np.zeros(self.B.shape)\n",
    "        pass\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"\n",
    "        self.Z_prev = X\n",
    "\n",
    "        A = X*self.W + self.B\n",
    "\n",
    "        return A\n",
    "\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "\n",
    "        # 更新\n",
    "        dZ = self.optimizer.update(self, dA)\n",
    "\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題2】\n",
    "#### 初期化方法のクラス化\n",
    "初期化を行うコードをクラス化してください。\n",
    "\n",
    "\n",
    "前述のように、全結合層のコンストラクタに初期化方法のインスタンスを渡せるようにします。以下の雛形に必要なコードを書き加えていってください。標準偏差の値（sigma）はコンストラクタで受け取るようにすることで、全結合層のクラス内にこの値（sigma）を渡さなくてすむようになります。\n",
    "\n",
    "\n",
    "これまで扱ってきた初期化方法はSimpleInitializerクラスと名付けることにします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "    \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        W = self.sigma * np.random.randn(self.n_nodes1, self.n_nodes2)\n",
    "        \n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        B = np.zeros((n_nodes2,))\n",
    "\n",
    "        return B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題3】\n",
    "#### 最適化手法のクラス化\n",
    "最適化手法のクラス化を行なってください。\n",
    "\n",
    "\n",
    "最適化手法に関しても初期化方法同様に全結合層にインスタンスとして渡します。バックワードのときに`self.optimizer.update(self)`のように更新できるようにします。以下の雛形に必要なコードを書き加えていってください。\n",
    "\n",
    "\n",
    "これまで扱ってきた最適化手法はSGDクラス（Stochastic Gradient Descent、確率的勾配降下法）として作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, layer, dA):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "        dW = np.sum(dA, axis=0)\n",
    "        dB = layer.Z_prev.T * dA\n",
    "        dZ = dA * layer.W.T\n",
    "\n",
    "        layer.W = layer.W - self.lr*dW\n",
    "        layer.B = layer.B - self.lr*dB\n",
    "\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題4】\n",
    "#### 活性化関数のクラス化\n",
    "活性化関数のクラス化を行なってください。\n",
    "\n",
    "\n",
    "ソフトマックス関数のバックプロパゲーションには交差エントロピー誤差の計算も含む実装を行うことで計算が簡略化されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    \"\"\"\n",
    "    ソフトマックス関数\n",
    "\n",
    "    Attribute\n",
    "    ----------\n",
    "    self.loss\n",
    "        出力の交差エントロピー誤差\n",
    "    \"\"\"\n",
    "    def forward(self, A):\n",
    "        Z = np.exp(A) / np.sum(np.exp(A), axis=1).reshape(-1, 1)\n",
    "        \n",
    "        return Z\n",
    "\n",
    "    def backward(self, Z, Y):\n",
    "        dA = Z + Y\n",
    "        self.loss = self.calc_cross_entropy_loss(Y, Z)\n",
    "\n",
    "        return dA\n",
    "    \n",
    "    def calc_cross_entropy_loss(self, y_true, y_pred):\n",
    "        n_samples = y_true.shape[0]\n",
    "\n",
    "        cross_entropy_loss = (-1 * (np.sum(y_true*np.log(y_pred)))) / n_samples\n",
    " \n",
    "        return cross_entropy_loss\n",
    "\n",
    "class Sigmoid:\n",
    "    \"\"\"\n",
    "    シグモイド関数\n",
    "\n",
    "    Attribute\n",
    "    ----------\n",
    "    self.A\n",
    "        活性化関数の入力\n",
    "    \"\"\"\n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        Z = 1 / (1+np.exp(-A))\n",
    "        \n",
    "        return Z\n",
    "\n",
    "    def backward(self, dZ):\n",
    "        dA = dZ * ((1 - self.A) * (self.A))\n",
    "\n",
    "        return dA\n",
    "\n",
    "class Tanh:\n",
    "    \"\"\"\n",
    "    tanh関数\n",
    "\n",
    "    Attribute\n",
    "    ----------\n",
    "    self.A\n",
    "        活性化関数の入力\n",
    "    \"\"\"\n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        Z = np.exp(A) / np.sum(np.exp(A), axis=1).reshape(-1, 1)\n",
    "        \n",
    "        return Z\n",
    "\n",
    "    def backward(self, dZ):\n",
    "        dA = dZ * (1 - self.A**2)\n",
    "\n",
    "        return dA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 発展的要素\n",
    "活性化関数や重みの初期値、最適化手法に関してこれまで見てきた以外のものを実装していきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題5】\n",
    "#### ReLUクラスの作成\n",
    "現在一般的に使われている活性化関数であるReLU（Rectified Linear Unit）をReLUクラスとして実装してください。\n",
    "\n",
    "\n",
    "ReLUは以下の数式です。\n",
    "\n",
    "$$\n",
    "% <![CDATA[\n",
    "f(x) = ReLU(x) = \\begin{cases}\n",
    "x  & \\text{if $x>0$,}\\\\\n",
    "0 & \\text{if $x\\leqq0$.}\n",
    "\\end{cases} %]]>\n",
    "$$\n",
    "\n",
    "$x$: ある特徴量。スカラー\n",
    "\n",
    "\n",
    "実装上は`np.maximum`を使い配列に対してまとめて計算が可能です。\n",
    "\n",
    "\n",
    "[numpy.maximum — NumPy v1.15 Manual](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.maximum.html)\n",
    "\n",
    "\n",
    "一方、バックプロパゲーションのための$x$に関する$f(x)$の微分は以下のようになります。\n",
    "\n",
    "$$\n",
    "% <![CDATA[\n",
    "\\frac{\\partial f(x)}{\\partial x} = \\begin{cases}\n",
    "1  & \\text{if $x>0$,}\\\\\n",
    "0 & \\text{if $x\\leqq0$.}\n",
    "\\end{cases} %]]>\n",
    "$$\n",
    "\n",
    "数学的には微分可能ではないですが、$x=0$のとき$0$とすることで対応しています。\n",
    "\n",
    "\n",
    "フォワード時の$x$の正負により、勾配を逆伝播するかどうかが決まるということになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    \"\"\"\n",
    "    ReLU関数\n",
    "\n",
    "    Attribute\n",
    "    ----------\n",
    "    self.A\n",
    "        活性化関数の入力\n",
    "    \"\"\"\n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        Z = np.maximum(0, A)\n",
    "        \n",
    "        return Z\n",
    "\n",
    "    def backward(self, dZ):\n",
    "        dA = dZ * np.where(self.A > 0, 1, 0)\n",
    "\n",
    "        return dA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題6】\n",
    "#### 重みの初期値\n",
    "ここまでは重みやバイアスの初期値は単純にガウス分布で、標準偏差をハイパーパラメータとして扱ってきました。しかし、どのような値にすると良いかが知られています。シグモイド関数やハイパボリックタンジェント関数のときは Xavierの初期値 （またはGlorotの初期値）、ReLUのときは Heの初期値 が使われます。\n",
    "\n",
    "\n",
    "XavierInitializerクラスと、HeInitializerクラスを作成してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xavierの初期値\n",
    "Xavierの初期値における標準偏差$\\sigma$は次の式で求められます。\n",
    "\n",
    "$$\n",
    "\\sigma = \\frac{1}{\\sqrt{n}}\n",
    "$$\n",
    "\n",
    "$n$ : 前の層のノード数\n",
    "\n",
    "\n",
    "**《論文》**\n",
    "\n",
    "\n",
    "[Glorot, X., & Bengio, Y. (n.d.). Understanding the difficulty of training deep feedforward neural networks.](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)\n",
    "\n",
    "\n",
    "### Heの初期値\n",
    "Heの初期値における標準偏差$\\sigma$は次の式で求められます。\n",
    "\n",
    "$$\n",
    "\\sigma = \\sqrt{\\frac{2}{n}}\n",
    "$$\n",
    "\n",
    "$n$ : 前の層のノード数\n",
    "\n",
    "\n",
    "**《論文》**\n",
    "\n",
    "\n",
    "[He, K., Zhang, X., Ren, S., & Sun, J. (2015). Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification.](https://arxiv.org/pdf/1502.01852.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XavierInitializer:\n",
    "    \"\"\"\n",
    "    Xavierの初期値\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    self.sigma\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1):\n",
    "        self.sigma = 1 / np.sqrt(n_nodes1)\n",
    "    \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "          重み\n",
    "        \"\"\"\n",
    "        W = self.sigma * np.random.randn(self.n_nodes1, self.n_nodes2)\n",
    "        \n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "          バイアス\n",
    "        \"\"\"\n",
    "        B = np.zeros((n_nodes2,))\n",
    "\n",
    "        return B\n",
    "\n",
    "class HeInitializer:\n",
    "    \"\"\"\n",
    "    Heの初期値\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    self.sigma\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1):\n",
    "        self.sigma = np.sqrt(2/n_nodes1)\n",
    "    \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "          重み\n",
    "        \"\"\"\n",
    "        W = self.sigma * np.random.randn(self.n_nodes1, self.n_nodes2)\n",
    "        \n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "          バイアス\n",
    "        \"\"\"\n",
    "        B = np.zeros((n_nodes2,))\n",
    "\n",
    "        return B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題7】\n",
    "#### 最適化手法\n",
    "学習率は学習過程で変化させていく方法が一般的です。基本的な手法である**AdaGrad**のクラスを作成してください。\n",
    "\n",
    "\n",
    "まず、これまで使ってきたSGDを確認します。\n",
    "\n",
    "$$\n",
    "W_i^{\\prime} = W_i - \\alpha E(\\frac{\\partial L}{\\partial W_i}) \\\\\n",
    "B_i^{\\prime} = B_i - \\alpha E(\\frac{\\partial L}{\\partial B_i})\n",
    "$$\n",
    "\n",
    "$\\alpha$ : 学習率（層ごとに変えることも可能だが、基本的には全て同じとする）\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{\\partial W_i}$ : $W_i$に関する損失$L$の勾配\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{\\partial B_i}$ : $B_i$に関する損失$L$の勾配\n",
    "\n",
    "\n",
    "$E()$ : ミニバッチ方向にベクトルの平均を計算\n",
    "\n",
    "\n",
    "続いて、AdaGradです。バイアスの数式は省略しますが、重みと同様のことをします。\n",
    "\n",
    "\n",
    "更新された分だけその重みに対する学習率を徐々に下げていきます。イテレーションごとの勾配の二乗和$H$を保存しておき、その分だけ学習率を小さくします。\n",
    "\n",
    "\n",
    "学習率は重み一つひとつに対して異なることになります。\n",
    "\n",
    "$$\n",
    "H_i^{\\prime} = H_i+E(\\frac{\\partial L}{\\partial W_i})×E(\\frac{\\partial L}{\\partial W_i})\\\\\n",
    "W_i^{\\prime} = W_i - \\alpha \\frac{1}{\\sqrt{H_i^{\\prime} }} E(\\frac{\\partial L}{\\partial W_i}) \\\\\n",
    "$$\n",
    "\n",
    "$H_i$ : i層目に関して、前のイテレーションまでの勾配の二乗和（初期値は0）\n",
    "\n",
    "\n",
    "$H_i^{\\prime}$ : 更新した$H_i$\n",
    "\n",
    "\n",
    "**《論文》**\n",
    "\n",
    "\n",
    "[Duchi JDUCHI, J., & Singer, Y. (2011). Adaptive Subgradient Methods for Online Learning and Stochastic Optimization * Elad Hazan. Journal of Machine Learning Research (Vol. 12).](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    \"\"\"\n",
    "    AdaGrad法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, layer, dA):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "        # calc mean and partial\n",
    "        dB = np.sum(dA, axis=0)\n",
    "        dW = layer.Z_prev.T * dA\n",
    "        \n",
    "        dZ = dA * layer.W.T\n",
    "\n",
    "        # update HB, HW\n",
    "        layer.HB = layer.HB + dB**2\n",
    "        layer.HW = layer.HW + dW**2\n",
    "\n",
    "        # update W and B\n",
    "        layer.B = layer.B - self.lr*(1/np.sqrt(layer.HB))*dB\n",
    "        layer.W = layer.W - self.lr*(1/np.sqrt(layer.HW))*dW\n",
    "\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題8】\n",
    "#### クラスの完成\n",
    "任意の構成で学習と推定が行えるScratchDeepNeuralNetrowkClassifierクラスを完成させてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "class ScratchDeepNeuralNetworkClassifier:\n",
    "    \"\"\"\n",
    "    多層ニューラルネットワーク分類器\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    alpha : float\n",
    "        学習率\n",
    "    sigma : float\n",
    "        ガウス分布の標準偏差\n",
    "    batch_size : int\n",
    "        バッチのサイズ\n",
    "    epoch : int\n",
    "        エポック数\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    self.loss : list\n",
    "        交差エントロピー誤差（訓練データ）\n",
    "    self.val_loss : list\n",
    "        交差エントロピー誤差（バリデーションデータ)\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=0.01, sigma=0.01, batch_size=20, epoch=1, verbose=True, optimizer='SGD', acitivator='ReLU', n_nodes=[400, 200, 10]):\n",
    "        self.alpha = alpha\n",
    "        self.sigma = sigma\n",
    "        self.batch_size = batch_size\n",
    "        self.epoch = epoch\n",
    "        self.verbose = verbose\n",
    "        self.optimizer_name = optimizer\n",
    "        self.activator_name = acitivator\n",
    "        self.n_nodes = n_nodes\n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を学習する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証データの正解値\n",
    "        \"\"\"\n",
    "        # def optimizer\n",
    "        if self.optimizer_name == 'SGD':\n",
    "            optimizer = SGD(self.lr)\n",
    "        elif self.optimizer_name == 'AdaGrad':\n",
    "            optimizer = AdaGrad(self.lr)\n",
    "        else:\n",
    "            raise ValueError(f'no such optimizer named \"{self.optimizer_name}\"')\n",
    "\n",
    "        # def layers\n",
    "        self.FC = []\n",
    "        self.activation = []\n",
    "        self.n_features = [X.shape[1]]\n",
    "        \n",
    "        for i, (n_nodes1, n_nodes2) in enumerate(zip(self.n_features+self.n_nodes[:-1], self.n_nodes)):\n",
    "            if i+1 != len(self.n_nodes):\n",
    "                # def initializer / activator\n",
    "                if self.activator_name == 'ReLU':\n",
    "                    initializer = HeInitializer(n_nodes1)\n",
    "                    self.activation.append(ReLU())\n",
    "                elif self.activator_name == 'Sigmoid':\n",
    "                    initializer = XavierInitializer(n_nodes1)\n",
    "                    self.activation.append(Sigmoid())\n",
    "                elif self.activator_name == 'Tanh':\n",
    "                    initializer = XavierInitializer(n_nodes1)\n",
    "                    self.activation.append(Tanh())\n",
    "                else:\n",
    "                    raise ValueError(f'no such activator named \"{self.activator_name}\"')\n",
    "\n",
    "                # def layer\n",
    "                self.FC.append(FC(n_nodes1, n_nodes2, initializer, optimizer))\n",
    "            # last layer\n",
    "            else:\n",
    "                # def initializer\n",
    "                if self.activator_name == 'ReLU':\n",
    "                    initializer = HeInitializer(n_nodes1)\n",
    "                elif self.activator_name == 'Sigmoid':\n",
    "                    initializer = XavierInitializer(n_nodes1)\n",
    "                elif self.activator_name == 'Tanh':\n",
    "                    initializer = XavierInitializer(n_nodes1)\n",
    "                else:\n",
    "                    raise ValueError(f'no such activator named \"{self.activator_name}\"')\n",
    "\n",
    "                # def activator\n",
    "                self.activation.append(Softmax())\n",
    "\n",
    "                # def layer\n",
    "                self.FC.append(FC(n_nodes1, n_nodes2, initializer, optimizer))\n",
    "        \n",
    "        ## one-hot encoding\n",
    "        enc = OneHotEncoder(handle_unknown='ignore', sparse=False).fit(y[:, np.newaxis])\n",
    "        if y_val is not None:\n",
    "            y_val_one_hot = enc.transform(y_val[:, np.newaxis])\n",
    "\n",
    "        ## loss list\n",
    "        self.loss = []\n",
    "        self.val_loss = []\n",
    "\n",
    "        if self.verbose:\n",
    "            #verboseをTrueにした際は学習過程などを出力する\n",
    "            print('start learning')\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # learning\n",
    "        for e in range(self.epoch):\n",
    "            print(f'start epoch {e+1}')\n",
    "\n",
    "            ## mini_batch\n",
    "            get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch_size)\n",
    "\n",
    "            ## loss list\n",
    "            self.loss.append([])\n",
    "            self.val_loss.append([])\n",
    "\n",
    "            for i, (mini_X_train, mini_y_train) in enumerate(get_mini_batch):\n",
    "                ## one-hot encoding\n",
    "                mini_y_train_one_hot = enc.transform(mini_y_train[:, np.newaxis])\n",
    "                \n",
    "                ## forward propagation\n",
    "\n",
    "                ## append loss\n",
    "                            \n",
    "                ## update weight, bias\n",
    "\n",
    "                ## validation\n",
    "                if (X_val is not None) and (y_val is not None):\n",
    "                    ### prediction\n",
    "\n",
    "                    ### append loss\n",
    "\n",
    "                ## print progress\n",
    "                if self.verbose:\n",
    "                    print(f'\\r{i+1}/{len(get_mini_batch)} loop finished', end='')\n",
    "            if self.verbose:\n",
    "                print(' : Complete!!')\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "        print(f'Done! elapsed time: {elapsed_time:.5f}s')\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を使い推定する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            推定結果\n",
    "        \"\"\"\n",
    "\n",
    "        return np.argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題9】\n",
    "#### 学習と推定\n",
    "層の数や活性化関数を変えたいくつかのネットワークを作成してください。そして、MNISTのデータを学習・推定し、Accuracyを計算してください。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}