{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint\n",
    "## 深層学習スクラッチ 畳み込みニューラルネットワーク1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.このSprintについて\n",
    "\n",
    "### Sprintの目的\n",
    "スクラッチを通してCNNの基礎を理解する\n",
    "\n",
    "### どのように学ぶか\n",
    "スクラッチで1次元用畳み込みニューラルネットワークを実装した後、学習と検証を行なっていきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1次元の畳み込みニューラルネットワークスクラッチ\n",
    "\n",
    "**畳み込みニューラルネットワーク（CNN）**のクラスをスクラッチで作成していきます。NumPyなど最低限のライブラリのみを使いアルゴリズムを実装していきます。\n",
    "\n",
    "\n",
    "このSprintでは1次元の**畳み込み層**を作成し、畳み込みの基礎を理解することを目指します。次のSprintでは2次元畳み込み層とプーリング層を作成することで、一般的に画像に対して利用されるCNNを完成させます。\n",
    "\n",
    "\n",
    "クラスの名前はScratch1dCNNClassifierとしてください。クラスの構造などは前のSprintで作成したScratchDeepNeuralNetrowkClassifierを参考にしてください。\n",
    "\n",
    "\n",
    "### 1次元畳み込み層とは\n",
    "CNNでは画像に対しての2次元畳み込み層が定番ですが、ここでは理解しやすくするためにまずは1次元畳み込み層を実装します。1次元畳み込みは実用上は自然言語や波形データなどの**系列データ**で使われることが多いです。\n",
    "\n",
    "\n",
    "畳み込みは任意の次元に対して考えることができ、立体データに対しての3次元畳み込みまではフレームワークで一般的に用意されています。\n",
    "\n",
    "\n",
    "### データセットの用意\n",
    "検証には引き続きMNISTデータセットを使用します。1次元畳み込みでは全結合のニューラルネットワークと同様に平滑化されたものを入力します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題1】\n",
    "#### チャンネル数を1に限定した1次元畳み込み層クラスの作成\n",
    "チャンネル数を1に限定した1次元畳み込み層のクラスSimpleConv1dを作成してください。基本構造は前のSprintで作成した全結合層のFCクラスと同じになります。なお、重みの初期化に関するクラスは必要に応じて作り変えてください。Xavierの初期値などを使う点は全結合層と同様です。\n",
    "\n",
    "\n",
    "ここでは**パディング**は考えず、**ストライド**も1に固定します。また、複数のデータを同時に処理することも考えなくて良く、バッチサイズは1のみに対応してください。この部分の拡張はアドバンス課題とします。\n",
    "\n",
    "\n",
    "フォワードプロパゲーションの数式は以下のようになります。\n",
    "\n",
    "$$\n",
    "a_i = \\sum_{s=0}^{F-1}x_{(i+s)}w_s+b\n",
    "$$\n",
    "\n",
    "$a_i$ : 出力される配列のi番目の値\n",
    "\n",
    "\n",
    "$F$ : フィルタのサイズ\n",
    "\n",
    "\n",
    "$x_{(i+s)}$ : 入力の配列の(i+s)番目の値\n",
    "\n",
    "\n",
    "$w_s$ : 重みの配列のs番目の値\n",
    "\n",
    "\n",
    "$b$ : バイアス項\n",
    "\n",
    "\n",
    "全てスカラーです。\n",
    "\n",
    "\n",
    "次に更新式です。ここがAdaGradなどに置き換えられる点は全結合層と同様です。\n",
    "\n",
    "$$\n",
    "w_s^{\\prime} = w_s - \\alpha \\frac{\\partial L}{\\partial w_s} \\\\\n",
    "b^{\\prime} = b - \\alpha \\frac{\\partial L}{\\partial b}\n",
    "$$\n",
    "\n",
    "$\\alpha$ : 学習率\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{\\partial w_s}$ : $w_s$に関する損失$L$の勾配\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{\\partial b}$ : $b$に関する損失$L$の勾配\n",
    "\n",
    "\n",
    "勾配$\\frac{\\partial L}{\\partial w_s}$や$\\frac{\\partial L}{\\partial b}$を求めるためのバックプロパゲーションの数式が以下です。\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_s} = \\sum_{i=0}^{N_{out}-1} \\frac{\\partial L}{\\partial a_i}x_{(i+s)}\\\\\n",
    "\\frac{\\partial L}{\\partial b} = \\sum_{i=0}^{N_{out}-1} \\frac{\\partial L}{\\partial a_i}\n",
    "$$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial a_i}$ : 勾配の配列のi番目の値\n",
    "\n",
    "\n",
    "$N_{out}$ : 出力のサイズ\n",
    "\n",
    "\n",
    "前の層に流す誤差の数式は以下です。\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x_j} = \\sum_{s=0}^{F-1} \\frac{\\partial L}{\\partial a_{(j-s)}}w_s\n",
    "$$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial x_j}$ : 前の層に流す誤差の配列のj番目の値\n",
    "\n",
    "\n",
    "ただし、$j−s<0$または$j−s>N_{out}−1$のとき$\\frac{\\partial L}{\\partial a_{(j-s)}}=0$です。\n",
    "\n",
    "\n",
    "全結合層との大きな違いは、重みが複数の特徴量に対して共有されていることです。この場合は共有されている分の誤差を全て足すことで勾配を求めます。計算グラフ上での分岐はバックプロパゲーションの際に誤差の足し算をすれば良いことになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "# import modules\n",
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConv1d:\n",
    "    \"\"\"\n",
    "    チャンネル数を1に限定した1次元畳み込み層\n",
    "    (パディングなし　ストライド1)\n",
    "    \"\"\"\n",
    "    def __init__(self, filter_size, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.W = initializer.W(filter_size)\n",
    "        self.B = initializer.B()\n",
    "        pass\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        \"\"\"\n",
    "        self.X_ = X\n",
    "\n",
    "        A = np.zeros((X.shape[0]-self.W.shape[0]+1,))\n",
    "        for i in range(A.shape[0]):\n",
    "            A[i] = np.sum(X[i:i+self.W.shape[0]]*self.W) + self.B\n",
    "\n",
    "        return A\n",
    "\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        \"\"\"\n",
    "        # 更新\n",
    "        dX = self.optimizer.update(self, dA)\n",
    "\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDConv:\n",
    "    \"\"\"\n",
    "    勾配降下法（畳み込み）\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, layer, dA):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        \"\"\"\n",
    "        dB = np.sum(dA)\n",
    "        dW = np.zeros(layer.W.shape[0])\n",
    "        for s in range(dW.shape[0]):\n",
    "            dW[s] = np.dot(layer.X_[s:s+dA.shape[0]], dA)\n",
    "        \n",
    "        dX = np.zeros(layer.input_size)\n",
    "        dA_padding = np.pad(dA, (layer.W.shape[0]-1, max((0, layer.input_size-dA.shape[0]))), mode='constant')\n",
    "        for s in range(dX.shape[0]):\n",
    "            dX[s] = np.sum(dA_padding[s:s+layer.W.shape[0]]*layer.W[::-1])\n",
    "\n",
    "        layer.B = layer.B - self.lr*dB\n",
    "        layer.W = layer.W - self.lr*dW\n",
    "\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題2】\n",
    "#### 1次元畳み込み後の出力サイズの計算\n",
    "畳み込みを行うと特徴量の数が変化します。どのように変化するかは以下の数式から求められます。パディングやストライドも含めています。この計算を行う関数を作成してください。\n",
    "\n",
    "$$\n",
    "N_{out} =  \\frac{N_{in}+2P-F}{S} + 1\\\\\n",
    "$$\n",
    "\n",
    "$N_{out}$ : 出力のサイズ（特徴量の数）\n",
    "\n",
    "\n",
    "$N_{in}$ : 入力のサイズ（特徴量の数）\n",
    "\n",
    "\n",
    "$P$ : ある方向へのパディングの数\n",
    "\n",
    "\n",
    "$F$ : フィルタのサイズ\n",
    "\n",
    "\n",
    "$S$ : ストライドのサイズ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_output_size(input_size, padding_size, filter_size, stride_size):\n",
    "    n_out = ((input_size+2*padding_size-filter_size) / stride_size) + 1\n",
    "    return int(n_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題3】\n",
    "#### 小さな配列での1次元畳み込み層の実験\n",
    "次に示す小さな配列でフォワードプロパゲーションとバックプロパゲーションが正しく行えているか確認してください。\n",
    "\n",
    "\n",
    "入力x、重みw、バイアスbを次のようにします。\n",
    "\n",
    "```python\n",
    "x = np.array([1,2,3,4])\n",
    "w = np.array([3, 5, 7])\n",
    "b = np.array([1])\n",
    "```\n",
    "\n",
    "フォワードプロパゲーションをすると出力は次のようになります。\n",
    "\n",
    "```python\n",
    "a = np.array([35, 50])\n",
    "```\n",
    "\n",
    "次にバックプロパゲーションを考えます。誤差は次のようであったとします。\n",
    "\n",
    "```python\n",
    "delta_a = np.array([10, 20])\n",
    "```\n",
    "\n",
    "バックプロパゲーションをすると次のような値になります。\n",
    "\n",
    "``` python\n",
    "delta_b = np.array([30])\n",
    "delta_w = np.array([50, 80, 110])\n",
    "delta_x = np.array([30, 110, 170, 140])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[35. 50.]\n"
    }
   ],
   "source": [
    "# forward\n",
    "x = np.array([1, 2, 3, 4])\n",
    "w = np.array([3, 5, 7])\n",
    "b = np.array([1])\n",
    "\n",
    "a = np.zeros((x.shape[0]-w.shape[0]+1,))\n",
    "for i in range(a.shape[0]):\n",
    "    a[i] = np.sum(x[i:i+w.shape[0]]*w) + b\n",
    "\n",
    "print(a)\n",
    "## a = np.array([35, 50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "30\n[ 50.  80. 110.]\n[ 30. 110. 170. 140.]\n"
    }
   ],
   "source": [
    "# backward\n",
    "delta_a = np.array([10, 20])\n",
    "\n",
    "delta_b = np.sum(delta_a)\n",
    "delta_w = np.zeros(w.shape[0])\n",
    "for s in range(delta_w.shape[0]):\n",
    "    delta_w[s] = np.dot(x[s:s+delta_a.shape[0]], delta_a)\n",
    "\n",
    "delta_x = np.zeros(x.shape[0])\n",
    "delta_a_padding = np.pad(delta_a, (w.shape[0]-1, max((0, x.shape[0]-delta_a.shape[0]))), mode='constant')\n",
    "for s in range(delta_x.shape[0]):\n",
    "    delta_x[s] = np.sum(delta_a_padding[s:s+w.shape[0]]*w[::-1])\n",
    "\n",
    "print(delta_b)\n",
    "## delta_b = np.array([30])\n",
    "print(delta_w)\n",
    "## delta_w = np.array([50, 80, 110])\n",
    "print(delta_x)\n",
    "## delta_x = np.array([30, 110, 170, 140])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 実装上の工夫\n",
    "畳み込みを実装する場合は、まずはfor文を重ねていく形で構いません。しかし、できるだけ計算は効率化させたいため、以下の式を一度に計算する方法を考えることにします。\n",
    "\n",
    "$$\n",
    "a_i = \\sum_{s=0}^{F-1}x_{(i+s)}w_s+b\n",
    "$$\n",
    "\n",
    "バイアス項は単純な足し算のため、重みの部分を見ます。\n",
    "\n",
    "$$\n",
    "\\sum_{s=0}^{F-1}x_{(i+s)}w_s\n",
    "$$\n",
    "\n",
    "これは、xの一部を取り出した配列とwの配列の内積です。具体的な状況を考えると、以下のようなコードで計算できます。この例では流れを分かりやすくするために、各要素同士でアダマール積を計算してから合計を計算しています。これは結果的に内積と同様です。\n",
    "\n",
    "```python\n",
    "x = np.array([1, 2, 3, 4])\n",
    "w = np.array([3, 5, 7])\n",
    "a = np.empty((2, 3))\n",
    "indexes0 = np.array([0, 1, 2]).astype(np.int)\n",
    "indexes1 = np.array([1, 2, 3]).astype(np.int)\n",
    "a[0] = x[indexes0]*w # x[indexes0]は([1, 2, 3])である\n",
    "a[1] = x[indexes1]*w # x[indexes1]は([2, 3, 4])である\n",
    "a = a.sum(axis=1)\n",
    "````\n",
    "\n",
    "ndarrayは配列を使ったインデックス指定ができることを利用した方法です。\n",
    "\n",
    "\n",
    "また、二次元配列を使えば一次元配列から二次元配列が取り出せます。\n",
    "\n",
    "```python\n",
    "x = np.array([1, 2, 3, 4])\n",
    "indexes = np.array([[0, 1, 2], [1, 2, 3]]).astype(np.int)\n",
    "print(x[indexes]) # ([[1, 2, 3], [2, 3, 4]])\n",
    "```\n",
    "\n",
    "このこととブロードキャストなどをうまく組み合わせることで、一度にまとめて計算することも可能です。\n",
    "\n",
    "\n",
    "畳み込みの計算方法に正解はないので、自分なりに効率化していってください。\n",
    "\n",
    "\n",
    "**《参考》**\n",
    "\n",
    "\n",
    "以下のページのInteger array indexingの部分がこの方法についての記述です。\n",
    "\n",
    "\n",
    "[Indexing — NumPy v1.17 Manual](https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題4】\n",
    "#### チャンネル数を限定しない1次元畳み込み層クラスの作成\n",
    "チャンネル数を1に限定しない1次元畳み込み層のクラスConv1dを作成してください。\n",
    "\n",
    "\n",
    "例えば以下のようなx, w, bがあった場合は、\n",
    "\n",
    "```python\n",
    "x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]]) # shape(2, 4)で、（入力チャンネル数、特徴量数）である。\n",
    "w = np.ones((3, 2, 3)) # 例の簡略化のため全て1とする。(出力チャンネル数、入力チャンネル数、フィルタサイズ)である。\n",
    "b = np.array([1, 2, 3]) # （出力チャンネル数）\n",
    "```\n",
    "\n",
    "出力は次のようになります。\n",
    "\n",
    "```python\n",
    "a = np.array([[16, 22], [17, 23], [18, 24]]) # shape(3, 2)で、（出力チャンネル数、入力チャンネル数）である。\n",
    "```\n",
    "\n",
    "入力が2チャンネル、出力が3チャンネルの例です。計算グラフを書いた上で、バックプロパゲーションも手計算で考えてみましょう。計算グラフの中には和と積しか登場しないので、微分を新たに考える必要はありません。\n",
    "\n",
    "\n",
    "**《補足》**\n",
    "\n",
    "\n",
    "チャンネル数を加える場合、配列をどういう順番にするかという問題があります。**(バッチサイズ、チャンネル数、特徴量数)**または**(バッチサイズ、特徴量数、チャンネル数)**が一般的で、ライブラリによって順番は異なっています。（切り替えて使用できるものもあります）\n",
    "\n",
    "\n",
    "今回のスクラッチでは自身の実装上どちらが効率的かを考えて選んでください。上記の例ではバッチサイズは考えておらず、**(チャンネル数、特徴量数)**です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1d:\n",
    "    \"\"\"\n",
    "    チャンネル数を限定しない1次元畳み込み層\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, input_channel, output_channel, filter_size, padding_size=0, stride=1):\n",
    "        self.optimizer = optimizer        \n",
    "        # 初期化\n",
    "        self.W = np.random.randn(output_channel, input_channel, filter_size)\n",
    "        self.B = np.random.randn(output_channel)\n",
    "        self.filter_size = filter_size\n",
    "        # kwargs\n",
    "        self.padding_size = padding_size\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        \"\"\"\n",
    "        # padding\n",
    "        # axisは(バッチサイズ, チャンネル数, 特徴量)とする\n",
    "        # 1チャンネルでも(m, 1, n)になるように調整する必要あり\n",
    "        if self.padding_size > 0:\n",
    "            # shapeが(m, n)で入ってきた場合の処理\n",
    "            if len(X.shape) == 2: X = X[:, np.newaxis, :]\n",
    "            X = np.pad(X, ((0, 0), (0, 0), (self.padding_size, self.padding_size)), mode='constant')\n",
    "        \n",
    "        # 逆伝搬用\n",
    "        self.X_ = X\n",
    "        # 特徴量のサイズ\n",
    "        self.input_size = X.shape[-1]\n",
    "        # 出力のサイズ\n",
    "        self.output_size = calc_output_size(self.input_size,\n",
    "                                            self.padding_size,\n",
    "                                            self.filter_size,\n",
    "                                            self.stride)\n",
    "\n",
    "        # self.Wに対応するXのインデックス\n",
    "        self.windows = []\n",
    "        for i in range(0, X.shape[-1]-self.W.shape[-1]+1, self.stride):\n",
    "            self.windows.append(list(range(i, i+self.W.shape[-1])))\n",
    "        self.windows = np.array(self.windows)\n",
    "        \n",
    "        A = np.zeros((X.shape[0], self.W.shape[0], self.output_size))\n",
    "        # batch内でループを行う\n",
    "        for sample_num, sample in enumerate(X):\n",
    "            ## X_tmp: バッチ内の1データに対応する\n",
    "            X_tmp = sample\n",
    "            if len(X_tmp.shape) < 2:\n",
    "                X_tmp = X_tmp.reshape(1, -1)\n",
    "            # output channel\n",
    "            for output_channel in range(self.W.shape[0]):\n",
    "                # input channel\n",
    "                for input_channel in range(X_tmp.shape[0]):\n",
    "                    A[sample_num][output_channel] += np.sum(X_tmp[input_channel][self.windows]*self.W[output_channel,input_channel], axis=1)\n",
    "\n",
    "        A += self.B[np.newaxis, :, np.newaxis]\n",
    "\n",
    "        return A\n",
    "\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        \"\"\"\n",
    "        # 更新\n",
    "        dX = self.optimizer.update(self, dA)\n",
    "\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[1 2 3 4]\n [2 3 4 5]]\n[[[1. 1. 1.]\n  [1. 1. 1.]]\n\n [[1. 1. 1.]\n  [1. 1. 1.]]\n\n [[1. 1. 1.]\n  [1. 1. 1.]]]\n[1 2 3]\n\n[[0 1 2]\n [1 2 3]]\n\n[[15. 21.]\n [15. 21.]\n [15. 21.]]\n[[1 2 3]\n [2 3 4]]\n[1. 1. 1.]\n[[16. 22.]\n [17. 23.]\n [18. 24.]]\n(3, 2)\n"
    }
   ],
   "source": [
    "x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]]) # shape(2, 4)で、（入力チャンネル数、特徴量数）である。\n",
    "w = np.ones((3, 2, 3)) # 例の簡略化のため全て1とする。(出力チャンネル数、入力チャンネル数、フィルタサイズ)である。\n",
    "b = np.array([1, 2, 3]) # （出力チャンネル数）\n",
    "print(x)\n",
    "print(w)\n",
    "print(b, end='\\n\\n')\n",
    "\n",
    "# self.Wに対応するXのインデックス\n",
    "windows = np.array([list(range(i, i+w.shape[2])) for i in range(x.shape[1]-w.shape[-1]+1)])\n",
    "print(windows, end='\\n\\n')\n",
    "\n",
    "#for i,xch in enumerate(x):\n",
    "#   print(w[:, i, :], end='\\n\\n')\n",
    "#   print(xch[windows])\n",
    "#   print(np.array([np.sum(w[:, i, :][j]*xch[windows]) for j in range(w.shape[0])])[:, np.newaxis])\n",
    "\n",
    "# 入力チャンネル単位でself.Wを適用してから結合\n",
    "#a = np.concatenate([np.array([np.sum(X_ch[windows]*w[:, i, :][j]) for j in range(w.shape[0])])[:, np.newaxis] for i, X_ch in enumerate(x)], axis=1) + b[:, np.newaxis]\n",
    "\n",
    "output_size = 2\n",
    "tmp_A = np.zeros((w.shape[0], output_size))\n",
    "# output channel\n",
    "for output_channel in range(w.shape[0]):\n",
    "    # input channel\n",
    "    for input_channel in range(x.shape[0]):\n",
    "            tmp_A[output_channel] += np.sum(x[input_channel][windows]*w[output_channel, input_channel], axis=1)\n",
    "            pass\n",
    "print(tmp_A)\n",
    "\n",
    "\n",
    "print(x[0][windows])\n",
    "print(w[0, 0])\n",
    "#print(np.sum(x[0][windows]*w[0, 0], axis=1))\n",
    "\n",
    "#print(np.dot(x[0][windows], w[0]))\n",
    "a = tmp_A + b[:, np.newaxis]\n",
    "print(a)\n",
    "print(a.shape)\n",
    "# a = np.array([[16, 22], [17, 23], [18, 24]]) # shape(3, 2)で、（出力チャンネル数、特徴量数）である"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**以下問題5, 6, 7についても`Conv1d`クラスを編集する。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題5】\n",
    "#### （アドバンス課題）パディングの実装\n",
    "畳み込み層にパディングの機能を加えてください。1次元配列の場合、前後にn個特徴量を増やせるようにしてください。\n",
    "\n",
    "\n",
    "最も単純なパディングは全て0で埋める ゼロパディング であり、CNNでは一般的です。他に端の値を繰り返す方法などもあります。\n",
    "\n",
    "\n",
    "フレームワークによっては、元の入力のサイズを保つようにという指定をすることができます。この機能も持たせておくと便利です。なお、NumPyにはパディングの関数が存在します。\n",
    "\n",
    "\n",
    "[numpy.pad — NumPy v1.17 Manual](https://docs.scipy.org/doc/numpy/reference/generated/numpy.pad.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# padding\n",
    "if self.padding_size > 0:\n",
    "    X = np.pad(X, [self.padding_size, self.padding_size], mode='constant')    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題6】\n",
    "#### （アドバンス課題）ミニバッチへの対応\n",
    "ここまでの課題はバッチサイズ1で良いとしてきました。しかし、実際は全結合層同様にミニバッチ学習が行われます。Conv1dクラスを複数のデータが同時に計算できるように変更してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[[1 2 3]\n  [2 3 4]]\n\n [[3 4 5]\n  [4 5 6]]\n\n [[5 6 7]\n  [6 7 8]]]\n[[[0 1 2 3 0]\n  [0 2 3 4 0]]\n\n [[0 3 4 5 0]\n  [0 4 5 6 0]]\n\n [[0 5 6 7 0]\n  [0 6 7 8 0]]]\n[[[1 2 3 4]]\n\n [[2 3 4 5]]]\n[[[[1 2 3]\n   [2 3 4]]]\n\n\n [[[3 4 5]\n   [4 5 6]]]\n\n\n [[[5 6 7]\n   [6 7 8]]]]\n"
    }
   ],
   "source": [
    "y =  np.array([[[1,2,3], [2,3,4]], [[3,4,5], [4,5,6]], [[5,6,7], [6,7,8]]])\n",
    "print(y)\n",
    "print(np.pad(y, ((0, 0), (0, 0), (1, 1)), mode='constant'))\n",
    "print(x[:, np.newaxis, :])\n",
    "print(y[:, np.newaxis, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題7】\n",
    "#### （アドバンス課題）任意のストライド数\n",
    "ストライドは1限定の実装をしてきましたが、任意のストライド数に対応できるようにしてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# self.Wに対応するXのインデックス\n",
    "self.windows = []\n",
    "for i in range(0, X.shape[1]-self.W.shape[2]+1, self.stride):\n",
    "    self.windows.append(list(range(i, i+self.W.shape[2])))\n",
    "self.windows = np.array(self.windows)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "問5,6,7の拡張に対応したバックプロパゲーションを実装する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class SGDConv:\n",
    "    \"\"\"\n",
    "    勾配降下法（畳み込み）\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, layer, dA):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        \"\"\"\n",
    "        dB = np.sum(dA)\n",
    "        dW = np.zeros(layer.W.shape[0])\n",
    "        for s in range(dW.shape[0]):\n",
    "            dW[s] = np.dot(layer.X_[s:s+dA.shape[0]], dA)\n",
    "        \n",
    "        dX = np.zeros(layer.input_size)\n",
    "        dA_padding = np.pad(dA, (layer.W.shape[0]-1, max((0, layer.input_size-dA.shape[0]))), mode='constant')\n",
    "        for s in range(dX.shape[0]):\n",
    "            dX[s] = np.sum(dA_padding[s:s+layer.W.shape[0]]*layer.W[::-1])\n",
    "\n",
    "        layer.B = layer.B - self.lr*dB\n",
    "        layer.W = layer.W - self.lr*dW\n",
    "\n",
    "        return dX\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDConvMulti:\n",
    "    \"\"\"\n",
    "    勾配降下法（畳み込み）\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, layer, dA):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        dA.shape -> (batch_size, 出力チャンネル数, output_size)\n",
    "        \"\"\"\n",
    "        output_size = calc_output_size(layer.input_size,\n",
    "                                        layer.padding_size,\n",
    "                                        layer.filter_size,\n",
    "                                        layer.stride)    \n",
    "        # dB\n",
    "        ## output_sizeの軸に対して合計する\n",
    "        ## batch_size方向に平均する\n",
    "        dB = np.mean(np.sum(dA, axis=-1), axis=0)\n",
    "        \n",
    "        # dW (出力チャンネル数、入力チャンネル数、フィルタサイズ)にしたい\n",
    "        dW = np.zeros(layer.W.shape)\n",
    "        for output_channel in range(dW.shape[0]):\n",
    "            dA_tmp = dA[:, output_channel]\n",
    "            for input_channel in range(dW.shape[1]):\n",
    "                X_tmp = layer.X_[:, input_channel]\n",
    "                dW[output_channel, input_channel] = [np.mean(np.dot(X_tmp, dA_tmp), axis=0) for s in range(dW.shape[-1])]\n",
    "\n",
    "        # dX (batch_size, 出力チャンネル数, output_size)\n",
    "        dX = np.zeros(layer.input_size)\n",
    "        for batch_num in range(dX.shape[0]):\n",
    "            for output_channel in range(dX.shape[1]):\n",
    "                dA_tmp = dA[batch_num, output_channel]\n",
    "                W_tmp = np.sum(layer.W[output_channel], axis=0)\n",
    "                dX_tmp = np.zeros(dA_tmp.shape[0])\n",
    "                for s in range(W_tmp.shape[0]):\n",
    "                    dA_tmp_reverse = dA_tmp[::-1][s:s+W_tmp_shape[0]]\n",
    "                    dX_tmp[s] = np.dot(dA_tmp_reverse, W_tmp[0:dA_tmp_reverse.shape[0]])\n",
    "                \n",
    "                \n",
    "                dX[batch_num, output_channel] = dX_tmp\n",
    "\n",
    "        dA_padding = np.pad(dA, (layer.W.shape[0]-1, max((0, layer.input_size-dA.shape[0]))), mode='constant')\n",
    "        for s in range(dX.shape[0]):\n",
    "            dX[s] = np.sum(dA_padding[s:s+layer.W.shape[0]]*layer.W[::-1])\n",
    "\n",
    "        layer.B = layer.B - self.lr*dB\n",
    "        layer.W = layer.W - self.lr*dW\n",
    "\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題8】\n",
    "#### 学習と推定\n",
    "これまで使ってきたニューラルネットワークの全結合層の一部をConv1dに置き換えてMNISTを学習・推定し、Accuracyを計算してください。\n",
    "\n",
    "\n",
    "出力層だけは全結合層をそのまま使ってください。ただし、チャンネルが複数ある状態では全結合層への入力は行えません。その段階でのチャンネルは1になるようにするか、**平滑化**を行なってください。\n",
    "\n",
    "\n",
    "画像に対しての1次元畳み込みは実用上は行わないことのため、精度は問いません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "活性化関数: ReLU, 最適化: AdaGradに限定する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchDeepNeuralNetworkClassifier:\n",
    "    \"\"\"\n",
    "    多層ニューラルネットワーク分類器\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    alpha : float\n",
    "        学習率\n",
    "    sigma : float\n",
    "        ガウス分布の標準偏差\n",
    "    batch_size : int\n",
    "        バッチのサイズ\n",
    "    epoch : int\n",
    "        エポック数\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    self.loss : list\n",
    "        交差エントロピー誤差（訓練データ）\n",
    "    self.val_loss : list\n",
    "        交差エントロピー誤差（バリデーションデータ)\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=0.01, sigma=0.01, batch_size=20, epoch=1, verbose=True, n_nodes=[400, 200, 10]):\n",
    "        self.alpha = alpha\n",
    "        self.sigma = sigma\n",
    "        self.batch_size = batch_size\n",
    "        self.epoch = epoch\n",
    "        self.verbose = verbose\n",
    "        self.n_nodes = n_nodes\n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を学習する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証データの正解値\n",
    "        \"\"\"\n",
    "        # def optimizer\n",
    "        optimizer = AdaGrad(self.alpha)\n",
    "\n",
    "        # def layers\n",
    "        self.layers = []\n",
    "        self.activations = []\n",
    "        self.n_features = [X.shape[1]]\n",
    "\n",
    "        for i, (n_nodes1, n_nodes2) in enumerate(zip([1]+self.n_nodes[:-1], self.n_nodes)):\n",
    "            if i+1 != len(self.n_nodes):\n",
    "                # def initializer / activator\n",
    "                initializer = HeInitializer(n_nodes1)\n",
    "                self.activations.append(ReLU())\n",
    "                self.layers.append(Conv1d(SGDConvMulti(self.alpha), n_nodes1, n_nodes2, 5))\n",
    "\n",
    "            # last layer\n",
    "            else:\n",
    "                # def initializer\n",
    "                initializer = HeInitializer(n_nodes1)\n",
    "                \n",
    "                # def activator\n",
    "                self.activations.append(Softmax())\n",
    "\n",
    "                # def layer\n",
    "                self.layers.append(FC(772, n_nodes2, initializer, optimizer))\n",
    "        \n",
    "        ## one-hot encoding\n",
    "        enc = OneHotEncoder(handle_unknown='ignore', sparse=False).fit(y[:, np.newaxis])\n",
    "        if y_val is not None:\n",
    "            y_val_one_hot = enc.transform(y_val[:, np.newaxis])\n",
    "\n",
    "        ## loss list\n",
    "        self.loss = []\n",
    "        self.val_loss = []\n",
    "\n",
    "        if self.verbose:\n",
    "            #verboseをTrueにした際は学習過程などを出力する\n",
    "            print('start learning')\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # learning\n",
    "        for e in range(self.epoch):\n",
    "            print(f'start epoch {e+1}')\n",
    "\n",
    "            ## mini_batch\n",
    "            get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch_size, seed=e)\n",
    "\n",
    "            ## loss list\n",
    "            self.loss.append([])\n",
    "\n",
    "            for i, (mini_X_train, mini_y_train) in enumerate(get_mini_batch):\n",
    "                ## one-hot encoding\n",
    "                mini_y_train_one_hot = enc.transform(mini_y_train[:, np.newaxis])\n",
    "                \n",
    "                ## forward propagation\n",
    "                A = []\n",
    "                Z = []\n",
    "                for j in range(len(self.layers)):\n",
    "                    ## calc A\n",
    "                    if j == 0:\n",
    "                        A.append(self.layers[j].forward(mini_X_train))\n",
    "                    else:\n",
    "                        A.append(self.layers[j].forward(Z[-1]))\n",
    "                    ## calc Z\n",
    "                    Z.append(self.activations[j].forward(A[-1]))\n",
    "                            \n",
    "                ## update weight, bias\n",
    "                ## append loss\n",
    "                dA = []\n",
    "                dZ = []\n",
    "                for j in range(1, len(self.layers)+1):\n",
    "                    if j == 1:\n",
    "                        ## backward\n",
    "                        dA.append(self.activations[-j].backward(Z[-1], mini_y_train_one_hot))\n",
    "                        ## loss\n",
    "                        self.loss[e].append(self.activations[-j].loss)\n",
    "                    else:\n",
    "                        ## backward\n",
    "                        dA.append(self.activations[-j].backward(dZ[-1]))\n",
    "                    dZ.append(self.layers[-j].backward(dA[-1]))\n",
    "\n",
    "                ## print progress\n",
    "                if self.verbose:\n",
    "                    print(f'\\r{i+1}/{len(get_mini_batch)} loop finished', end='')\n",
    "\n",
    "            ## validation\n",
    "            if (X_val is not None) and (y_val is not None):\n",
    "                ### prediction\n",
    "                tmp_A = []\n",
    "                tmp_Z = []\n",
    "                for j in range(len(self.layers)):\n",
    "                    ## calc A\n",
    "                    if j == 0:\n",
    "                        tmp_A.append(self.layers[j].forward(X_val))\n",
    "                    else:\n",
    "                        tmp_A.append(self.layers[j].forward(tmp_Z[-1]))\n",
    "                    ## calc Z\n",
    "                    tmp_Z.append(self.activations[j].forward(tmp_A[-1]))\n",
    "                ### append loss\n",
    "                self.val_loss.append(self.activations[-1].calc_cross_entropy_loss(y_val_one_hot, tmp_Z[-1]))\n",
    "\n",
    "            if self.verbose:\n",
    "                print(' : Complete!!')\n",
    "            if y_val is not None:\n",
    "                print(f'epoch {e+1} valid loss: {self.val_loss[-1]}')\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "        # output loss\n",
    "        print(f'last train loss: {self.loss[-1][-1]}')\n",
    "\n",
    "        print(f'Done! elapsed time: {elapsed_time:.5f}s')\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を使い推定する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            推定結果\n",
    "        \"\"\"\n",
    "        ## forward propagation\n",
    "        A = []\n",
    "        Z = []\n",
    "        for i in range(len(self.layers)):\n",
    "            ## calc A\n",
    "            if i == 0:\n",
    "                A.append(self.layers[i].forward(X))\n",
    "            else:\n",
    "                A.append(self.layers[i].forward(Z[-1]))\n",
    "            ## calc Z\n",
    "            Z.append(self.activations[i].forward(A[-1]))\n",
    "        \n",
    "        return np.argmax(Z[-1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "\n",
    "    Attribute\n",
    "    ---------\n",
    "    self.W : ndarray(n_nodes1, n_nodes2)\n",
    "      重み\n",
    "    self.B : ndarray(n_node2,)\n",
    "      バイアス\n",
    "    self.H : float\n",
    "      前イテレーションまでの勾配の二乗和\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
    "        self.B = initializer.B(n_nodes2)\n",
    "        self.HW = np.zeros(self.W.shape)\n",
    "        self.HB = np.zeros(self.B.shape)\n",
    "        pass\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"\n",
    "        if len(X.shape) > 2:\n",
    "            X = np.sum(X, axis=1)\n",
    "        print(X.shape)\n",
    "        print(self.W.shape)\n",
    "        print(self.B.shape)\n",
    "\n",
    "        self.Z_prev = X\n",
    "\n",
    "        A = X@self.W + self.B\n",
    "\n",
    "        return A\n",
    "\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "\n",
    "        # 更新\n",
    "        dZ = self.optimizer.update(self, dA)\n",
    "\n",
    "        return dZ\n",
    "\n",
    "class Softmax:\n",
    "    \"\"\"\n",
    "    ソフトマックス関数\n",
    "\n",
    "    Attribute\n",
    "    ----------\n",
    "    self.loss\n",
    "        出力の交差エントロピー誤差\n",
    "    \"\"\"\n",
    "    def forward(self, A):\n",
    "        Z = np.exp(A) / np.sum(np.exp(A), axis=1).reshape(-1, 1)\n",
    "        \n",
    "        return Z\n",
    "\n",
    "    def backward(self, Z, Y):\n",
    "        dA = Z - Y\n",
    "        self.loss = self.calc_cross_entropy_loss(Y, Z)\n",
    "\n",
    "        return dA\n",
    "    \n",
    "    def calc_cross_entropy_loss(self, y_true, y_pred):\n",
    "        n_samples = y_true.shape[0]\n",
    "\n",
    "        cross_entropy_loss = (-1 * (np.sum(y_true*np.log(y_pred)))) / n_samples\n",
    " \n",
    "        return cross_entropy_loss\n",
    "\n",
    "class ReLU:\n",
    "    \"\"\"\n",
    "    ReLU関数\n",
    "\n",
    "    Attribute\n",
    "    ----------\n",
    "    self.A\n",
    "        活性化関数の入力\n",
    "    \"\"\"\n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        Z = np.maximum(0, A)\n",
    "        \n",
    "        return Z\n",
    "\n",
    "    def backward(self, dZ):\n",
    "        dA = dZ * np.where(self.A > 0, 1, 0)\n",
    "\n",
    "        return dA\n",
    "\n",
    "class HeInitializer:\n",
    "    \"\"\"\n",
    "    Heの初期値\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    self.sigma\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1):\n",
    "        self.sigma = np.sqrt(2/n_nodes1)\n",
    "    \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "          重み\n",
    "        \"\"\"\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        \n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "          バイアス\n",
    "        \"\"\"\n",
    "        B = np.zeros((n_nodes2,))\n",
    "\n",
    "        return B\n",
    "\n",
    "class AdaGrad:\n",
    "    \"\"\"\n",
    "    AdaGrad法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, layer, dA):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "        # calc partial\n",
    "        dB = np.mean(dA, axis=0)\n",
    "        dW = (layer.Z_prev.T@dA) / dA.shape[0]\n",
    "        dZ = dA@layer.W.T\n",
    "\n",
    "        # update HB, HW\n",
    "        layer.HB = layer.HB + dB**2\n",
    "        layer.HW = layer.HW + dW**2\n",
    "\n",
    "        # update W and B\n",
    "        layer.B = layer.B - self.lr*(1/(np.sqrt(layer.HB+1e-7)))*dB\n",
    "        layer.W = layer.W - self.lr*(1/(np.sqrt(layer.HW+1e-7)))*dW\n",
    "\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprint9から拝借\n",
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self._X[p0:p1], self._y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self._X[p0:p1], self._y[p0:p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(48000, 784)\n(12000, 784)\n(10000, 784)\n"
    }
   ],
   "source": [
    "# データのロード\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
    "print(X_train.shape) # (48000, 784)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "start learning\nstart epoch 1\n(300, 772)\n(772, 10)\n(10,)\n"
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "shapes (300,776) and (300,772) not aligned: 776 (dim 1) != 300 (dim 0)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-1e2d8c436406>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m }\n\u001b[0;32m     10\u001b[0m \u001b[0mscratchDNN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mScratchDeepNeuralNetworkClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mscratchDNN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-74-1c9005c96f37>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, X_val, y_val)\u001b[0m\n\u001b[0;32m    124\u001b[0m                         \u001b[1;31m## backward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m                         \u001b[0mdA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdZ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m                     \u001b[0mdZ\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdA\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[1;31m## print progress\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-63-05c3056e8fcb>\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, dA)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \"\"\"\n\u001b[0;32m     64\u001b[0m         \u001b[1;31m# 更新\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[0mdX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-eace1e8eb8b7>\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, layer, dA)\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0minput_channel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m                 \u001b[0mX_tmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_channel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m                 \u001b[0mdW\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moutput_channel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_channel\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_tmp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdA_tmp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;31m# dX (batch_size, 出力チャンネル数, output_size)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-eace1e8eb8b7>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0minput_channel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m                 \u001b[0mX_tmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_channel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m                 \u001b[0mdW\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moutput_channel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_channel\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_tmp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdA_tmp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;31m# dX (batch_size, 出力チャンネル数, output_size)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (300,776) and (300,772) not aligned: 776 (dim 1) != 300 (dim 0)"
     ]
    }
   ],
   "source": [
    "# 学習\n",
    "params = {\n",
    "    'alpha':0.005,\n",
    "    'sigma':1,\n",
    "    'batch_size':300,\n",
    "    'epoch':3,\n",
    "    'verbose':True,\n",
    "    'n_nodes':[5, 3, 1, 10]\n",
    "}\n",
    "scratchDNN = ScratchDeepNeuralNetworkClassifier(**params)\n",
    "scratchDNN.fit(X_train, y_train, X_val, y_val)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}