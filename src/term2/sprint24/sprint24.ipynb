{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint\n",
    "## Seq2Seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.このSprintについて\n",
    "\n",
    "### Sprintの目的\n",
    "- 系列データに関する応用例を学ぶ\n",
    "\n",
    "### どのように学ぶか\n",
    "公開されているコードを元に学んでいきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.機械翻訳\n",
    "\n",
    "系列データに関する手法の基本的な活用例としては機械翻訳があります。これは系列データを入力し、系列データを出力する**Sequence to Sequence**の手法によって行えます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題1】\n",
    "#### 機械翻訳の実行とコードリーディング\n",
    "Keras公式のサンプルコードで、短い英語からフランス語への変換を行うものが公開されています。これを動かしてください。\n",
    "\n",
    "\n",
    "[keras/lstm_seq2seq.py at master · keras-team/keras](https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py)\n",
    "\n",
    "\n",
    "その上でこのサンプルコードの各部分がどういった役割かを読み取り、まとめてください。以下のようにどこからどこの行が何をしているかを記述してください。\n",
    "\n",
    "\n",
    "（例）\n",
    "\n",
    "\n",
    "- 51から55行目 : ライブラリのimport\n",
    "- 57から62行目 : ハイパーパラメータの設定\n",
    "\n",
    "**《文字単位のトークン化》**\n",
    "\n",
    "\n",
    "この実装ではテキストのベクトル化の際に、単語ではなく文字ごとを1つのトークンとして扱っています。\n",
    "\n",
    "\n",
    "scikit-learnでBoWを計算するCountVectorizerの場合では、デフォルトの引数は`analyzer='word'`で単語を扱いますが、`char`や`char_wb`とすることで文字を扱えるようになります。\n",
    "\n",
    "\n",
    "`char`と`char_wb`の2種類の方法があり、`char_wb`を指定した場合、n_gramが単語内からのみ作成されます。逆に`char`は単語の区切りが関係なくn_gramが作成されます。`This movie is very good.`というテキストを3-gramでカウントする時、`char`では`s m`や`e i`といった単語をまたぐ数え方もしますが、`char_wb`ではこれらを見ません。\n",
    "\n",
    "\n",
    "[sklearn.feature_extraction.text.CountVectorizer — scikit-learn 0.21.3 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Number of samples: 10000\nNumber of unique input tokens: 71\nNumber of unique output tokens: 92\nMax sequence length for inputs: 16\nMax sequence length for outputs: 59\nWARNING:tensorflow:From C:\\Users\\190450781\\Anaconda3\\envs\\dic\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\nInstructions for updating:\nIf using Keras pass *_constraint arguments to layers.\nWARNING:tensorflow:From C:\\Users\\190450781\\Anaconda3\\envs\\dic\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.where in 2.0, which has the same broadcast rule as np.where\nWARNING:tensorflow:From C:\\Users\\190450781\\Anaconda3\\envs\\dic\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n\nTrain on 8000 samples, validate on 2000 samples\nEpoch 1/100\n8000/8000 [==============================] - 47s 6ms/step - loss: 1.1785 - accuracy: 0.7228 - val_loss: 1.0874 - val_accuracy: 0.7034\nEpoch 2/100\n8000/8000 [==============================] - 43s 5ms/step - loss: 0.8548 - accuracy: 0.7687 - val_loss: 0.8349 - val_accuracy: 0.7674\nEpoch 3/100\n8000/8000 [==============================] - 50s 6ms/step - loss: 0.6931 - accuracy: 0.8062 - val_loss: 0.7208 - val_accuracy: 0.7934\nEpoch 4/100\n8000/8000 [==============================] - 48s 6ms/step - loss: 0.6016 - accuracy: 0.8254 - val_loss: 0.6565 - val_accuracy: 0.8068\nEpoch 5/100\n8000/8000 [==============================] - 45s 6ms/step - loss: 0.5489 - accuracy: 0.8396 - val_loss: 0.6082 - val_accuracy: 0.8221\nEpoch 6/100\n8000/8000 [==============================] - 46s 6ms/step - loss: 0.5097 - accuracy: 0.8508 - val_loss: 0.5792 - val_accuracy: 0.8295\nEpoch 7/100\n8000/8000 [==============================] - 44s 5ms/step - loss: 0.4803 - accuracy: 0.8581 - val_loss: 0.5553 - val_accuracy: 0.8345\nEpoch 8/100\n8000/8000 [==============================] - 44s 6ms/step - loss: 0.4564 - accuracy: 0.8650 - val_loss: 0.5320 - val_accuracy: 0.8427\nEpoch 9/100\n8000/8000 [==============================] - 39s 5ms/step - loss: 0.4351 - accuracy: 0.8705 - val_loss: 0.5165 - val_accuracy: 0.8471\nEpoch 10/100\n8000/8000 [==============================] - 43s 5ms/step - loss: 0.4161 - accuracy: 0.8760 - val_loss: 0.5077 - val_accuracy: 0.8492\nEpoch 11/100\n8000/8000 [==============================] - 43s 5ms/step - loss: 0.3988 - accuracy: 0.8807 - val_loss: 0.4904 - val_accuracy: 0.8540\nEpoch 12/100\n8000/8000 [==============================] - 45s 6ms/step - loss: 0.3827 - accuracy: 0.8854 - val_loss: 0.4817 - val_accuracy: 0.8575\nEpoch 13/100\n8000/8000 [==============================] - 40s 5ms/step - loss: 0.3678 - accuracy: 0.8896 - val_loss: 0.4778 - val_accuracy: 0.8578\nEpoch 14/100\n8000/8000 [==============================] - 36s 5ms/step - loss: 0.3542 - accuracy: 0.8937 - val_loss: 0.4665 - val_accuracy: 0.8618\nEpoch 15/100\n8000/8000 [==============================] - 34s 4ms/step - loss: 0.3410 - accuracy: 0.8976 - val_loss: 0.4581 - val_accuracy: 0.8648\nEpoch 16/100\n8000/8000 [==============================] - 34s 4ms/step - loss: 0.3287 - accuracy: 0.9012 - val_loss: 0.4540 - val_accuracy: 0.8659\nEpoch 17/100\n8000/8000 [==============================] - 34s 4ms/step - loss: 0.3173 - accuracy: 0.9043 - val_loss: 0.4484 - val_accuracy: 0.8682\nEpoch 18/100\n8000/8000 [==============================] - 34s 4ms/step - loss: 0.3060 - accuracy: 0.9075 - val_loss: 0.4434 - val_accuracy: 0.8697\nEpoch 19/100\n8000/8000 [==============================] - 34s 4ms/step - loss: 0.2954 - accuracy: 0.9108 - val_loss: 0.4469 - val_accuracy: 0.8695\nEpoch 20/100\n8000/8000 [==============================] - 34s 4ms/step - loss: 0.2854 - accuracy: 0.9139 - val_loss: 0.4454 - val_accuracy: 0.8700\nEpoch 21/100\n8000/8000 [==============================] - 35s 4ms/step - loss: 0.2756 - accuracy: 0.9171 - val_loss: 0.4404 - val_accuracy: 0.8725\nEpoch 22/100\n8000/8000 [==============================] - 34s 4ms/step - loss: 0.2667 - accuracy: 0.9193 - val_loss: 0.4426 - val_accuracy: 0.8722\nEpoch 23/100\n8000/8000 [==============================] - 34s 4ms/step - loss: 0.2580 - accuracy: 0.9219 - val_loss: 0.4379 - val_accuracy: 0.8750\nEpoch 24/100\n8000/8000 [==============================] - 34s 4ms/step - loss: 0.2498 - accuracy: 0.9243 - val_loss: 0.4384 - val_accuracy: 0.8747\nEpoch 25/100\n8000/8000 [==============================] - 34s 4ms/step - loss: 0.2415 - accuracy: 0.9266 - val_loss: 0.4410 - val_accuracy: 0.8737\nEpoch 26/100\n8000/8000 [==============================] - 34s 4ms/step - loss: 0.2337 - accuracy: 0.9290 - val_loss: 0.4410 - val_accuracy: 0.8745\nEpoch 27/100\n8000/8000 [==============================] - 34s 4ms/step - loss: 0.2264 - accuracy: 0.9311 - val_loss: 0.4442 - val_accuracy: 0.8744\nEpoch 28/100\n8000/8000 [==============================] - 34s 4ms/step - loss: 0.2192 - accuracy: 0.9330 - val_loss: 0.4469 - val_accuracy: 0.8747\nEpoch 29/100\n8000/8000 [==============================] - 34s 4ms/step - loss: 0.2120 - accuracy: 0.9352 - val_loss: 0.4481 - val_accuracy: 0.8744\nEpoch 30/100\n8000/8000 [==============================] - 34s 4ms/step - loss: 0.2055 - accuracy: 0.9374 - val_loss: 0.4451 - val_accuracy: 0.8763\nEpoch 31/100\n8000/8000 [==============================] - 34s 4ms/step - loss: 0.1994 - accuracy: 0.9391 - val_loss: 0.4549 - val_accuracy: 0.8748\nEpoch 32/100\n8000/8000 [==============================] - 34s 4ms/step - loss: 0.1935 - accuracy: 0.9409 - val_loss: 0.4558 - val_accuracy: 0.8745\nEpoch 33/100\n8000/8000 [==============================] - 34s 4ms/step - loss: 0.1873 - accuracy: 0.9426 - val_loss: 0.4577 - val_accuracy: 0.8755\nEpoch 34/100\n8000/8000 [==============================] - 34s 4ms/step - loss: 0.1821 - accuracy: 0.9442 - val_loss: 0.4641 - val_accuracy: 0.8753\nEpoch 35/100\n8000/8000 [==============================] - 34s 4ms/step - loss: 0.1763 - accuracy: 0.9460 - val_loss: 0.4666 - val_accuracy: 0.8752\nEpoch 36/100\n8000/8000 [==============================] - 34s 4ms/step - loss: 0.1715 - accuracy: 0.9473 - val_loss: 0.4709 - val_accuracy: 0.8753\nEpoch 37/100\n8000/8000 [==============================] - 34s 4ms/step - loss: 0.1668 - accuracy: 0.9487 - val_loss: 0.4753 - val_accuracy: 0.8760\nEpoch 38/100\n8000/8000 [==============================] - 34s 4ms/step - loss: 0.1616 - accuracy: 0.9504 - val_loss: 0.4795 - val_accuracy: 0.8753\nEpoch 39/100\n8000/8000 [==============================] - 34s 4ms/step - loss: 0.1571 - accuracy: 0.9517 - val_loss: 0.4833 - val_accuracy: 0.8750\nEpoch 40/100\n8000/8000 [==============================] - 34s 4ms/step - loss: 0.1525 - accuracy: 0.9532 - val_loss: 0.4871 - val_accuracy: 0.8755\nEpoch 41/100\n8000/8000 [==============================] - 34s 4ms/step - loss: 0.1483 - accuracy: 0.9544 - val_loss: 0.4932 - val_accuracy: 0.8751\nEpoch 42/100\n8000/8000 [==============================] - 34s 4ms/step - loss: 0.1440 - accuracy: 0.9559 - val_loss: 0.5009 - val_accuracy: 0.8742\nEpoch 43/100\n8000/8000 [==============================] - 34s 4ms/step - loss: 0.1404 - accuracy: 0.9566 - val_loss: 0.4954 - val_accuracy: 0.8762\nEpoch 44/100\n8000/8000 [==============================] - 34s 4ms/step - loss: 0.1366 - accuracy: 0.9578 - val_loss: 0.5066 - val_accuracy: 0.8743\nEpoch 45/100\n8000/8000 [==============================] - 34s 4ms/step - loss: 0.1332 - accuracy: 0.9589 - val_loss: 0.5094 - val_accuracy: 0.8748\nEpoch 46/100\n8000/8000 [==============================] - 34s 4ms/step - loss: 0.1296 - accuracy: 0.9600 - val_loss: 0.5145 - val_accuracy: 0.8741\nEpoch 47/100\n8000/8000 [==============================] - 34s 4ms/step - loss: 0.1265 - accuracy: 0.9610 - val_loss: 0.5216 - val_accuracy: 0.8751\nEpoch 48/100\n8000/8000 [==============================] - 34s 4ms/step - loss: 0.1237 - accuracy: 0.9616 - val_loss: 0.5221 - val_accuracy: 0.8759\nEpoch 49/100\n8000/8000 [==============================] - 35s 4ms/step - loss: 0.1201 - accuracy: 0.9629 - val_loss: 0.5303 - val_accuracy: 0.8742\nEpoch 50/100\n8000/8000 [==============================] - 34s 4ms/step - loss: 0.1173 - accuracy: 0.9635 - val_loss: 0.5321 - val_accuracy: 0.8746\nEpoch 51/100\n8000/8000 [==============================] - 34s 4ms/step - loss: 0.1143 - accuracy: 0.9645 - val_loss: 0.5355 - val_accuracy: 0.8751\nEpoch 52/100\n8000/8000 [==============================] - 34s 4ms/step - loss: 0.1119 - accuracy: 0.9651 - val_loss: 0.5421 - val_accuracy: 0.8747\nEpoch 53/100\n8000/8000 [==============================] - 34s 4ms/step - loss: 0.1092 - accuracy: 0.9658 - val_loss: 0.5460 - val_accuracy: 0.8740\nEpoch 54/100\n8000/8000 [==============================] - 34s 4ms/step - loss: 0.1065 - accuracy: 0.9668 - val_loss: 0.5512 - val_accuracy: 0.8746\nEpoch 55/100\n8000/8000 [==============================] - 34s 4ms/step - loss: 0.1041 - accuracy: 0.9676 - val_loss: 0.5547 - val_accuracy: 0.8745\nEpoch 56/100\n8000/8000 [==============================] - 34s 4ms/step - loss: 0.1018 - accuracy: 0.9679 - val_loss: 0.5622 - val_accuracy: 0.8739\nEpoch 57/100\n8000/8000 [==============================] - 34s 4ms/step - loss: 0.0994 - accuracy: 0.9691 - val_loss: 0.5644 - val_accuracy: 0.8744\nEpoch 58/100\n8000/8000 [==============================] - 34s 4ms/step - loss: 0.0973 - accuracy: 0.9693 - val_loss: 0.5701 - val_accuracy: 0.8740\nEpoch 59/100\n8000/8000 [==============================] - 34s 4ms/step - loss: 0.0950 - accuracy: 0.9700 - val_loss: 0.5743 - val_accuracy: 0.8747\nEpoch 60/100\n8000/8000 [==============================] - 34s 4ms/step - loss: 0.0931 - accuracy: 0.9704 - val_loss: 0.5853 - val_accuracy: 0.8735\nEpoch 61/100\n8000/8000 [==============================] - 34s 4ms/step - loss: 0.0911 - accuracy: 0.9710 - val_loss: 0.5853 - val_accuracy: 0.8743\nEpoch 62/100\n8000/8000 [==============================] - 34s 4ms/step - loss: 0.0895 - accuracy: 0.9716 - val_loss: 0.5886 - val_accuracy: 0.8735\nEpoch 63/100\n8000/8000 [==============================] - 34s 4ms/step - loss: 0.0875 - accuracy: 0.9720 - val_loss: 0.5924 - val_accuracy: 0.8736\nEpoch 64/100\n8000/8000 [==============================] - 34s 4ms/step - loss: 0.0857 - accuracy: 0.9726 - val_loss: 0.5997 - val_accuracy: 0.8736\nEpoch 65/100\n8000/8000 [==============================] - 34s 4ms/step - loss: 0.0841 - accuracy: 0.9731 - val_loss: 0.6015 - val_accuracy: 0.8735\nEpoch 66/100\n8000/8000 [==============================] - 34s 4ms/step - loss: 0.0821 - accuracy: 0.9737 - val_loss: 0.6016 - val_accuracy: 0.8734\nEpoch 67/100\n8000/8000 [==============================] - 34s 4ms/step - loss: 0.0809 - accuracy: 0.9739 - val_loss: 0.6109 - val_accuracy: 0.8737\nEpoch 68/100\n8000/8000 [==============================] - 34s 4ms/step - loss: 0.0795 - accuracy: 0.9745 - val_loss: 0.6121 - val_accuracy: 0.8736\nEpoch 69/100\n8000/8000 [==============================] - 34s 4ms/step - loss: 0.0775 - accuracy: 0.9753 - val_loss: 0.6184 - val_accuracy: 0.8731\nEpoch 70/100\n8000/8000 [==============================] - 34s 4ms/step - loss: 0.0761 - accuracy: 0.9754 - val_loss: 0.6186 - val_accuracy: 0.8737\nEpoch 71/100\n8000/8000 [==============================] - 34s 4ms/step - loss: 0.0750 - accuracy: 0.9754 - val_loss: 0.6247 - val_accuracy: 0.8736\nEpoch 72/100\n8000/8000 [==============================] - 34s 4ms/step - loss: 0.0735 - accuracy: 0.9761 - val_loss: 0.6244 - val_accuracy: 0.8738\nEpoch 73/100\n8000/8000 [==============================] - 34s 4ms/step - loss: 0.0720 - accuracy: 0.9765 - val_loss: 0.6368 - val_accuracy: 0.8733\nEpoch 74/100\n8000/8000 [==============================] - 34s 4ms/step - loss: 0.0709 - accuracy: 0.9770 - val_loss: 0.6380 - val_accuracy: 0.8734\nEpoch 75/100\n8000/8000 [==============================] - 34s 4ms/step - loss: 0.0695 - accuracy: 0.9771 - val_loss: 0.6414 - val_accuracy: 0.8727\nEpoch 76/100\n8000/8000 [==============================] - 34s 4ms/step - loss: 0.0685 - accuracy: 0.9775 - val_loss: 0.6437 - val_accuracy: 0.8729\nEpoch 77/100\n8000/8000 [==============================] - 34s 4ms/step - loss: 0.0672 - accuracy: 0.9780 - val_loss: 0.6488 - val_accuracy: 0.8736\nEpoch 78/100\n8000/8000 [==============================] - 34s 4ms/step - loss: 0.0658 - accuracy: 0.9783 - val_loss: 0.6481 - val_accuracy: 0.8722\nEpoch 79/100\n8000/8000 [==============================] - 36s 4ms/step - loss: 0.0649 - accuracy: 0.9786 - val_loss: 0.6572 - val_accuracy: 0.8730\nEpoch 80/100\n8000/8000 [==============================] - 37s 5ms/step - loss: 0.0639 - accuracy: 0.9787 - val_loss: 0.6601 - val_accuracy: 0.8731\nEpoch 81/100\n8000/8000 [==============================] - 36s 4ms/step - loss: 0.0626 - accuracy: 0.9792 - val_loss: 0.6610 - val_accuracy: 0.8732\nEpoch 82/100\n8000/8000 [==============================] - 35s 4ms/step - loss: 0.0616 - accuracy: 0.9795 - val_loss: 0.6663 - val_accuracy: 0.8730\nEpoch 83/100\n8000/8000 [==============================] - 35s 4ms/step - loss: 0.0608 - accuracy: 0.9796 - val_loss: 0.6654 - val_accuracy: 0.8729\nEpoch 84/100\n8000/8000 [==============================] - 35s 4ms/step - loss: 0.0597 - accuracy: 0.9798 - val_loss: 0.6741 - val_accuracy: 0.8725\nEpoch 85/100\n8000/8000 [==============================] - 35s 4ms/step - loss: 0.0589 - accuracy: 0.9802 - val_loss: 0.6752 - val_accuracy: 0.8733\nEpoch 86/100\n8000/8000 [==============================] - 35s 4ms/step - loss: 0.0578 - accuracy: 0.9806 - val_loss: 0.6773 - val_accuracy: 0.8733\nEpoch 87/100\n8000/8000 [==============================] - 35s 4ms/step - loss: 0.0572 - accuracy: 0.9805 - val_loss: 0.6822 - val_accuracy: 0.8732\nEpoch 88/100\n8000/8000 [==============================] - 35s 4ms/step - loss: 0.0562 - accuracy: 0.9810 - val_loss: 0.6827 - val_accuracy: 0.8735\nEpoch 89/100\n8000/8000 [==============================] - 35s 4ms/step - loss: 0.0553 - accuracy: 0.9811 - val_loss: 0.6885 - val_accuracy: 0.8719\nEpoch 90/100\n8000/8000 [==============================] - 35s 4ms/step - loss: 0.0545 - accuracy: 0.9814 - val_loss: 0.6913 - val_accuracy: 0.8718\nEpoch 91/100\n8000/8000 [==============================] - 35s 4ms/step - loss: 0.0536 - accuracy: 0.9817 - val_loss: 0.6887 - val_accuracy: 0.8736\nEpoch 92/100\n8000/8000 [==============================] - 35s 4ms/step - loss: 0.0529 - accuracy: 0.9819 - val_loss: 0.6981 - val_accuracy: 0.8729\nEpoch 93/100\n8000/8000 [==============================] - 35s 4ms/step - loss: 0.0521 - accuracy: 0.9822 - val_loss: 0.7008 - val_accuracy: 0.8725\nEpoch 94/100\n8000/8000 [==============================] - 35s 4ms/step - loss: 0.0514 - accuracy: 0.9825 - val_loss: 0.7000 - val_accuracy: 0.8729\nEpoch 95/100\n8000/8000 [==============================] - 35s 4ms/step - loss: 0.0508 - accuracy: 0.9825 - val_loss: 0.7011 - val_accuracy: 0.8731\nEpoch 96/100\n8000/8000 [==============================] - 35s 4ms/step - loss: 0.0501 - accuracy: 0.9827 - val_loss: 0.7122 - val_accuracy: 0.8722\nEpoch 97/100\n8000/8000 [==============================] - 35s 4ms/step - loss: 0.0492 - accuracy: 0.9829 - val_loss: 0.7151 - val_accuracy: 0.8723\nEpoch 98/100\n8000/8000 [==============================] - 35s 4ms/step - loss: 0.0485 - accuracy: 0.9830 - val_loss: 0.7217 - val_accuracy: 0.8727\nEpoch 99/100\n8000/8000 [==============================] - 35s 4ms/step - loss: 0.0483 - accuracy: 0.9832 - val_loss: 0.7219 - val_accuracy: 0.8721\nEpoch 100/100\n8000/8000 [==============================] - 35s 4ms/step - loss: 0.0477 - accuracy: 0.9834 - val_loss: 0.7220 - val_accuracy: 0.8717\n-\nInput sentence: Go.\nDecoded sentence: Va !\n\n-\nInput sentence: Hi.\nDecoded sentence: Salut.\n\n-\nInput sentence: Hi.\nDecoded sentence: Salut.\n\n-\nInput sentence: Run!\nDecoded sentence: Cours !\n\n-\nInput sentence: Run!\nDecoded sentence: Cours !\n\n-\nInput sentence: Who?\nDecoded sentence: Qui ?\n\n-\nInput sentence: Wow!\nDecoded sentence: Ça alors !\n\n-\nInput sentence: Fire!\nDecoded sentence: Au feu !\n\n-\nInput sentence: Help!\nDecoded sentence: À l'aide !\n\n-\nInput sentence: Jump.\nDecoded sentence: Saute.\n\n-\nInput sentence: Stop!\nDecoded sentence: Ça suffit !\n\n-\nInput sentence: Stop!\nDecoded sentence: Ça suffit !\n\n-\nInput sentence: Stop!\nDecoded sentence: Ça suffit !\n\n-\nInput sentence: Wait!\nDecoded sentence: Attends !\n\n-\nInput sentence: Wait!\nDecoded sentence: Attends !\n\n-\nInput sentence: Go on.\nDecoded sentence: Poursuivez.\n\n-\nInput sentence: Go on.\nDecoded sentence: Poursuivez.\n\n-\nInput sentence: Go on.\nDecoded sentence: Poursuivez.\n\n-\nInput sentence: Hello!\nDecoded sentence: Salut !\n\n-\nInput sentence: Hello!\nDecoded sentence: Salut !\n\n-\nInput sentence: I see.\nDecoded sentence: Je comprends.\n\n-\nInput sentence: I try.\nDecoded sentence: J'essaye.\n\n-\nInput sentence: I won!\nDecoded sentence: J'ai gagné !\n\n-\nInput sentence: I won!\nDecoded sentence: J'ai gagné !\n\n-\nInput sentence: I won.\nDecoded sentence: J’ai gagné.\n\n-\nInput sentence: Oh no!\nDecoded sentence: Oh non !\n\n-\nInput sentence: Attack!\nDecoded sentence: Attaque !\n\n-\nInput sentence: Attack!\nDecoded sentence: Attaque !\n\n-\nInput sentence: Cheers!\nDecoded sentence: À votre santé !\n\n-\nInput sentence: Cheers!\nDecoded sentence: À votre santé !\n\n-\nInput sentence: Cheers!\nDecoded sentence: À votre santé !\n\n-\nInput sentence: Cheers!\nDecoded sentence: À votre santé !\n\n-\nInput sentence: Get up.\nDecoded sentence: Lève-toi.\n\n-\nInput sentence: Go now.\nDecoded sentence: Va, maintenant.\n\n-\nInput sentence: Go now.\nDecoded sentence: Va, maintenant.\n\n-\nInput sentence: Go now.\nDecoded sentence: Va, maintenant.\n\n-\nInput sentence: Got it!\nDecoded sentence: Compris !\n\n-\nInput sentence: Got it!\nDecoded sentence: Compris !\n\n-\nInput sentence: Got it?\nDecoded sentence: Pigé ?\n\n-\nInput sentence: Got it?\nDecoded sentence: Pigé ?\n\n-\nInput sentence: Got it?\nDecoded sentence: Pigé ?\n\n-\nInput sentence: Hop in.\nDecoded sentence: Monte.\n\n-\nInput sentence: Hop in.\nDecoded sentence: Monte.\n\n-\nInput sentence: Hug me.\nDecoded sentence: Serrez-moi dans vos bras !\n\n-\nInput sentence: Hug me.\nDecoded sentence: Serrez-moi dans vos bras !\n\n-\nInput sentence: I fell.\nDecoded sentence: Je suis tombée.\n\n-\nInput sentence: I fell.\nDecoded sentence: Je suis tombée.\n\n-\nInput sentence: I know.\nDecoded sentence: Je sais.\n\n-\nInput sentence: I left.\nDecoded sentence: Je suis parti.\n\n-\nInput sentence: I left.\nDecoded sentence: Je suis parti.\n\n-\nInput sentence: I lied.\nDecoded sentence: J'ai menti.\n\n-\nInput sentence: I lost.\nDecoded sentence: J'ai perdu.\n\n-\nInput sentence: I paid.\nDecoded sentence: J’ai payé.\n\n-\nInput sentence: I'm 19.\nDecoded sentence: J'ai terminé.\n\n-\nInput sentence: I'm OK.\nDecoded sentence: Je suis malade.\n\n-\nInput sentence: I'm OK.\nDecoded sentence: Je suis malade.\n\n-\nInput sentence: Listen.\nDecoded sentence: Écoutez !\n\n-\nInput sentence: No way!\nDecoded sentence: C'est hors de question !\n\n-\nInput sentence: No way!\nDecoded sentence: C'est hors de question !\n\n-\nInput sentence: No way!\nDecoded sentence: C'est hors de question !\n\n-\nInput sentence: No way!\nDecoded sentence: C'est hors de question !\n\n-\nInput sentence: No way!\nDecoded sentence: C'est hors de question !\n\n-\nInput sentence: No way!\nDecoded sentence: C'est hors de question !\n\n-\nInput sentence: No way!\nDecoded sentence: C'est hors de question !\n\n-\nInput sentence: No way!\nDecoded sentence: C'est hors de question !\n\n-\nInput sentence: No way!\nDecoded sentence: C'est hors de question !\n\n-\nInput sentence: Really?\nDecoded sentence: Vraiment ?\n\n-\nInput sentence: Really?\nDecoded sentence: Vraiment ?\n\n-\nInput sentence: Really?\nDecoded sentence: Vraiment ?\n\n-\nInput sentence: Thanks.\nDecoded sentence: Merci !\n\n-\nInput sentence: We try.\nDecoded sentence: On essaye.\n\n-\nInput sentence: We won.\nDecoded sentence: Nous avons gagné.\n\n-\nInput sentence: We won.\nDecoded sentence: Nous avons gagné.\n\n-\nInput sentence: We won.\nDecoded sentence: Nous avons gagné.\n\n-\nInput sentence: We won.\nDecoded sentence: Nous avons gagné.\n\n-\nInput sentence: Ask Tom.\nDecoded sentence: Demande à Tom.\n\n-\nInput sentence: Awesome!\nDecoded sentence: Fantastique !\n\n-\nInput sentence: Be calm.\nDecoded sentence: Soyez calmes !\n\n-\nInput sentence: Be calm.\nDecoded sentence: Soyez calmes !\n\n-\nInput sentence: Be calm.\nDecoded sentence: Soyez calmes !\n\n-\nInput sentence: Be cool.\nDecoded sentence: Sois détendu !\n\n-\nInput sentence: Be fair.\nDecoded sentence: Soyez équitable !\n\n-\nInput sentence: Be fair.\nDecoded sentence: Soyez équitable !\n\n-\nInput sentence: Be fair.\nDecoded sentence: Soyez équitable !\n\n-\nInput sentence: Be fair.\nDecoded sentence: Soyez équitable !\n\n-\nInput sentence: Be fair.\nDecoded sentence: Soyez équitable !\n\n-\nInput sentence: Be fair.\nDecoded sentence: Soyez équitable !\n\n-\nInput sentence: Be kind.\nDecoded sentence: Sois gentil.\n\n-\nInput sentence: Be nice.\nDecoded sentence: Sois gentil !\n\n-\nInput sentence: Be nice.\nDecoded sentence: Sois gentil !\n\n-\nInput sentence: Be nice.\nDecoded sentence: Sois gentil !\n\n-\nInput sentence: Be nice.\nDecoded sentence: Sois gentil !\n\n-\nInput sentence: Be nice.\nDecoded sentence: Sois gentil !\n\n-\nInput sentence: Be nice.\nDecoded sentence: Sois gentil !\n\n-\nInput sentence: Beat it.\nDecoded sentence: Dégage !\n\n-\nInput sentence: Call me.\nDecoded sentence: Appelle-moi !\n\n-\nInput sentence: Call me.\nDecoded sentence: Appelle-moi !\n\n-\nInput sentence: Call us.\nDecoded sentence: Appelle-nous !\n\n-\nInput sentence: Call us.\nDecoded sentence: Appelle-nous !\n\n-\nInput sentence: Come in.\nDecoded sentence: Entrez !\n\n"
    }
   ],
   "source": [
    "# english to french\n",
    "%run lstm_seq2seq.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**コードリーディング**\n",
    "\n",
    "-> `lstm_seq2seq.py`にコメントとして記載。英文コメントをベースに各部分の動作を確認。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.イメージキャプショニング\n",
    "\n",
    "他の活用例としてイメージキャプショニングがあります。画像に対する説明の文章を推定するタスクです。これは画像を入力し、系列データを出力する**Image to Sequence**の手法によって行えます。\n",
    "\n",
    "\n",
    "[pytorch-tutorial/tutorials/03-advanced/image_captioning at master · yunjey/pytorch-tutorial](https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/03-advanced/image_captioning)\n",
    "\n",
    "\n",
    "イメージキャプショニングは学習に多くの時間がかかるため、ここでは学習済みの重みが公開されている実装を動かすことにします。Kerasには平易に扱える実装が公開されていないため、今回はPyTorchによる実装を扱います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題2】\n",
    "#### イメージキャプショニングの学習済みモデルの実行\n",
    "上記実装において 5. Test the model の項目を実行してください。また、自身で用意した画像に対しても文章を生成してください。これらに対してどういった文章が出力されたかを記録して提出してください。\n",
    "\n",
    "\n",
    "データセットからの学習は行わず、学習済みの重みをダウンロードして利用します。\n",
    "\n",
    "\n",
    "注意点として、デフォルトで設定されている重みのファイル名と、ダウンロードできる重みのファイル名は異なっています。ここは書き換える必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# require Microsoft Visual C++ build tools\n",
    "# https://visualstudio.microsoft.com/ja/downloads/\n",
    "# Build Tools for Visual Studio 2019 -> C++ Build Tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題3】\n",
    "#### Kerasで動かしたい場合はどうするかを調査\n",
    "PyTorchによる実装を動かしましたが、何らかの理由からKerasで動かしたい状況が考えられます。どういった手順を踏むことになるか調査し、できるだけ詳しく説明してください。\n",
    "\n",
    "\n",
    "特に今回はPyTorchのための学習済みの重みをKerasで使えるようにしたいので、その点については必ず触れてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題4】\n",
    "#### （アドバンス課題）コードリーディングと書き換え\n",
    "モデル部分はmodel.pyに書かれていますが、Kerasではこのモデルがどのように記述できるかを考え、コーディングしてください。その際機械翻訳のサンプルコードが参考になります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題5】\n",
    "#### （アドバンス課題）発展的調査\n",
    "**《他の言語の翻訳を行う場合は？》**\n",
    "\n",
    "\n",
    "問題1の実装を使い日本語と英語の翻訳を行いたい場合はどのような手順を踏むか考えてみましょう。\n",
    "\n",
    "\n",
    "**《機械翻訳の発展的手法にはどのようなものがある？》**\n",
    "\n",
    "\n",
    "機械翻訳のための発展的手法にはどういったものがあるか調査してみましょう。\n",
    "\n",
    "\n",
    "**《文章から画像生成するには？》**\n",
    "\n",
    "\n",
    "イメージキャプショニングとは逆に文章から画像を生成する手法もあります。どういったものがあるか調査してみましょう。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bitdicconda58dbae13a5ad45af92cdb395e5ca7493",
   "display_name": "Python 3.7.7 64-bit ('dic': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}