{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint\n",
    "## ゲート付きリカレントニューラルネットワーク"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.このSprintについて\n",
    "\n",
    "### Sprintの目的\n",
    "- 発展的なRNNの手法を理解する\n",
    "- ドキュメントを網羅的に読む\n",
    "\n",
    "### どのように学ぶか\n",
    "Kerasに用意されているRNN関係のレイヤーを動作させながら学んでいきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.KerasのRecurrentレイヤー\n",
    "\n",
    "Kerasには複数のRecurrentレイヤーや、それに関連したクラスが用意されています。今回のSprintではこれら全てを動かした上で、それぞれの役割を説明できる状態を目指します。\n",
    "\n",
    "\n",
    "以下のドキュメントにまとめられています。\n",
    "\n",
    "\n",
    "[Recurrentレイヤー - Keras Documentation](https://keras.io/ja/layers/recurrent/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題1】\n",
    "#### 各種手法の実行\n",
    "Kerasには4種類のReccurentレイヤーが用意されています。SimpleRNN以外はゲート付きリカレントニューラルネットワークです。\n",
    "\n",
    "\n",
    "- SimpleRNN\n",
    "- GRU\n",
    "- LSTM\n",
    "- ConvLSTM2D\n",
    "\n",
    "これらを実行してください。この中でSimpleRNN、GRU、LSTMは同様のタスクに用いることができるため、精度の比較も行なってください。\n",
    "\n",
    "\n",
    "Keras公式のサンプルコードを利用してください。\n",
    "\n",
    "\n",
    "**LSTMのサンプルコード**\n",
    "\n",
    "\n",
    "[keras/imdb_lstm.py at master · keras-team/keras](https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py)\n",
    "\n",
    "\n",
    "**ConvLSTM2Dのサンプルコード**\n",
    "\n",
    "\n",
    "[keras/conv_lstm.py at master · keras-team/keras](https://github.com/keras-team/keras/blob/master/examples/conv_lstm.py)\n",
    "\n",
    "\n",
    "このサンプルコードをそのまま使う必要はなく、ノード数やエポックなどは変更して構いません。全て実行する上での実行時間を考慮した数に設定してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題2】\n",
    "#### （アドバンス課題）複数のデータセット間での比較\n",
    "他のデータセットでも実験を行なってください。\n",
    "\n",
    "\n",
    "[データセット - Keras Documentation](https://keras.io/ja/datasets/#_5)\n",
    "\n",
    "\n",
    "Kerasで簡単に利用できる自然言語データセットとしてロイターのニュースワイヤー トピックス分類があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2.3.1\n"
    }
   ],
   "source": [
    "import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\nLoading data...\n8982 train sequences\n2246 test sequences\n(8982,) (8982,)\n[1, 27595, 28842, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 4579, 1005, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 1245, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12] 3\n(2246,) (2246,)\n[1, 4, 1378, 2025, 9, 697, 4622, 111, 8, 25, 109, 29, 3650, 11, 150, 244, 364, 33, 30, 30, 1398, 333, 6, 18292, 159, 9, 1084, 363, 13, 19231, 71, 9, 16273, 71, 117, 4, 225, 78, 206, 10, 9, 1214, 8, 4, 270, 5, 16273, 7, 748, 48, 9, 19231, 7, 207, 1451, 966, 1864, 793, 97, 133, 336, 7, 4, 493, 98, 273, 104, 284, 25, 39, 338, 22, 905, 220, 3465, 644, 59, 20, 6, 119, 61, 11, 15, 58, 579, 26, 10, 67, 7, 4, 738, 98, 43, 88, 333, 722, 12, 20, 6, 19, 746, 35, 15, 10, 9, 1214, 855, 129, 783, 21, 4, 2280, 244, 364, 51, 16, 299, 452, 16, 515, 4, 99, 29, 5, 4, 364, 281, 48, 10, 9, 1214, 23, 644, 47, 20, 324, 27, 56, 23406, 28185, 5, 192, 510, 17, 12] 3\n(8982, 46) (2246, 46)\n"
    }
   ],
   "source": [
    "# load data\n",
    "from keras.datasets import reuters\n",
    "import numpy as np\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(path=\"reuters.npz\",\n",
    "                                                         num_words=None,\n",
    "                                                         skip_top=0,\n",
    "                                                         maxlen=None,\n",
    "                                                         test_split=0.2,\n",
    "                                                         seed=113,\n",
    "                                                         start_char=1,\n",
    "                                                         oov_char=2,\n",
    "                                                         index_from=3)\n",
    "\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_train[0], y_train[0])\n",
    "print(x_test.shape, y_test.shape)\n",
    "print(x_test[0], y_test[0])\n",
    "\n",
    "# one-hot encoding\n",
    "num_class = len(np.unique(np.concatenate((y_train, y_test))))\n",
    "y_train = np.eye(num_class)[y_train]\n",
    "y_test = np.eye(num_class)[y_test]\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Pad sequences (samples x time)\nx_train shape: (8982, 2376)\nx_test shape: (2246, 2376)\nBuild model...\nTrain...\nWARNING:tensorflow:From C:\\Users\\190450781\\Anaconda3\\envs\\dic\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n\nTrain on 8982 samples, validate on 2246 samples\nEpoch 1/10\n8982/8982 [==============================] - 440s 49ms/step - loss: 3.7670 - accuracy: 0.3244 - val_loss: 3.6220 - val_accuracy: 0.3816\nEpoch 2/10\n8982/8982 [==============================] - 472s 53ms/step - loss: 3.1216 - accuracy: 0.3545 - val_loss: 2.6066 - val_accuracy: 0.3620\nEpoch 3/10\n8982/8982 [==============================] - 471s 52ms/step - loss: 2.5021 - accuracy: 0.3518 - val_loss: 2.4415 - val_accuracy: 0.3620\nEpoch 4/10\n8982/8982 [==============================] - 457s 51ms/step - loss: 2.4302 - accuracy: 0.3517 - val_loss: 2.4241 - val_accuracy: 0.3620\nEpoch 5/10\n8982/8982 [==============================] - 516s 57ms/step - loss: 2.4133 - accuracy: 0.3517 - val_loss: 2.4174 - val_accuracy: 0.3620\nEpoch 6/10\n8982/8982 [==============================] - 488s 54ms/step - loss: 2.4085 - accuracy: 0.3517 - val_loss: 2.4152 - val_accuracy: 0.3620\nEpoch 7/10\n8982/8982 [==============================] - 282s 31ms/step - loss: 2.4069 - accuracy: 0.3518 - val_loss: 2.4144 - val_accuracy: 0.3620\nEpoch 8/10\n8982/8982 [==============================] - 291s 32ms/step - loss: 2.4058 - accuracy: 0.3517 - val_loss: 2.4136 - val_accuracy: 0.3620\nEpoch 9/10\n8982/8982 [==============================] - 286s 32ms/step - loss: 2.4049 - accuracy: 0.3517 - val_loss: 2.4138 - val_accuracy: 0.3620\nEpoch 10/10\n8982/8982 [==============================] - 271s 30ms/step - loss: 2.4039 - accuracy: 0.3517 - val_loss: 2.4136 - val_accuracy: 0.3620\n2246/2246 [==============================] - 9s 4ms/step\nTest score: 2.413559815974928\nTest accuracy: 0.36197686195373535\n"
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "\n",
    "max_features = max([max(x) for x in np.concatenate((x_train, x_test))])+1\n",
    "maxlen = max([len(x) for x in np.concatenate((x_train, x_test))])\n",
    "batch_size = 512\n",
    "dim_lstm = 32\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, dim_lstm))\n",
    "model.add(LSTM(dim_lstm,\n",
    "               dropout=0.2,\n",
    "               recurrent_dropout=0.2))\n",
    "model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題3】\n",
    "#### 他のクラスの説明\n",
    "ドキュメントには他にも関連するクラスが記載されています。それらがどういうものなのかを説明してください。この中には実際に扱うことは少ないクラスも含まれています。\n",
    "\n",
    "\n",
    "- RNN\n",
    "- SimpleRNNCell\n",
    "- GRUCell\n",
    "- LSTMCell\n",
    "- StackedRNNCells\n",
    "- CuDNNGRU\n",
    "- CuDNNLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN\n",
    "RNN各種の基底クラス\n",
    "\n",
    "下のCellを引数として渡すことができる。\n",
    "\n",
    "#### SimpleRNNCell\n",
    "Cell:RNNの「核」となる部分？\n",
    "\n",
    "長期記憶機構を持たない単純なRNN機構\n",
    "\n",
    "`RNN(cell=SimpleRNNCell())`=`SimpleRNN()`となるらしい。\n",
    "\n",
    "#### GRUCell\n",
    "GRU: LSTMに近い、長期記憶を持つRNN機構。\n",
    "\n",
    "のCell\n",
    "\n",
    "LSTMと違いforget gateを持たない。\n",
    "\n",
    "#### LSTMCell\n",
    "Long short-term memory\n",
    "\n",
    "長期記憶をもつRNN機構。\n",
    "\n",
    "のCell\n",
    "\n",
    "GRUに比べてパラメータ数が多く、その結果計算は遅くなるが精度は良くなる。\n",
    "\n",
    "#### StackedRNNCells\n",
    "#### CuDNNGRU\n",
    "#### CuDNNLSTM"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bitdicconda58dbae13a5ad45af92cdb395e5ca7493",
   "display_name": "Python 3.7.7 64-bit ('dic': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}