{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint\n",
    "## 深層学習スクラッチ 畳み込みニューラルネットワーク2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.このSprintについて\n",
    "\n",
    "### Sprintの目的\n",
    "スクラッチを通してCNNの基礎を理解する\n",
    "\n",
    "### どのように学ぶか\n",
    "スクラッチで2次元用畳み込みニューラルネットワークを実装した後、学習と検証を行なっていきます"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2次元の畳み込みニューラルネットワークスクラッチ\n",
    "\n",
    "2次元に対応した畳み込みニューラルネットワーク（CNN）のクラスをスクラッチで作成していきます。NumPyなど最低限のライブラリのみを使いアルゴリズムを実装していきます。\n",
    "\n",
    "\n",
    "プーリング層なども作成することで、CNNの基本形を完成させます。クラスの名前はScratch2dCNNClassifierとしてください。\n",
    "\n",
    "\n",
    "### データセットの用意\n",
    "引き続きMNISTデータセットを使用します。2次元畳み込み層へは、28×28の状態で入力します。\n",
    "\n",
    "\n",
    "今回は白黒画像ですからチャンネルは1つしかありませんが、チャンネル方向の軸は用意しておく必要があります。\n",
    "\n",
    "\n",
    "`(n_samples, n_channels, height, width)`の`NCHW`または`(n_samples, height, width, n_channels)`の`NHWC`どちらかの形にしてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> `NCHW`のほうがよさそう？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題1】\n",
    "#### 2次元畳み込み層の作成\n",
    "1次元畳み込み層のクラスConv1dを発展させ、2次元畳み込み層のクラスConv2dを作成してください。\n",
    "\n",
    "\n",
    "フォワードプロパゲーションの数式は以下のようになります。\n",
    "\n",
    "$$\n",
    "a_{i,j,m} = \\sum_{k=0}^{K-1}\\sum_{s=0}^{F_{h}-1}\\sum_{t=0}^{F_{w}-1}x_{(i+s),(j+t),k}w_{s,t,k,m}+b_{m}\n",
    "$$\n",
    "\n",
    "$a_{i,j,m}$ : 出力される配列のi行j列、mチャンネルの値\n",
    "\n",
    "\n",
    "$i$ : 配列の行方向のインデックス\n",
    "\n",
    "\n",
    "$j$ : 配列の列方向のインデックス\n",
    "\n",
    "\n",
    "$m$ : 出力チャンネルのインデックス\n",
    "\n",
    "\n",
    "$K$ : 入力チャンネル数\n",
    "\n",
    "\n",
    "$F_h, F_w$ : 高さ方向（h）と幅方向（w）のフィルタのサイズ\n",
    "\n",
    "\n",
    "$x_{(i+s),(j+t),k}$ : 入力の配列の(i+s)行(j+t)列、kチャンネルの値\n",
    "\n",
    "\n",
    "$w_{s,t,k,m}$ : 重みの配列のs行t列目。kチャンネルの入力に対して、mチャンネルへ出力する重み\n",
    "\n",
    "\n",
    "$b_m$ : mチャンネルへの出力のバイアス項\n",
    "\n",
    "\n",
    "全てスカラーです。\n",
    "\n",
    "\n",
    "次に更新式です。1次元畳み込み層や全結合層と同じ形です。\n",
    "\n",
    "$$\n",
    "w_{s,t,k,m}^{\\prime} = w_{s,t,k,m} - \\alpha \\frac{\\partial L}{\\partial w_{s,t,k,m}} \\\\\n",
    "b_{m}^{\\prime} = b_{m} - \\alpha \\frac{\\partial L}{\\partial b_{m}}\n",
    "$$\n",
    "\n",
    "$\\alpha$ : 学習率\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{\\partial w_{s,t,k,m}}$ : $w_{s,t,k,m}$に関する損失$L$の勾配\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{\\partial b_{m}}$ : $b_m$に関する損失$L$の勾配\n",
    "\n",
    "\n",
    "勾配$\\frac{\\partial L}{\\partial w_{s,t,k,m}}$や$\\frac{\\partial L}{\\partial b_{m}}$を求めるためのバックプロパゲーションの数式が以下である。\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_{s,t,k,m}} = \\sum_{i=0}^{N_{out,h}-1}\\sum_{j=0}^{N_{out,w}-1} \\frac{\\partial L}{\\partial a_{i,j,m}}x_{(i+s),(j+t),k}\\\\\n",
    "\\frac{\\partial L}{\\partial b_{m}} = \\sum_{i=0}^{N_{out,h}-1}\\sum_{j=0}^{N_{out,w}-1}\\frac{\\partial L}{\\partial a_{i,j,m}}\n",
    "$$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial a_{i,j,m}}$ : 勾配の配列のi行j列、mチャンネルの値\n",
    "\n",
    "\n",
    "$N_{out,h}, N_{out,w}$ : 高さ方向（h）と幅方向（w）の出力のサイズ\n",
    "\n",
    "\n",
    "前の層に流す誤差の数式は以下です。\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x_{i,j,k}} = \\sum_{m=0}^{M-1}\\sum_{s=0}^{F_{h}-1}\\sum_{t=0}^{F_{w}-1} \\frac{\\partial L}{\\partial a_{(i-s),(j-t),m}}w_{s,t,k,m}\n",
    "$$\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{\\partial x_{i,j,k}}$ : 前の層に流す誤差の配列のi列j行、kチャンネルの値\n",
    "\n",
    "\n",
    "$M$ : 出力チャンネル数\n",
    "\n",
    "\n",
    "ただし、$i−s<0$または$i−s>N_{out,h}−1$または$j−t<0$または$j−t>N_{out,w}−1$のとき$\\frac{\\partial L}{\\partial a_{(i-s),(j-t),m}}=0$です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題2】\n",
    "#### 2次元畳み込み後の出力サイズ\n",
    "畳み込みを行うと特徴マップのサイズが変化します。どのように変化するかは以下の数式から求められます。この計算を行う関数を作成してください。\n",
    "\n",
    "$$\n",
    "N_{h,out} =  \\frac{N_{h,in}+2P_{h}-F_{h}}{S_{h}} + 1\\\\\n",
    "N_{w,out} =  \\frac{N_{w,in}+2P_{w}-F_{w}}{S_{w}} + 1\n",
    "$$\n",
    "\n",
    "$N_{out}$ : 出力のサイズ（特徴量の数）\n",
    "\n",
    "\n",
    "$N_{in}$ : 入力のサイズ（特徴量の数）\n",
    "\n",
    "\n",
    "$P$ : ある方向へのパディングの数\n",
    "\n",
    "\n",
    "$F$ : フィルタのサイズ\n",
    "\n",
    "\n",
    "$S$ : ストライドのサイズ\n",
    "\n",
    "\n",
    "$h$が高さ方向、$w$が幅方向である"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2d:\n",
    "    \"\"\"\n",
    "\n",
    "    self.W (C_out, C_in, H, W)\n",
    "    self.B (C_out,)\n",
    "    \"\"\"\n",
    "    def __init__(self, output_ch, input_ch, filter_h, filter_w, lr, stride=1, pad=0):\n",
    "        self.output_ch = output_ch\n",
    "        self.input_ch = input_ch\n",
    "        self.filter_h = filter_h\n",
    "        self.filter_w = filter_w\n",
    "        self.lr = lr\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        self.W = np.sqrt(2/output_ch) * np.random.randn(output_ch, input_ch, filter_h, filter_w)\n",
    "        self.B = np.zeros(output_ch)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # X (N, C_in, H, W)\n",
    "        self.X_ = X\n",
    "        N, C, H, W = X.shape\n",
    "        self.out_h, self.out_w = calc_out_size(H, W, self.filter_h, self.filter_w, stride=self.stride, pad=self.pad)\n",
    "\n",
    "        col = im2col(X, self.filter_h, self.filter_w, self.stride, self.pad)\n",
    "        col_W = self.W.reshape(self.output_ch, -1).T\n",
    "        # A (N*H*W, C_out)\n",
    "        A = np.dot(col, col_W) + self.B\n",
    "        # A (N, H, W, C_out) -> (N, C_out, H, W)\n",
    "        A = A.reshape(N, self.out_h, self.out_w, -1).transpose(0, 3, 1, 2)\n",
    "\n",
    "        return A\n",
    "\n",
    "    def backward(self, dA):\n",
    "        # dA (N, C_out, H, W)\n",
    "        \n",
    "        # dW (C_out, C_in, H, W)\n",
    "        dW = np.zeros(self.W.shape)\n",
    "        iterator_dW = itertools.product(range(self.filter_h),\n",
    "                                        range(self.filter_w),\n",
    "                                        range(self.input_ch),\n",
    "                                        range(self.output_ch))\n",
    "        for s, t, k, m in iterator_dW:\n",
    "            dW[m, k, s, t] = np.sum(dA[:, m, 0:self.out_h, 0:self.out_w] * self.X_[:, k, s:s+self.out_h, t:t+self.out_w])\n",
    "        \n",
    "        # dB (C_out,)\n",
    "        dB = np.sum(dA, axis=(0, 2, 3))\n",
    "\n",
    "        # dX (N, C_in, H, W)\n",
    "        dX = np.zeros(self.X_.shape)\n",
    "        iterators_dX = itertools.product(range(self.filter_h),\n",
    "                                         range(self.filter_w),\n",
    "                                         range(self.filter_h),\n",
    "                                         range(self.filter_w),\n",
    "                                         range(self.input_ch),\n",
    "                                         range(self.output_ch))\n",
    "        for i, j, s, t, k, m in iterators_dX:\n",
    "            # 全バッチ分まとめて計算\n",
    "            if (i-s < 0) or (i-s > self.out_h-1) or (j-t < 0) or (j-t > self.out_w-1):\n",
    "                continue\n",
    "            else:\n",
    "                dX[:, k, i, j] += dA[:, m, (i-s), (j-t)] * self.W[m, k, s, t]\n",
    "\n",
    "        # update W/B\n",
    "        self.W = self.W - self.lr*dW/dA.shape[0]\n",
    "        self.B = self.B - self.lr*dB/dA.shape[0]\n",
    "\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_out_size(input_h, input_w, filter_h, filter_w, stride, pad):\n",
    "    out_h = (input_h + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (input_w + 2*pad - filter_w)//stride + 1\n",
    "    return (out_h, out_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
    "    N, C, H, W = input_data.shape\n",
    "    out_h, out_w = calc_out_size(H, W, filter_h, filter_w, stride, pad)\n",
    "\n",
    "    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
    "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "\n",
    "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
    "    N, C, H, W = input_shape\n",
    "    out_h, out_w = calc_out_size(H, W, filter_h, filter_w, stride, pad)\n",
    "    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
    "\n",
    "    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
    "\n",
    "    return img[:, :, pad:H + pad, pad:W + pad]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題3】\n",
    "#### 最大プーリング層の作成\n",
    "最大プーリング層のクラスMaxPool2Dを作成してください。プーリング層は数式で表さない方が分かりやすい部分もありますが、数式で表すとフォワードプロパゲーションは以下のようになります。\n",
    "\n",
    "$$\n",
    "a_{i,j,k} = \\max_{(p,q)\\in P_{i,j}}x_{p,q,k}\n",
    "$$\n",
    "\n",
    "$P_{i,j}$ : i行j列への出力する場合の入力配列のインデックスの集合。$S_h \\times S_w$の範囲内の行（p）と列（q）\n",
    "\n",
    "\n",
    "$S_h, S_w$ : 高さ方向（h）と幅方向（w）のストライドのサイズ\n",
    "\n",
    "\n",
    "$(p,q)\\in P_{i,j}$ : $P_{i,j}$に含まれる行（p）と列（q）のインデックス\n",
    "\n",
    "\n",
    "$a_{i,j,k}$ : 出力される配列のi行j列、kチャンネルの値\n",
    "\n",
    "\n",
    "$x_{p,q,k}$ : 入力の配列のp行q列、kチャンネルの値\n",
    "\n",
    "\n",
    "ある範囲の中でチャンネル方向の軸は残したまま最大値を計算することになります。\n",
    "\n",
    "\n",
    "バックプロパゲーションのためには、フォワードプロパゲーションのときの最大値のインデックス$(p, q)$を保持しておく必要があります。フォワード時に最大値を持っていた箇所にそのままの誤差を流し、そこ以外には0を入れるためです。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool2D:\n",
    "    \n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "    \n",
    "    def forward(self, X):\n",
    "        ## C_in = C_out?\n",
    "        # X (N, C_in, H, W)\n",
    "        N, C, H, W = X.shape\n",
    "        self.input_shape = (N, C, H, W)\n",
    "        out_h, out_w = calc_out_size(H, W, self.pool_h, self.pool_w, stride=self.stride, pad=self.pad)\n",
    "        \n",
    "        col = im2col(X, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        # col (-1, pool_h*pool_w)\n",
    "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
    "        \n",
    "        # index保存\n",
    "        self.max_index = np.argmax(col, axis=1)\n",
    "\n",
    "        out = np.max(col, axis=1)\n",
    "\n",
    "        # out (N, C_out, H, W)\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dX):\n",
    "        # dX (N, C, H, W)\n",
    "        dX = dX.transpose(0, 2, 3, 1)\n",
    "\n",
    "        pool_size = self.pool_h * self.pool_w \n",
    "        \n",
    "        dmax = np.zeros((dX.size, pool_size))\n",
    "        # 最大値を取るindexに誤差を代入\n",
    "        dmax[np.arange(self.max_index.size), self.max_index.flatten()] = dX.flatten()\n",
    "        dmax = dmax.reshape(dX.shape + (pool_size,))\n",
    "        \n",
    "        dcol = dmax.reshape(dmax.shape[0]*dmax.shape[1]*dmax.shape[2], -1)\n",
    "        dX_reshaped = col2im(dcol, self.input_shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        \n",
    "        return dX_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,2,3],[4,3,2], [6,5,4], [3,4,5], [6,7,5]])\n",
    "print(a)\n",
    "print(a[range(a.shape[0]), np.argmax(a, axis=1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題4】\n",
    "#### （アドバンス課題）平均プーリングの作成\n",
    "平均プーリング層のクラスAveragePool2Dを作成してください。\n",
    "\n",
    "\n",
    "範囲内の最大値ではなく、平均値を出力とするプーリング層です。\n",
    "\n",
    "\n",
    "画像認識関係では最大プーリング層が一般的で、平均プーリングはあまり使われません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AveragePool2D:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題5】\n",
    "#### 平滑化\n",
    "平滑化するためのFlattenクラスを作成してください。\n",
    "\n",
    "\n",
    "フォワードのときはチャンネル、高さ、幅の3次元を1次元にreshapeします。その値は記録しておき、バックワードのときに再びreshapeによって形を戻します。\n",
    "\n",
    "\n",
    "この平滑化のクラスを挟むことで出力前の全結合層に適した配列を作ることができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten:\n",
    "    \"\"\"\n",
    "    (N, C_out, H, W)を(N, C_out*H*W)にする\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.input_shape = X.shape\n",
    "\n",
    "        out = X.reshape(self.input_shape[0], -1)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dX):\n",
    "        dX_reshaped = dX.reshape(self.input_shape)\n",
    "\n",
    "        return dX_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[[[1,2],[3,4]],[[5,6],[7,8]]],[[[9,10],[11,12]],[[13,14],[15,16]]]])\n",
    "b = a.reshape(a.shape[0], -1)\n",
    "print(a)\n",
    "print(b)\n",
    "print(b.reshape(a.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.検証"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題6】\n",
    "#### 学習と推定\n",
    "作成したConv2dを使用してMNISTを学習・推定し、Accuracyを計算してください。\n",
    "\n",
    "\n",
    "精度は低くともまずは動くことを目指してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DNN実装を拝借する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "\n",
    "    Attribute\n",
    "    --------\n",
    "    self.W : ndarray(n_nodes1, n_nodes2)\n",
    "      重み\n",
    "    self.B : ndarray(n_node2,)\n",
    "      バイアス\n",
    "    self.H : float\n",
    "      前イテレーションまでの勾配の二乗和\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
    "        self.B = initializer.B(n_nodes2)\n",
    "        self.HW = np.zeros(self.W.shape)\n",
    "        self.HB = np.zeros(self.B.shape)\n",
    "        pass\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"\n",
    "        self.X_ = X\n",
    "\n",
    "        A = X@self.W + self.B\n",
    "\n",
    "        return A\n",
    "\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "\n",
    "        # 更新\n",
    "        dZ = self.optimizer.update(self, dA)\n",
    "\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    \"\"\"\n",
    "    ソフトマックス関数\n",
    "\n",
    "    Attribute\n",
    "    ----------\n",
    "    self.loss\n",
    "        出力の交差エントロピー誤差\n",
    "    \"\"\"\n",
    "    def forward(self, A):\n",
    "        Z = np.exp(A) / np.sum(np.exp(A), axis=1).reshape(-1, 1)\n",
    "        \n",
    "        return Z\n",
    "\n",
    "    def backward(self, Z, Y):\n",
    "        dA = Z - Y\n",
    "        self.loss = self.calc_cross_entropy_loss(Y, Z)\n",
    "\n",
    "        return dA\n",
    "    \n",
    "    def calc_cross_entropy_loss(self, y_true, y_pred):\n",
    "        n_samples = y_true.shape[0]\n",
    "\n",
    "        cross_entropy_loss = (-1 * (np.sum(y_true*np.log(y_pred)))) / n_samples\n",
    " \n",
    "        return cross_entropy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    \"\"\"\n",
    "    ReLU関数\n",
    "\n",
    "    Attribute\n",
    "    ----------\n",
    "    self.A\n",
    "        活性化関数の入力\n",
    "    \"\"\"\n",
    "    def forward(self, A):\n",
    "        self.pos = A > 0\n",
    "        Z = self.pos*A\n",
    "        return Z\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        if dZ.shape == self.pos.shape:\n",
    "            dA = self.pos*dZ\n",
    "        else:\n",
    "            dA = self.pos*dZ[:, np.newaxis, :]\n",
    "\n",
    "        return dA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeInitializer:\n",
    "    \"\"\"\n",
    "    Heの初期値\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      後の層のノード数\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    self.sigma\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes2):\n",
    "        self.sigma = np.sqrt(2/n_nodes2)\n",
    "    \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "          重み\n",
    "        \"\"\"\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        \n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "          バイアス\n",
    "        \"\"\"\n",
    "        B = np.zeros((n_nodes2,))\n",
    "\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, layer, dA):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "        dB = np.mean(dA, axis=0)\n",
    "        dW = (layer.X_.T@dA) / layer.X_.shape[0]\n",
    "        dZ = dA@layer.W.T\n",
    "\n",
    "        layer.B = layer.B - self.lr*dB\n",
    "        layer.W = layer.W - self.lr*dW\n",
    "\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    \"\"\"\n",
    "    AdaGrad法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, layer, dA):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "        # calc partial\n",
    "        dB = np.mean(dA, axis=0)\n",
    "        dW = (layer.X_.T@dA) / dA.shape[0]\n",
    "        dZ = dA@layer.W.T\n",
    "\n",
    "        # update HB, HW\n",
    "        layer.HB = layer.HB + dB**2\n",
    "        layer.HW = layer.HW + dW**2\n",
    "\n",
    "        # update W and B\n",
    "        layer.B = layer.B - self.lr*(1/(np.sqrt(layer.HB+1e-7)))*dB\n",
    "        layer.W = layer.W - self.lr*(1/(np.sqrt(layer.HW+1e-7)))*dW\n",
    "\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一旦\n",
    "\n",
    "*input -> Conv -> ReLU -> Pooling -> Conv -> ReLU -> Pooling -> Flatten (-> Affine -> ReLU)+ -> Affine -> Softmax*\n",
    "\n",
    "で試してみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchConvolutionalNeuralNetworkClassifier:\n",
    "    \"\"\"\n",
    "    畳み込みニューラルネットワーク分類器\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    alpha : float\n",
    "    sigma : float\n",
    "    batch_size : int\n",
    "    epoch : int\n",
    "    verbose : bool\n",
    "    output_ch : int\n",
    "    filter_size : tuple(int, int)\n",
    "    pooling_size : tuple(int, int)\n",
    "    stride : int\n",
    "    pad : int\n",
    "    n_nodes : list[int...]\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    self.loss : list\n",
    "    self.val_loss : list\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=0.01, sigma=0.01, batch_size=20, epoch=1, verbose=True,\n",
    "                 output_ch=10, filter_size=(3, 3), pooling_size=(2, 2), stride=1, pad=0, \n",
    "                 n_nodes=[100, 10]):\n",
    "        self.alpha = alpha\n",
    "        self.sigma = sigma\n",
    "        self.batch_size = batch_size\n",
    "        self.epoch = epoch\n",
    "        self.verbose = verbose\n",
    "        self.output_ch = output_ch\n",
    "        self.filter_size = filter_size\n",
    "        self.stride = stride\n",
    "        self.pad = pad \n",
    "        self.pooling_size = pooling_size\n",
    "        self.n_nodes = n_nodes\n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を学習する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X (N, C, H, W)\n",
    "        y (N,)\n",
    "        X_val (N, C, H, W)\n",
    "        y_val (N)\n",
    "        \"\"\"\n",
    "        # def optimizer\n",
    "        optimizer = SGD(self.alpha)\n",
    "\n",
    "        # def layers\n",
    "        self.layers = []\n",
    "        self.input_ch = X.shape[1]\n",
    "        \n",
    "        # first layer (Conv2d)\n",
    "        self.layers.append(Conv2d(math.ceil((self.input_ch+self.output_ch)/2),\n",
    "                                  self.input_ch,\n",
    "                                  *self.filter_size,\n",
    "                                  self.alpha,\n",
    "                                  stride=self.stride,\n",
    "                                  pad=self.pad))\n",
    "        # activator (ReLU)\n",
    "        self.layers.append(ReLU())\n",
    "\n",
    "        # pooling (MaxPool2D)\n",
    "        self.layers.append(MaxPool2D(*self.pooling_size, stride=self.stride, pad=self.pad))\n",
    "\n",
    "        # calc output size after 1st layer\n",
    "        output_size_tmp = calc_out_size(*calc_out_size(X.shape[2],\n",
    "                                                       X.shape[3],\n",
    "                                                       *self.filter_size,\n",
    "                                                       self.stride,\n",
    "                                                       self.pad),\n",
    "                                        *self.pooling_size,\n",
    "                                        self.stride,\n",
    "                                        self.pad)\n",
    "\n",
    "        # second layer (Conv2d)\n",
    "        self.layers.append(Conv2d(self.output_ch,\n",
    "                                  math.ceil((self.input_ch+self.output_ch)/2),\n",
    "                                  *self.filter_size,\n",
    "                                  self.alpha,\n",
    "                                  stride=self.stride,\n",
    "                                  pad=self.pad))\n",
    "        # activator (ReLU)\n",
    "        self.layers.append(ReLU())\n",
    "\n",
    "        # pooling (MaxPool2D)\n",
    "        self.layers.append(MaxPool2D(*self.pooling_size, stride=self.stride, pad=self.pad))\n",
    "\n",
    "        # calc output size after 2nd layer\n",
    "        self.output_size = calc_out_size(*calc_out_size(*output_size_tmp,\n",
    "                                                        *self.filter_size,\n",
    "                                                        self.stride,\n",
    "                                                        self.pad),\n",
    "                                         *self.pooling_size,\n",
    "                                         self.stride,\n",
    "                                         self.pad)\n",
    "\n",
    "        # flatten (Flatten)\n",
    "        self.layers.append(Flatten())\n",
    "\n",
    "        # calc flatten output size\n",
    "        self.output_size_flatten = self.output_ch*self.output_size[0]*self.output_size[1]\n",
    "\n",
    "        for i, (n_nodes1, n_nodes2) in enumerate(zip([self.output_size_flatten]+self.n_nodes[:-1], self.n_nodes)):\n",
    "            if i+1 != len(self.n_nodes):\n",
    "                # def initializer\n",
    "                initializer = HeInitializer(n_nodes2)\n",
    "\n",
    "                # def layer\n",
    "                self.layers.append(FC(n_nodes1, n_nodes2, initializer, optimizer))\n",
    "\n",
    "                # def activator\n",
    "                self.layers.append(ReLU())\n",
    "            # last layer\n",
    "            else:\n",
    "                # def initializer\n",
    "                initializer = HeInitializer(n_nodes2)\n",
    "\n",
    "                # def layer\n",
    "                self.layers.append(FC(n_nodes1, n_nodes2, initializer, optimizer))\n",
    "\n",
    "                # def activator\n",
    "                self.layers.append(Softmax())\n",
    "    \n",
    "        ## one-hot encoding\n",
    "        enc = OneHotEncoder(handle_unknown='ignore', sparse=False).fit(y[:, np.newaxis])\n",
    "        if y_val is not None:\n",
    "            y_val_one_hot = enc.transform(y_val[:, np.newaxis])\n",
    "\n",
    "        ## loss list\n",
    "        self.loss = []\n",
    "        self.val_loss = []\n",
    "\n",
    "        if self.verbose:\n",
    "            #verboseをTrueにした際は学習過程などを出力する\n",
    "            print('start learning')\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # learning\n",
    "        for e in range(self.epoch):\n",
    "            print(f'start epoch {e+1}')\n",
    "\n",
    "            ## mini_batch\n",
    "            get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch_size, seed=e)\n",
    "\n",
    "            ## loss list\n",
    "            self.loss.append([])\n",
    "\n",
    "            for i, (mini_X_train, mini_y_train) in enumerate(get_mini_batch):\n",
    "                ## one-hot encoding\n",
    "                mini_y_train_one_hot = enc.transform(mini_y_train[:, np.newaxis])\n",
    "                \n",
    "                # save output\n",
    "                output = None\n",
    "                for layer in self.layers:\n",
    "                    # forward\n",
    "                    if output is None:\n",
    "                        output = layer.forward(mini_X_train)\n",
    "                    else:\n",
    "                        output = layer.forward(output)\n",
    "                    \n",
    "                ## update weight, bias\n",
    "                ## append loss\n",
    "                dLoss = None\n",
    "                for layer in self.layers[::-1]:\n",
    "                    # backward\n",
    "                    if dLoss is None:\n",
    "                        dLoss = layer.backward(output, mini_y_train_one_hot)\n",
    "                        ## append loss \n",
    "                        self.loss[e].append(layer.loss)\n",
    "                    else:\n",
    "                        dLoss = layer.backward(dLoss)\n",
    "\n",
    "                ## print progress\n",
    "                if self.verbose:\n",
    "                    print(f'\\r{i+1}/{len(get_mini_batch)} loop finished (average train loss: {sum(self.loss[-1])/len(self.loss[-1]):.5f})', end='')\n",
    "\n",
    "            ## validation\n",
    "            if (X_val is not None) and (y_val is not None):\n",
    "                ### prediction\n",
    "                output_val = None\n",
    "                for layer in self.layers:\n",
    "                    # forward\n",
    "                    if output_val is None:\n",
    "                        output_val = layer.forward(X_val)\n",
    "                    else:\n",
    "                        output_val = layer.forward(output_val)\n",
    "\n",
    "                ### append loss\n",
    "                self.val_loss.append(self.layers[-1].calc_cross_entropy_loss(y_val_one_hot, output_val))\n",
    "\n",
    "            if self.verbose:\n",
    "                print(' : Complete!!')\n",
    "            if y_val is not None:\n",
    "                print(f'epoch {e+1} valid loss: {self.val_loss[-1]}')\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "        # output loss\n",
    "        print(f'last train loss: {self.loss[-1][-1]}')\n",
    "\n",
    "        print(f'Done! elapsed time: {elapsed_time:.5f}s')\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を使い推定する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X (N, C, H, W)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            (n_samples, 1)\n",
    "        \"\"\"\n",
    "        # save output\n",
    "        output = None\n",
    "\n",
    "        ## forward propagation\n",
    "        for layer in self.layers:\n",
    "            if output is None:\n",
    "                output = layer.forward(X)\n",
    "            else:\n",
    "                output = layer.forward(output)\n",
    "\n",
    "        return np.argmax(output, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self._X[p0:p1], self._y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self._X[p0:p1], self._y[p0:p1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習してみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データのロード\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train[:, np.newaxis]\n",
    "X_test = X_test[:, np.newaxis]\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習\n",
    "params = {\n",
    "    'alpha':0.03,\n",
    "    'batch_size':600,\n",
    "    'epoch':30,\n",
    "}\n",
    "scratchCNN = ScratchConvolutionalNeuralNetworkClassifier(**params)\n",
    "%time scratchCNN.fit(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_test_pred = scratchCNN.predict(X_test)\n",
    "print(y_test_pred)\n",
    "print(y_test)\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_test_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "conf_m = confusion_matrix(y_test, y_test_pred) \n",
    "print(conf_m)\n",
    "sample_sum = np.sum(conf_m, axis=1)\n",
    "for i in range(conf_m.shape[0]):\n",
    "    correct = conf_m[i][i]\n",
    "    incorrect = sample_sum[i] - correct\n",
    "    print(f'{i}: {correct}:{incorrect} {correct/sample_sum[i]:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(model):\n",
    "    loss = model.loss\n",
    "    val_loss = model.val_loss\n",
    "\n",
    "    %matplotlib inline\n",
    "\n",
    "    # train lossの詳細な推移\n",
    "    for i in range(model.epoch):\n",
    "        fig = plt.figure(figsize=(12, 4))\n",
    "        ax1 = fig.add_subplot(121)\n",
    "        plt.title(f'cross entropy loss: epoch{i+1}')\n",
    "        plt.plot(range(len(loss[i])), loss[i], label='loss')\n",
    "        plt.legend()\n",
    "\n",
    "        ## 移動平均\n",
    "        ax2 = fig.add_subplot(122)\n",
    "        rolling_loss = []\n",
    "        rolling_val_loss = []\n",
    "        window_size = 20\n",
    "        for j in range(len(loss[i])-window_size):\n",
    "            rolling_loss.append(sum(loss[i][j:j+window_size])/window_size)\n",
    "        plt.title(f'rolling loss: epoch{i+1}')  \n",
    "        plt.plot(range(len(rolling_loss)), rolling_loss, label='rolling loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    # エポックあたりのloss推移(バリデーション含む)\n",
    "    if len(val_loss) > 0:\n",
    "        plt.title('cross entropy loss: epoches')\n",
    "        plt.plot(range(1, len(loss)+1), [sum(x)/len(x) for x in loss], label='loss')\n",
    "        plt.plot(range(1, len(val_loss)+1), val_loss, label='val_loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    # 全体\n",
    "    plt.title('cross entropy loss: all')\n",
    "    plt.plot(range(len(sum(loss, []))), sum(loss, []), label='loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(scratchCNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題9】\n",
    "#### 出力サイズとパラメータ数の計算\n",
    "CNNモデルを構築する際には、全結合層に入力する段階で特徴量がいくつになっているかを事前に計算する必要があります。\n",
    "\n",
    "\n",
    "また、巨大なモデルを扱うようになると、メモリや計算速度の関係でパラメータ数の計算は必須になってきます。フレームワークでは各層のパラメータ数を表示させることが可能ですが、意味を理解していなくては適切な調整が行えません。\n",
    "\n",
    "\n",
    "以下の3つの畳み込み層の出力サイズとパラメータ数を計算してください。パラメータ数についてはバイアス項も考えてください。\n",
    "\n",
    "\n",
    "1.\n",
    "\n",
    "\n",
    "- 入力サイズ : 144×144, 3チャンネル\n",
    "- フィルタサイズ : 3×3, 6チャンネル\n",
    "- ストライド : 1\n",
    "- パディング : なし\n",
    "\n",
    "2.\n",
    "\n",
    "\n",
    "- 入力サイズ : 60×60, 24チャンネル\n",
    "- フィルタサイズ : 3×3, 48チャンネル\n",
    "- ストライド　: 1\n",
    "- パディング : なし\n",
    "\n",
    "3.\n",
    "\n",
    "\n",
    "- 入力サイズ : 20×20, 10チャンネル\n",
    "- フィルタサイズ: 3×3, 20チャンネル\n",
    "- ストライド : 2\n",
    "- パディング : なし\n",
    "\n",
    "＊最後の例は丁度良く畳み込みをすることができない場合です。フレームワークでは余ったピクセルを見ないという処理が行われることがあるので、その場合を考えて計算してください。端が欠けてしまうので、こういった設定は好ましくないという例です。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "N_{h,out} =  \\frac{N_{h,in}+2P_{h}-F_{h}}{S_{h}} + 1 \\\\\n",
    "N_{w,out} =  \\frac{N_{w,in}+2P_{w}-F_{w}}{S_{w}} + 1 \\\\\n",
    "N_p = (F_h \\times F_w \\times C_{out} \\times C_{in}) + C_{out}\n",
    "$$\n",
    "\n",
    "1.\n",
    ">- 入力サイズ : 144×144, 3チャンネル\n",
    "- フィルタサイズ : 3×3, 6チャンネル\n",
    "- ストライド : 1\n",
    "- パディング : なし\n",
    "\n",
    "#### 出力サイズ\n",
    "$$\n",
    "N_{h,out} = \\frac {144+2 \\times 0-3}{1} + 1 = \\textbf{142} \\\\\n",
    "N_{w,out} = \\frac {144+2 \\times 0-3}{1} + 1 = \\textbf{142} \\\\\n",
    "$$\n",
    "#### パラメータ数\n",
    "$$\n",
    "N_p = (3 \\times 3 \\times 6 \\times 3) + 6 = \\textbf{60}\n",
    "$$\n",
    "2.\n",
    ">- 入力サイズ : 60×60, 24チャンネル\n",
    "- フィルタサイズ : 3×3, 48チャンネル\n",
    "- ストライド　: 1\n",
    "- パディング : なし\n",
    "\n",
    "#### 出力サイズ\n",
    "$$\n",
    "N_{h,out} = \\frac {60+2 \\times 0-3}{1} + 1 = \\textbf{58} \\\\\n",
    "N_{w,out} = \\frac {60+2 \\times 0-3}{1} + 1 = \\textbf{58} \\\\\n",
    "$$\n",
    "#### パラメータ数\n",
    "$$\n",
    "N_p = (3 \\times 3 \\times 48 \\times 24) + 48 = \\textbf{10416}\n",
    "$$\n",
    "3.\n",
    ">- 入力サイズ : 20×20, 10チャンネル\n",
    "- フィルタサイズ: 3×3, 20チャンネル\n",
    "- ストライド : 2\n",
    "- パディング : なし\n",
    "\n",
    "#### 出力サイズ\n",
    "余ったピクセルは見ないため、出力の**小数点以下を切り捨てる**。\n",
    "$$\n",
    "N_{h,out} = \\frac {20+2 \\times 0-3}{2} + 1 = 9.5 \\rightarrow \\textbf{9} \\\\\n",
    "N_{w,out} = \\frac {20+2 \\times 0-3}{2} + 1 = 9.5 \\rightarrow \\textbf{9} \\\\\n",
    "$$\n",
    "#### パラメータ数\n",
    "$$\n",
    "N_p = (3 \\times 3 \\times 20 \\times 10) + 20 = \\textbf{2000}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}