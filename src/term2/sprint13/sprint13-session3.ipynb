{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint\n",
    "## ディープラーニングフレームワーク1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.このSprintについて\n",
    "\n",
    "### Sprintの目的\n",
    "- フレームワークのコードを読めるようにする\n",
    "- フレームワークを習得し続けられるようになる\n",
    "- 理論を知っている範囲をフレームワークで動かす\n",
    "\n",
    "### どのように学ぶか\n",
    "TensorFLowのサンプルコードを元に、これまで扱ってきたデータセットを学習していきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.コードリーディング\n",
    "\n",
    "TensorFLowによって2値分類を行うサンプルコードを載せました。今回はこれをベースにして進めます。\n",
    "\n",
    "\n",
    "tf.kerasやtf.estimatorなどの高レベルAPIは使用していません。低レベルなところから見ていくことにします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題1】\n",
    "#### スクラッチを振り返る\n",
    "ここまでのスクラッチを振り返り、ディープラーニングを実装するためにはどのようなものが必要だったかを列挙してください。\n",
    "\n",
    "\n",
    "（例）\n",
    "\n",
    "\n",
    "- 重みを初期化する必要があった\n",
    "- エポックのループが必要だった\n",
    "\n",
    "それらがフレームワークにおいてはどのように実装されるかを今回覚えていきましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- forward, backwardの実装が必要\n",
    "- 最適化手法の実装が必要"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセットの用意\n",
    "以前から使用しているIrisデータセットを使用します。以下のサンプルコードでは`Iris.csv`が同じ階層にある想定です。\n",
    "\n",
    "\n",
    "[Iris Species](https://www.kaggle.com/uciml/iris/data)\n",
    "\n",
    "\n",
    "目的変数は`Species`ですが、3種類ある中から以下の2種類のみを取り出して使用します。\n",
    "\n",
    "```\n",
    "Iris-versicolor\n",
    "Iris-virginica\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題2】\n",
    "#### スクラッチとTensorFlowの対応を考える\n",
    "以下のサンプルコードを見て、先ほど列挙した「ディープラーニングを実装するために必要なもの」がTensorFlowではどう実装されているかを確認してください。\n",
    "\n",
    "\n",
    "それを簡単に言葉でまとめてください。単純な一対一の対応であるとは限りません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#《サンプルコード》\n",
    "# ＊バージョン1.5から1.14の間で動作を確認済みです。\n",
    "\"\"\"\n",
    "TensorFlowで実装したニューラルネットワークを使いIrisデータセットを2値分類する\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "# データセットの読み込み\n",
    "dataset_path =\"Iris.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "# データフレームから条件抽出\n",
    "df = df[(df[\"Species\"] == \"Iris-versicolor\")|(df[\"Species\"] == \"Iris-virginica\")]\n",
    "y = df[\"Species\"]\n",
    "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "# ラベルを数値に変換\n",
    "y[y=='Iris-versicolor'] = 0\n",
    "y[y=='Iris-virginica'] = 1\n",
    "y = y.astype(np.int)[:, np.newaxis]\n",
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From C:\\Users\\190450781\\Anaconda3\\envs\\dic\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.where in 2.0, which has the same broadcast rule as np.where\nEpoch 0, loss : 2.9396, val_loss : 10.0471, acc : 0.750, val_acc : 0.375\nEpoch 1, loss : 0.0000, val_loss : 4.4855, acc : 1.000, val_acc : 0.750\nEpoch 2, loss : 1.4334, val_loss : 7.3675, acc : 0.750, val_acc : 0.562\nEpoch 3, loss : 0.0000, val_loss : 7.1097, acc : 1.000, val_acc : 0.750\nEpoch 4, loss : 1.4344, val_loss : 10.7628, acc : 0.750, val_acc : 0.500\nEpoch 5, loss : 0.0000, val_loss : 6.0986, acc : 1.000, val_acc : 0.750\nEpoch 6, loss : 0.0004, val_loss : 6.3735, acc : 1.000, val_acc : 0.688\nEpoch 7, loss : 0.0000, val_loss : 2.4152, acc : 1.000, val_acc : 0.875\nEpoch 8, loss : 0.0000, val_loss : 3.2437, acc : 1.000, val_acc : 0.812\nEpoch 9, loss : 0.0000, val_loss : 0.1243, acc : 1.000, val_acc : 0.938\ntest_acc : 0.950\n"
    }
   ],
   "source": [
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 1\n",
    "# 計算グラフに渡す引数の形を決める\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "# trainのミニバッチイテレータ\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
    "def example_net(x):\n",
    "    \"\"\"\n",
    "    単純な3層ニューラルネットワーク\n",
    "    \"\"\"\n",
    "    # 重みとバイアスの宣言\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
    "    return layer_output\n",
    "# ネットワーク構造の読み込み                               \n",
    "logits = example_net(X)\n",
    "# 目的関数\n",
    "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
    "# 最適化手法\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "# 推定結果\n",
    "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
    "# 指標値計算\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "# variableの初期化\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 計算グラフの実行\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        # エポックごとにループ\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # ミニバッチごとにループ\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "        total_loss /= n_samples\n",
    "        total_acc /= n_samples\n",
    "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 重み・バイアスの初期化\n",
    "\n",
    "サンプルコードでは`tf.Variable`型で管理を行っている。\n",
    "\n",
    "sessionを開始した際に一度だけ`tf.global_variables_initializer()`を実行することで、Variable（重みやバイアス）を初期化している。\n",
    "\n",
    "- エポックのループ\n",
    "\n",
    "スクラッチ実装の際と同じようにforループを使って実装している。\n",
    "\n",
    "高レベルAPIである`tf.estimator`を使う場合だと`num_epochs`引数で指定できるらしい。\n",
    "\n",
    "- forward, backward\n",
    "- 最適化手法\n",
    "\n",
    "最適化手法は実装済みで、損失関数をminimizeするように指定することでweightsとbiasesが最適化される。\n",
    "\n",
    "実際にsession内でrunする際には、計算グラフの目的地点である`train_op`や`accracy`などに対して行うだけで、それより前の計算グラフは自動的に実行される。\n",
    "\n",
    "スクラッチ実装の時のようにforループを使って出力を保存しながらレイヤーでの計算を行う必要はない。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.他のデータセットへの適用\n",
    "\n",
    "これまで扱ってきた小さなデータセットがいくつかあります。上記サンプルコードを書き換え、これらに対して学習・推定を行うニューラルネットワークを作成してください。\n",
    "\n",
    "\n",
    "- Iris（3種類全ての目的変数を使用）\n",
    "- House Prices\n",
    "\n",
    "どのデータセットもtrain, val, testの3種類に分けて使用してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題3】\n",
    "#### 3種類全ての目的変数を使用したIrisのモデルを作成\n",
    "Irisデータセットのtrain.csvの中で、目的変数Speciesに含まれる3種類全てを分類できるモデルを作成してください。\n",
    "\n",
    "\n",
    "[Iris Species](https://www.kaggle.com/uciml/iris/data)\n",
    "\n",
    "\n",
    "2クラスの分類と3クラス以上の分類の違いを考慮してください。それがTensorFlowでどのように書き換えられるかを公式ドキュメントなどを参考に調べてください。\n",
    "\n",
    "\n",
    "**《ヒント》**\n",
    "\n",
    "\n",
    "以下の2箇所は2クラス分類特有の処理です。\n",
    "\n",
    "```python\n",
    "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
    "```\n",
    "\n",
    "```python\n",
    "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
    "```\n",
    "\n",
    "メソッドは以下のように公式ドキュメントを確認してください。\n",
    "\n",
    "\n",
    "[tf.nn.sigmoid_cross_entropy_with_logits  |  TensorFlow](https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits)\n",
    "\n",
    "\n",
    "[tf.math.sign  |  TensorFlow](https://www.tensorflow.org/api_docs/python/tf/math/sign)\n",
    "\n",
    "\n",
    "＊tf.signとtf.math.signは同じ働きをします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "      Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm  \\\n0      1            5.1           3.5            1.4           0.2   \n1      2            4.9           3.0            1.4           0.2   \n2      3            4.7           3.2            1.3           0.2   \n3      4            4.6           3.1            1.5           0.2   \n4      5            5.0           3.6            1.4           0.2   \n..   ...            ...           ...            ...           ...   \n145  146            6.7           3.0            5.2           2.3   \n146  147            6.3           2.5            5.0           1.9   \n147  148            6.5           3.0            5.2           2.0   \n148  149            6.2           3.4            5.4           2.3   \n149  150            5.9           3.0            5.1           1.8   \n\n            Species  \n0       Iris-setosa  \n1       Iris-setosa  \n2       Iris-setosa  \n3       Iris-setosa  \n4       Iris-setosa  \n..              ...  \n145  Iris-virginica  \n146  Iris-virginica  \n147  Iris-virginica  \n148  Iris-virginica  \n149  Iris-virginica  \n\n[150 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>SepalLengthCm</th>\n      <th>SepalWidthCm</th>\n      <th>PetalLengthCm</th>\n      <th>PetalWidthCm</th>\n      <th>Species</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>5.1</td>\n      <td>3.5</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>4.9</td>\n      <td>3.0</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>4.7</td>\n      <td>3.2</td>\n      <td>1.3</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>4.6</td>\n      <td>3.1</td>\n      <td>1.5</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>5.0</td>\n      <td>3.6</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>146</td>\n      <td>6.7</td>\n      <td>3.0</td>\n      <td>5.2</td>\n      <td>2.3</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>146</th>\n      <td>147</td>\n      <td>6.3</td>\n      <td>2.5</td>\n      <td>5.0</td>\n      <td>1.9</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>148</td>\n      <td>6.5</td>\n      <td>3.0</td>\n      <td>5.2</td>\n      <td>2.0</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>148</th>\n      <td>149</td>\n      <td>6.2</td>\n      <td>3.4</td>\n      <td>5.4</td>\n      <td>2.3</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>149</th>\n      <td>150</td>\n      <td>5.9</td>\n      <td>3.0</td>\n      <td>5.1</td>\n      <td>1.8</td>\n      <td>Iris-virginica</td>\n    </tr>\n  </tbody>\n</table>\n<p>150 rows × 6 columns</p>\n</div>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "(150, 4)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "(150, 3)"
     },
     "metadata": {}
    }
   ],
   "source": [
    "df = pd.read_csv(dataset_path)\n",
    "display(df)\n",
    "X = np.array(df.iloc[:, 1:5])\n",
    "y = np.array(pd.get_dummies(df.iloc[:, 5]))\n",
    "display(X.shape)\n",
    "display(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.astype(np.int)\n",
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 0, loss : 4.9765, val_loss : 15.4531, acc : 0.833, val_acc : 0.792\nEpoch 1, loss : 3.9537, val_loss : 10.6266, acc : 0.500, val_acc : 0.958\nEpoch 2, loss : 2.1317, val_loss : 9.4606, acc : 1.000, val_acc : 0.750\nEpoch 3, loss : 0.9530, val_loss : 7.3960, acc : 1.000, val_acc : 0.833\nEpoch 4, loss : 0.5371, val_loss : 4.9845, acc : 1.000, val_acc : 0.917\nEpoch 5, loss : 0.3341, val_loss : 4.2910, acc : 1.000, val_acc : 0.958\nEpoch 6, loss : 0.0850, val_loss : 6.0582, acc : 1.000, val_acc : 0.833\nEpoch 7, loss : 0.0529, val_loss : 4.2008, acc : 1.000, val_acc : 0.875\nEpoch 8, loss : 0.0367, val_loss : 4.0703, acc : 1.000, val_acc : 0.917\nEpoch 9, loss : 0.0219, val_loss : 4.2483, acc : 1.000, val_acc : 0.875\ntest_acc : 0.967\n"
    }
   ],
   "source": [
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 3\n",
    "# 計算グラフに渡す引数の形を決める\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "# trainのミニバッチイテレータ\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
    "def example_net(x):\n",
    "    \"\"\"\n",
    "    単純な3層ニューラルネットワーク\n",
    "    \"\"\"\n",
    "    # 重みとバイアスの宣言\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1], 0, 0.1)),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2], 0, 0.1)),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes], 0, 0.1))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1], 0, 0.1)),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2], 0, 0.1)),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes], 0, 0.1))\n",
    "    }\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
    "    layer_output = tf.nn.softmax(layer_output)\n",
    "    return layer_output\n",
    "# ネットワーク構造の読み込み                               \n",
    "logits = example_net(X)\n",
    "# 目的関数\n",
    "loss_op = -tf.reduce_sum(Y*tf.log(logits+1e-12))\n",
    "# 最適化手法\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "# 推定結果\n",
    "correct_pred = tf.equal(tf.argmax(Y, axis=1), tf.argmax(logits, axis=1))\n",
    "# 指標値計算\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "# variableの初期化\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 計算グラフの実行\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        # エポックごとにループ\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
    "        total_val_loss = 0\n",
    "        total_val_acc = 0\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # ミニバッチごとにループ\n",
    "            _, loss, acc = sess.run([train_op, loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "            total_val_loss += val_loss\n",
    "            total_val_acc += val_acc\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "        total_loss /= total_batch\n",
    "        total_acc /= total_batch\n",
    "        total_val_loss /= total_batch\n",
    "        total_val_acc /= total_batch\n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題4】\n",
    "#### House Pricesのモデルを作成\n",
    "回帰問題のデータセットであるHouse Pricesを使用したモデルを作成してください。\n",
    "\n",
    "\n",
    "[House Prices: Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)\n",
    "\n",
    "\n",
    "この中の`train.csv`をダウンロードし、目的変数として`SalePrice`、説明変数として、`GrLivArea`と`YearBuilt`を使ってください。説明変数はさらに増やしても構いません。\n",
    "\n",
    "\n",
    "分類問題と回帰問題の違いを考慮してください。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "        Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n0        1          60       RL         65.0     8450   Pave   NaN      Reg   \n1        2          20       RL         80.0     9600   Pave   NaN      Reg   \n2        3          60       RL         68.0    11250   Pave   NaN      IR1   \n3        4          70       RL         60.0     9550   Pave   NaN      IR1   \n4        5          60       RL         84.0    14260   Pave   NaN      IR1   \n...    ...         ...      ...          ...      ...    ...   ...      ...   \n1455  1456          60       RL         62.0     7917   Pave   NaN      Reg   \n1456  1457          20       RL         85.0    13175   Pave   NaN      Reg   \n1457  1458          70       RL         66.0     9042   Pave   NaN      Reg   \n1458  1459          20       RL         68.0     9717   Pave   NaN      Reg   \n1459  1460          20       RL         75.0     9937   Pave   NaN      Reg   \n\n     LandContour Utilities  ... PoolArea PoolQC  Fence MiscFeature MiscVal  \\\n0            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n1            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n2            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n3            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n4            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n...          ...       ...  ...      ...    ...    ...         ...     ...   \n1455         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n1456         Lvl    AllPub  ...        0    NaN  MnPrv         NaN       0   \n1457         Lvl    AllPub  ...        0    NaN  GdPrv        Shed    2500   \n1458         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n1459         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n\n     MoSold YrSold  SaleType  SaleCondition  SalePrice  \n0         2   2008        WD         Normal     208500  \n1         5   2007        WD         Normal     181500  \n2         9   2008        WD         Normal     223500  \n3         2   2006        WD        Abnorml     140000  \n4        12   2008        WD         Normal     250000  \n...     ...    ...       ...            ...        ...  \n1455      8   2007        WD         Normal     175000  \n1456      2   2010        WD         Normal     210000  \n1457      5   2010        WD         Normal     266500  \n1458      4   2010        WD         Normal     142125  \n1459      6   2008        WD         Normal     147500  \n\n[1460 rows x 81 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>MSSubClass</th>\n      <th>MSZoning</th>\n      <th>LotFrontage</th>\n      <th>LotArea</th>\n      <th>Street</th>\n      <th>Alley</th>\n      <th>LotShape</th>\n      <th>LandContour</th>\n      <th>Utilities</th>\n      <th>...</th>\n      <th>PoolArea</th>\n      <th>PoolQC</th>\n      <th>Fence</th>\n      <th>MiscFeature</th>\n      <th>MiscVal</th>\n      <th>MoSold</th>\n      <th>YrSold</th>\n      <th>SaleType</th>\n      <th>SaleCondition</th>\n      <th>SalePrice</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>65.0</td>\n      <td>8450</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>208500</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>20</td>\n      <td>RL</td>\n      <td>80.0</td>\n      <td>9600</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>5</td>\n      <td>2007</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>181500</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>68.0</td>\n      <td>11250</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>9</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>223500</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>70</td>\n      <td>RL</td>\n      <td>60.0</td>\n      <td>9550</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2006</td>\n      <td>WD</td>\n      <td>Abnorml</td>\n      <td>140000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>84.0</td>\n      <td>14260</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>12</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>250000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1455</th>\n      <td>1456</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>62.0</td>\n      <td>7917</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>8</td>\n      <td>2007</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>175000</td>\n    </tr>\n    <tr>\n      <th>1456</th>\n      <td>1457</td>\n      <td>20</td>\n      <td>RL</td>\n      <td>85.0</td>\n      <td>13175</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>MnPrv</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2010</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>210000</td>\n    </tr>\n    <tr>\n      <th>1457</th>\n      <td>1458</td>\n      <td>70</td>\n      <td>RL</td>\n      <td>66.0</td>\n      <td>9042</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>GdPrv</td>\n      <td>Shed</td>\n      <td>2500</td>\n      <td>5</td>\n      <td>2010</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>266500</td>\n    </tr>\n    <tr>\n      <th>1458</th>\n      <td>1459</td>\n      <td>20</td>\n      <td>RL</td>\n      <td>68.0</td>\n      <td>9717</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>4</td>\n      <td>2010</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>142125</td>\n    </tr>\n    <tr>\n      <th>1459</th>\n      <td>1460</td>\n      <td>20</td>\n      <td>RL</td>\n      <td>75.0</td>\n      <td>9937</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>6</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>147500</td>\n    </tr>\n  </tbody>\n</table>\n<p>1460 rows × 81 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "display(df)\n",
    "X = np.array(df[['GrLivArea', 'YearBuilt']])\n",
    "y = np.array(df['SalePrice']).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(934, 2) (934, 1)\n(234, 2) (234, 1)\n(292, 2) (292, 1)\n"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "y_train = np.log(y_train)\n",
    "y_val = np.log(y_val)\n",
    "y_test = np.log(y_test)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 0, loss : 43.1601, val_loss : 40.8252, r2 : -260.8171, val_r2 : -399.3053\nEpoch 1, loss : 0.7864, val_loss : 0.6923, r2 : -3.4399, val_r2 : -5.9386\nEpoch 2, loss : 0.2326, val_loss : 0.2129, r2 : -0.3656, val_r2 : -1.0160\nEpoch 3, loss : 0.0945, val_loss : 0.0875, r2 : 0.4389, val_r2 : 0.2105\nEpoch 4, loss : 0.0668, val_loss : 0.0621, r2 : 0.6019, val_r2 : 0.4453\nEpoch 5, loss : 0.0583, val_loss : 0.0540, r2 : 0.6536, val_r2 : 0.5148\nEpoch 6, loss : 0.0550, val_loss : 0.0508, r2 : 0.6739, val_r2 : 0.5451\nEpoch 7, loss : 0.0538, val_loss : 0.0498, r2 : 0.6805, val_r2 : 0.5567\nEpoch 8, loss : 0.0536, val_loss : 0.0497, r2 : 0.6816, val_r2 : 0.5539\nEpoch 9, loss : 0.0533, val_loss : 0.0500, r2 : 0.6791, val_r2 : 0.5468\ntest_loss : 0.0577, tesr_r2 : 0.6191\n"
    }
   ],
   "source": [
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.03\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "n_hidden1 = 5\n",
    "n_hidden2 = 5\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "# 計算グラフに渡す引数の形を決める\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, 1])\n",
    "# trainのミニバッチイテレータ\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
    "def example_net(x):\n",
    "    \"\"\"\n",
    "    単純な3層ニューラルネットワーク\n",
    "    \"\"\"\n",
    "    # 重みとバイアスの宣言\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, 1]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([1]))\n",
    "    }\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
    "    return layer_output\n",
    "# 決定係数\n",
    "def r2_score(y_true, y_pred):\n",
    "    residual_fluctuation = tf.reduce_sum(tf.square(y_true-y_pred))\n",
    "    total_fluctuation = tf.reduce_sum(tf.square(y_true-tf.reduce_mean(y_true)))\n",
    "    r2 = 1 - (residual_fluctuation/total_fluctuation)\n",
    "    return r2\n",
    "# ネットワーク構造の読み込み                               \n",
    "logits = example_net(X)\n",
    "# 目的関数\n",
    "loss_op = tf.losses.mean_squared_error(Y, logits)\n",
    "# 最適化手法\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "# 決定係数\n",
    "r2 = r2_score(Y, logits)\n",
    "# variableの初期化\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 計算グラフの実行\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        # エポックごとにループ\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
    "        total_val_loss = 0\n",
    "        total_loss = 0\n",
    "        total_val_r2_score = 0\n",
    "        total_r2_score = 0\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # ミニバッチごとにループ\n",
    "            _, loss, r2_score = sess.run([train_op, loss_op, r2], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            val_loss, val_r2_score = sess.run([loss_op, r2], feed_dict={X: X_val, Y: y_val})\n",
    "            total_val_loss += val_loss\n",
    "            total_loss += loss\n",
    "            total_val_r2_score += r2_score\n",
    "            total_r2_score += val_r2_score\n",
    "        total_loss /= total_batch\n",
    "        total_val_loss /= total_batch\n",
    "        total_r2_score /= total_batch\n",
    "        total_val_r2_score /= total_batch\n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, r2 : {:.4f}, val_r2 : {:.4f}\".format(epoch, total_loss, total_val_loss, total_r2_score, total_val_r2_score))\n",
    "    test_loss, test_r2_score = sess.run([loss_op, r2], feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"test_loss : {:.4f}, tesr_r2 : {:.4f}\".format(test_loss, test_r2_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回帰問題の出力層においては**活性化関数を用いない**、もしくは**恒等関数**（出力と入力が同じ）を利用することになる。\n",
    "\n",
    "また、モデルの評価指標として正解率ではなく誤差や決定係数が用いられる。今回は**MSE**と決定係数を利用した。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題5】\n",
    "#### MNISTのモデルを作成\n",
    "ニューラルネットワークのスクラッチで使用したMNISTを分類するモデルを作成してください。\n",
    "\n",
    "\n",
    "3クラス以上の分類という点ではひとつ前のIrisと同様です。入力が画像であるという点で異なります。\n",
    "\n",
    "\n",
    "スクラッチで実装したモデルの再現を目指してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\n(48000, 784)\n(12000, 784)\n(10000, 784)\n"
    }
   ],
   "source": [
    "# データのロード\n",
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
    "print(X_train.shape) # (48000, 784)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(48000, 10)\n(12000, 10)\n(10000, 10)\n"
    }
   ],
   "source": [
    "y_train = np.identity(10)[y_train]\n",
    "y_val = np.identity(10)[y_val]\n",
    "y_test = np.identity(10)[y_test]\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 0, loss : 343.1610, val_loss : 20420.8223, acc : 0.750, val_acc : 0.759, 19.271s\nEpoch 1, loss : 305.0857, val_loss : 18292.0723, acc : 0.930, val_acc : 0.936, 20.198s\nEpoch 2, loss : 303.3423, val_loss : 18152.8398, acc : 0.945, val_acc : 0.949, 20.600s\nEpoch 3, loss : 301.1102, val_loss : 18067.0195, acc : 0.955, val_acc : 0.956, 22.269s\nEpoch 4, loss : 297.9541, val_loss : 18103.1445, acc : 0.970, val_acc : 0.952, 20.922s\nEpoch 5, loss : 297.3806, val_loss : 18074.5527, acc : 0.975, val_acc : 0.955, 19.845s\nEpoch 6, loss : 298.0678, val_loss : 18055.5469, acc : 0.970, val_acc : 0.956, 21.898s\nEpoch 7, loss : 301.6006, val_loss : 18044.9707, acc : 0.950, val_acc : 0.957, 20.104s\nEpoch 8, loss : 301.1674, val_loss : 17983.2617, acc : 0.955, val_acc : 0.962, 22.378s\nEpoch 9, loss : 299.3402, val_loss : 18091.4062, acc : 0.965, val_acc : 0.953, 21.722s\ntest_acc : 0.960\n"
    }
   ],
   "source": [
    "# ハイパーパラメータの設定\n",
    "import time\n",
    "learning_rate = 0.005\n",
    "batch_size = 200\n",
    "num_epochs = 10\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 150\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 10\n",
    "# 計算グラフに渡す引数の形を決める\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "# trainのミニバッチイテレータ\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
    "def example_net(x):\n",
    "    \"\"\"\n",
    "    単純な3層ニューラルネットワーク\n",
    "    \"\"\"\n",
    "    # 重みとバイアスの宣言\n",
    "    initializer = tf.initializers.he_normal()\n",
    "    weights = {\n",
    "        'w1': tf.get_variable(name='w1', shape=[n_input, n_hidden1], initializer=initializer),\n",
    "        'w2': tf.get_variable(name='w2', shape=[n_hidden1, n_hidden2], initializer=initializer),\n",
    "        'w3': tf.get_variable(name='w3', shape=[n_hidden2, n_classes], initializer=initializer)\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
    "    layer_output = tf.nn.softmax(layer_output)\n",
    "    return layer_output\n",
    "# ネットワーク構造の読み込み                               \n",
    "logits = example_net(X)\n",
    "# 目的関数\n",
    "loss_op = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\n",
    "# 最適化手法\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "# 推定結果\n",
    "correct_pred = tf.equal(tf.argmax(Y, axis=1), tf.argmax(logits, axis=1))\n",
    "# 指標値計算\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "# variableの初期化\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 計算グラフの実行\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        start = time.time()\n",
    "        # エポックごとにループ\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
    "        total_val_loss = 0\n",
    "        total_val_acc = 0\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # ミニバッチごとにループ\n",
    "            _, loss, acc = sess.run([train_op, loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "            total_val_loss += val_loss\n",
    "            total_val_acc += val_acc\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "        total_loss /= total_batch\n",
    "        total_acc /= total_batch\n",
    "        total_val_loss /= total_batch\n",
    "        total_val_acc /= total_batch\n",
    "        elapse = time.time() - start\n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}, {:.3f}s\".format(epoch, loss, val_loss, acc, val_acc, elapse))\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正解率96%と十分な精度を出すことができた。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bitdicconda58dbae13a5ad45af92cdb395e5ca7493",
   "display_name": "Python 3.7.7 64-bit ('dic': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}