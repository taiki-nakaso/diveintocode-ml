{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint\n",
    "## ディープラーニングフレームワーク2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.このSprintについて\n",
    "\n",
    "### Sprintの目的\n",
    "- フレームワークのコードを読めるようにする\n",
    "- フレームワークを習得し続けられるようになる\n",
    "- 理論を知っている範囲をフレームワークで動かす\n",
    "\n",
    "### どのように学ぶか\n",
    "前半はTensorFlowのExampleを動かします。後半ではKerasのコードを書いていきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.公式Example\n",
    "\n",
    "深層学習フレームワークには公式に様々なモデルのExampleコードが公開されています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題1】\n",
    "#### 公式チュートリアルモデルを分担して実行\n",
    "TensorFLowの公式チュートリアルモデルを分担して実行してください。\n",
    "\n",
    "\n",
    "以下の中から1人ひとつ選び実行し、その結果を簡単に発表してください。\n",
    "\n",
    "\n",
    "[models/tutorials at master · tensorflow/models](https://www.tensorflow.org/tutorials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kerasによる分散トレーニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "WARNING:absl:Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n"
    }
   ],
   "source": [
    "# mnistデータセットの読み込み\n",
    "tfds.disable_progress_bar()\n",
    "datasets, info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
    "\n",
    "mnist_train, mnist_test = datasets['train'], datasets['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用可能デバイス探索\n",
    "def get_available_devices():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x for x in local_device_protos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[name: \"/device:CPU:0\"\n device_type: \"CPU\"\n memory_limit: 268435456\n locality {\n }\n incarnation: 16863763081094276903]"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "get_available_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:There is non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\nWARNING:tensorflow:There is non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n"
    }
   ],
   "source": [
    "# 複数GPUでの計算を可能にするAPI\n",
    "# このスコープ内でモデルを作成すると分散処理ができる\n",
    "strategy = tf.distribute.MirroredStrategy(devices=['/cpu:0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Number of devices: 1\n"
    }
   ],
   "source": [
    "# デバイス数表示\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ分割\n",
    "num_train_examples = info.splits['train'].num_examples\n",
    "num_test_examples = info.splits['test'].num_examples\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "BATCH_SIZE_PER_REPLICA = 64\n",
    "BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# スケーリング\n",
    "def scale(image, label):\n",
    "  image = tf.cast(image, tf.float32)\n",
    "  image /= 255\n",
    "\n",
    "  return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = mnist_train.map(scale).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "eval_dataset = mnist_test.map(scale).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From C:\\Users\\tnaka\\anaconda3\\envs\\dic\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\nInstructions for updating:\nIf using Keras pass *_constraint arguments to layers.\nWARNING:tensorflow:From C:\\Users\\tnaka\\anaconda3\\envs\\dic\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\nInstructions for updating:\nIf using Keras pass *_constraint arguments to layers.\n"
    }
   ],
   "source": [
    "# model作成\n",
    "# strategyのスコープ内で作成することで計算をうまく複数デバイスに分散してくれる\n",
    "with strategy.scope():\n",
    "  model = tf.keras.Sequential([\n",
    "      tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\n",
    "      tf.keras.layers.MaxPooling2D(),\n",
    "      tf.keras.layers.Flatten(),\n",
    "      tf.keras.layers.Dense(64, activation='relu'),\n",
    "      tf.keras.layers.Dense(10)\n",
    "  ])\n",
    "\n",
    "  model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint作成\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習率をエポックごとに変更する\n",
    "def decay(epoch):\n",
    "  if epoch < 3:\n",
    "    return 1e-3\n",
    "  elif epoch >= 3 and epoch < 7:\n",
    "    return 1e-4\n",
    "  else:\n",
    "    return 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習率を表示する\n",
    "class PrintLR(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    print('\\nLearning rate for epoch {} is {}'.format(epoch + 1,\n",
    "                                                      model.optimizer.lr.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コールバック設定\n",
    "callbacks = [\n",
    "    #tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,\n",
    "                                       save_weights_only=True),\n",
    "    tf.keras.callbacks.LearningRateScheduler(decay),\n",
    "    PrintLR()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習\n",
    "# model.fit(train_dataset, epochs=12, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> localだとうまく動かない/CPU1基のみなのでGoogle Colabを利用: ./keras_distribute.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題2】\n",
    "#### （アドバンス課題）様々な手法を実行\n",
    "TensorFLowやGoogle AI ResearchのGitHubリポジトリには、定番のモデルから最新のモデルまで多様なコードが公開されています。これらから興味あるものを選び実行してください。\n",
    "\n",
    "\n",
    "なお、これらのコードは初学者向けではないため、巨大なデータセットのダウンロードが必要な場合など、実行が簡単ではないこともあります。そういった場合は、コードリーディングを行ってください。\n",
    "\n",
    "\n",
    "[models/research at master · tensorflow/models]()\n",
    "\n",
    "\n",
    "[google-research/google-research: Google AI Research]()\n",
    "\n",
    "\n",
    "更新日が古いものはPythonやTensorFlowのバージョンが古く、扱いずらい場合があります。新しいものから見ることを推奨します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.異なるフレームワークへの書き換え\n",
    "\n",
    "「ディープラーニングフレームワーク1」で作成した4種類のデータセットを扱うTensorFLowのコードを異なるフレームワークに変更していきます。\n",
    "\n",
    "\n",
    "- Iris（Iris-versicolorとIris-virginicaのみの2値分類）\n",
    "- Iris（3種類全ての目的変数を使用して多値分類）\n",
    "- House Prices\n",
    "- MNIST\n",
    "\n",
    "### Kerasへの書き換え\n",
    "KerasはTensorFLowに含まれるtf.kerasモジュールを使用してください。\n",
    "\n",
    "\n",
    "KerasにはSequentialモデルかFunctional APIかなど書き方に種類がありますが、これは指定しません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題3】\n",
    "#### Iris（2値分類）をKerasで学習\n",
    "TensorFlowによるIrisデータセットに対する2値分類をKerasに書き換えてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "      Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm  \\\n0      1            5.1           3.5            1.4           0.2   \n1      2            4.9           3.0            1.4           0.2   \n2      3            4.7           3.2            1.3           0.2   \n3      4            4.6           3.1            1.5           0.2   \n4      5            5.0           3.6            1.4           0.2   \n..   ...            ...           ...            ...           ...   \n145  146            6.7           3.0            5.2           2.3   \n146  147            6.3           2.5            5.0           1.9   \n147  148            6.5           3.0            5.2           2.0   \n148  149            6.2           3.4            5.4           2.3   \n149  150            5.9           3.0            5.1           1.8   \n\n            Species  \n0       Iris-setosa  \n1       Iris-setosa  \n2       Iris-setosa  \n3       Iris-setosa  \n4       Iris-setosa  \n..              ...  \n145  Iris-virginica  \n146  Iris-virginica  \n147  Iris-virginica  \n148  Iris-virginica  \n149  Iris-virginica  \n\n[150 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>SepalLengthCm</th>\n      <th>SepalWidthCm</th>\n      <th>PetalLengthCm</th>\n      <th>PetalWidthCm</th>\n      <th>Species</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>5.1</td>\n      <td>3.5</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>4.9</td>\n      <td>3.0</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>4.7</td>\n      <td>3.2</td>\n      <td>1.3</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>4.6</td>\n      <td>3.1</td>\n      <td>1.5</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>5.0</td>\n      <td>3.6</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>146</td>\n      <td>6.7</td>\n      <td>3.0</td>\n      <td>5.2</td>\n      <td>2.3</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>146</th>\n      <td>147</td>\n      <td>6.3</td>\n      <td>2.5</td>\n      <td>5.0</td>\n      <td>1.9</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>148</td>\n      <td>6.5</td>\n      <td>3.0</td>\n      <td>5.2</td>\n      <td>2.0</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>148</th>\n      <td>149</td>\n      <td>6.2</td>\n      <td>3.4</td>\n      <td>5.4</td>\n      <td>2.3</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>149</th>\n      <td>150</td>\n      <td>5.9</td>\n      <td>3.0</td>\n      <td>5.1</td>\n      <td>1.8</td>\n      <td>Iris-virginica</td>\n    </tr>\n  </tbody>\n</table>\n<p>150 rows × 6 columns</p>\n</div>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "(100, 4)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "(100, 1)"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(64, 4) (64, 1)\n(16, 4) (16, 1)\n(20, 4) (20, 1)\n"
    }
   ],
   "source": [
    "# dataset読み込み\n",
    "df_iris = pd.read_csv('Iris.csv')\n",
    "display(df_iris)\n",
    "X = np.array(df_iris[(df_iris['Species'] == 'Iris-versicolor')|(df_iris['Species'] == 'Iris-virginica')].iloc[:, 1:5])\n",
    "y = np.array(df_iris[(df_iris['Species'] == 'Iris-versicolor')|(df_iris['Species'] == 'Iris-virginica')].iloc[:, 5])\n",
    "y[y=='Iris-versicolor'] = 0\n",
    "y[y=='Iris-virginica'] = 1\n",
    "y = y.astype(np.int)[:, np.newaxis]\n",
    "display(X.shape)\n",
    "display(y.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense (Dense)                (None, 10)                50        \n_________________________________________________________________\ndense_1 (Dense)              (None, 5)                 55        \n_________________________________________________________________\ndense_2 (Dense)              (None, 1)                 6         \n=================================================================\nTotal params: 111\nTrainable params: 111\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "K.clear_session()\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(10, activation = tf.nn.relu, input_shape=(4,)))\n",
    "model.add(tf.keras.layers.Dense(5, activation = tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(1, activation = tf.nn.sigmoid))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From C:\\Users\\tnaka\\anaconda3\\envs\\dic\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.where in 2.0, which has the same broadcast rule as np.where\nWARNING:tensorflow:From C:\\Users\\tnaka\\anaconda3\\envs\\dic\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.where in 2.0, which has the same broadcast rule as np.where\nTrain on 64 samples, validate on 16 samples\nEpoch 1/10\n64/64 [==============================] - 0s 6ms/sample - loss: 0.3320 - acc: 0.8750 - val_loss: 0.0393 - val_acc: 1.0000\nEpoch 2/10\n64/64 [==============================] - 0s 2ms/sample - loss: 0.1015 - acc: 0.9375 - val_loss: 0.0452 - val_acc: 1.0000\nEpoch 3/10\n64/64 [==============================] - 0s 2ms/sample - loss: 0.0828 - acc: 0.9688 - val_loss: 0.1055 - val_acc: 0.9375\nEpoch 4/10\n64/64 [==============================] - 0s 1ms/sample - loss: 0.1488 - acc: 0.9531 - val_loss: 0.0628 - val_acc: 0.9375\nEpoch 5/10\n64/64 [==============================] - 0s 2ms/sample - loss: 0.0373 - acc: 0.9844 - val_loss: 0.0088 - val_acc: 1.0000\nEpoch 6/10\n64/64 [==============================] - 0s 1ms/sample - loss: 0.0188 - acc: 1.0000 - val_loss: 0.0349 - val_acc: 1.0000\nEpoch 7/10\n64/64 [==============================] - 0s 2ms/sample - loss: 0.0096 - acc: 1.0000 - val_loss: 0.0285 - val_acc: 1.0000\nEpoch 8/10\n64/64 [==============================] - 0s 2ms/sample - loss: 0.0051 - acc: 1.0000 - val_loss: 0.0068 - val_acc: 1.0000\nEpoch 9/10\n64/64 [==============================] - 0s 1ms/sample - loss: 0.0034 - acc: 1.0000 - val_loss: 0.0067 - val_acc: 1.0000\nEpoch 10/10\n64/64 [==============================] - 0s 1ms/sample - loss: 0.0020 - acc: 1.0000 - val_loss: 0.0082 - val_acc: 1.0000\n"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=tf.train.AdamOptimizer(learning_rate=0.03),\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=1,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "y_pred_proba [8.9406967e-08 9.9999940e-01 0.0000000e+00 1.0000000e+00 9.9999851e-01\n 9.9999988e-01 1.7851591e-05 9.9187887e-01 9.9999964e-01 9.9998343e-01\n 9.9997473e-01 9.9962682e-01 9.9999738e-01 1.9073486e-06 0.0000000e+00\n 0.0000000e+00 8.3328927e-01 0.0000000e+00 9.9541843e-01 0.0000000e+00]\ny_pred [0 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 1 0 1 0]\ny_test [0 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 0 0 0 0]\n"
    }
   ],
   "source": [
    "y_pred_proba = model.predict(X_test)[:, 0]\n",
    "y_pred = np.where(y_pred_proba >0.5, 1, 0)\n",
    "print(\"y_pred_proba\", y_pred_proba)\n",
    "print(\"y_pred\", y_pred)\n",
    "print(\"y_test\", y_test.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題4】\n",
    "#### Iris（多値分類）をKerasで学習\n",
    "TensorFlowによるIrisデータセットに対する3値分類をKerasに書き換えてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "(150, 4)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "(150, 3)"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(96, 4) (96, 3)\n(24, 4) (24, 3)\n(30, 4) (30, 3)\n"
    }
   ],
   "source": [
    "# 抽出\n",
    "X = np.array(df_iris.iloc[:, 1:5])\n",
    "y = np.array(pd.get_dummies(df_iris.iloc[:, 5]))\n",
    "y = y.astype(np.int)\n",
    "display(X.shape)\n",
    "display(y.shape)\n",
    "\n",
    "# 分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "# 標準化\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense (Dense)                (None, 100)               500       \n_________________________________________________________________\ndense_1 (Dense)              (None, 10)                1010      \n_________________________________________________________________\ndense_2 (Dense)              (None, 3)                 33        \n=================================================================\nTotal params: 1,543\nTrainable params: 1,543\nNon-trainable params: 0\n_________________________________________________________________\nTrain on 96 samples, validate on 24 samples\nEpoch 1/10\n96/96 [==============================] - 0s 3ms/sample - loss: 0.7900 - acc: 0.7708 - val_loss: 0.2544 - val_acc: 0.9167\nEpoch 2/10\n96/96 [==============================] - 0s 1ms/sample - loss: 0.1596 - acc: 0.9479 - val_loss: 0.3655 - val_acc: 0.8333\nEpoch 3/10\n96/96 [==============================] - 0s 1ms/sample - loss: 0.1511 - acc: 0.9792 - val_loss: 0.9949 - val_acc: 0.7917\nEpoch 4/10\n96/96 [==============================] - 0s 1ms/sample - loss: 0.1885 - acc: 0.9271 - val_loss: 0.2970 - val_acc: 0.7917\nEpoch 5/10\n96/96 [==============================] - 0s 1ms/sample - loss: 0.1969 - acc: 0.9479 - val_loss: 0.4544 - val_acc: 0.8333\nEpoch 6/10\n96/96 [==============================] - 0s 1ms/sample - loss: 0.1901 - acc: 0.9583 - val_loss: 0.4480 - val_acc: 0.8333\nEpoch 7/10\n96/96 [==============================] - 0s 1ms/sample - loss: 0.1203 - acc: 0.9583 - val_loss: 0.2153 - val_acc: 0.9167\nEpoch 8/10\n96/96 [==============================] - 0s 1ms/sample - loss: 0.0847 - acc: 0.9792 - val_loss: 0.4116 - val_acc: 0.9167\nEpoch 9/10\n96/96 [==============================] - 0s 1ms/sample - loss: 0.1660 - acc: 0.9271 - val_loss: 1.1373 - val_acc: 0.8333\nEpoch 10/10\n96/96 [==============================] - 0s 1ms/sample - loss: 0.1089 - acc: 0.9583 - val_loss: 0.2908 - val_acc: 0.9167\n"
    }
   ],
   "source": [
    "# モデル構築\n",
    "K.clear_session()\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(100, activation = tf.nn.relu, input_shape=(4,)))\n",
    "model.add(tf.keras.layers.Dense(10, activation = tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(3, activation = tf.nn.softmax))\n",
    "model.summary()\n",
    "\n",
    "# コンパイル\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=tf.train.AdamOptimizer(learning_rate=0.03),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 学習\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=1,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "y_pred_proba [[1.92435613e-21 3.50792142e-08 1.00000000e+00]\n [4.19442030e-03 9.71257687e-01 2.45479178e-02]\n [1.00000000e+00 3.20510076e-19 1.57585004e-17]\n [3.79966855e-06 5.24018612e-03 9.94755983e-01]\n [1.00000000e+00 3.18331550e-14 9.54395051e-14]\n [9.07631409e-25 4.99462749e-10 1.00000000e+00]\n [1.00000000e+00 8.95304508e-15 3.78033142e-14]\n [1.37826335e-02 8.97834659e-01 8.83827433e-02]\n [1.20781241e-02 9.11284983e-01 7.66369179e-02]\n [6.77118869e-03 9.52149987e-01 4.10788357e-02]\n [4.10487968e-03 2.35637903e-01 7.60257244e-01]\n [1.17960507e-02 9.13497269e-01 7.47066811e-02]\n [8.95293336e-03 9.35557604e-01 5.54894507e-02]\n [1.46283088e-02 8.91111493e-01 9.42602009e-02]\n [1.52892657e-02 8.85834992e-01 9.88757983e-02]\n [1.00000000e+00 5.71461932e-13 7.47656223e-13]\n [1.58574600e-02 8.70604396e-01 1.13538139e-01]\n [7.28745153e-03 9.48253810e-01 4.44587246e-02]\n [1.00000000e+00 2.64302955e-11 1.93906315e-11]\n [1.00000000e+00 4.72409809e-16 4.90247804e-15]\n [5.34839588e-13 6.51026276e-05 9.99934912e-01]\n [1.46349203e-02 7.31865227e-01 2.53499866e-01]\n [1.00000000e+00 1.38812816e-13 3.53072172e-13]\n [1.00000000e+00 3.08065596e-12 4.35319749e-12]\n [4.66975820e-04 1.35939687e-01 8.63593340e-01]\n [1.00000000e+00 9.37597065e-18 3.11284079e-16]\n [1.00000000e+00 2.40138499e-14 1.35031743e-13]\n [7.93473795e-03 9.43341672e-01 4.87236232e-02]\n [1.72951340e-03 9.88786757e-01 9.48375463e-03]\n [1.00000000e+00 1.61423381e-12 3.02803962e-12]]\ny_pred [2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0]\ny_test [2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0]\n"
    }
   ],
   "source": [
    "y_pred_proba = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "print(\"y_pred_proba\", y_pred_proba)\n",
    "print(\"y_pred\", y_pred)\n",
    "print(\"y_test\", np.argmax(y_test, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題5】\n",
    "#### House PricesをKerasで学習\n",
    "TensorFlowによるHouse Pricesデータセットに対する回帰をKerasに書き換えてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "        Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n0        1          60       RL         65.0     8450   Pave   NaN      Reg   \n1        2          20       RL         80.0     9600   Pave   NaN      Reg   \n2        3          60       RL         68.0    11250   Pave   NaN      IR1   \n3        4          70       RL         60.0     9550   Pave   NaN      IR1   \n4        5          60       RL         84.0    14260   Pave   NaN      IR1   \n...    ...         ...      ...          ...      ...    ...   ...      ...   \n1455  1456          60       RL         62.0     7917   Pave   NaN      Reg   \n1456  1457          20       RL         85.0    13175   Pave   NaN      Reg   \n1457  1458          70       RL         66.0     9042   Pave   NaN      Reg   \n1458  1459          20       RL         68.0     9717   Pave   NaN      Reg   \n1459  1460          20       RL         75.0     9937   Pave   NaN      Reg   \n\n     LandContour Utilities  ... PoolArea PoolQC  Fence MiscFeature MiscVal  \\\n0            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n1            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n2            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n3            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n4            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n...          ...       ...  ...      ...    ...    ...         ...     ...   \n1455         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n1456         Lvl    AllPub  ...        0    NaN  MnPrv         NaN       0   \n1457         Lvl    AllPub  ...        0    NaN  GdPrv        Shed    2500   \n1458         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n1459         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n\n     MoSold YrSold  SaleType  SaleCondition  SalePrice  \n0         2   2008        WD         Normal     208500  \n1         5   2007        WD         Normal     181500  \n2         9   2008        WD         Normal     223500  \n3         2   2006        WD        Abnorml     140000  \n4        12   2008        WD         Normal     250000  \n...     ...    ...       ...            ...        ...  \n1455      8   2007        WD         Normal     175000  \n1456      2   2010        WD         Normal     210000  \n1457      5   2010        WD         Normal     266500  \n1458      4   2010        WD         Normal     142125  \n1459      6   2008        WD         Normal     147500  \n\n[1460 rows x 81 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>MSSubClass</th>\n      <th>MSZoning</th>\n      <th>LotFrontage</th>\n      <th>LotArea</th>\n      <th>Street</th>\n      <th>Alley</th>\n      <th>LotShape</th>\n      <th>LandContour</th>\n      <th>Utilities</th>\n      <th>...</th>\n      <th>PoolArea</th>\n      <th>PoolQC</th>\n      <th>Fence</th>\n      <th>MiscFeature</th>\n      <th>MiscVal</th>\n      <th>MoSold</th>\n      <th>YrSold</th>\n      <th>SaleType</th>\n      <th>SaleCondition</th>\n      <th>SalePrice</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>65.0</td>\n      <td>8450</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>208500</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>20</td>\n      <td>RL</td>\n      <td>80.0</td>\n      <td>9600</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>5</td>\n      <td>2007</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>181500</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>68.0</td>\n      <td>11250</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>9</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>223500</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>70</td>\n      <td>RL</td>\n      <td>60.0</td>\n      <td>9550</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2006</td>\n      <td>WD</td>\n      <td>Abnorml</td>\n      <td>140000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>84.0</td>\n      <td>14260</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>12</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>250000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1455</th>\n      <td>1456</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>62.0</td>\n      <td>7917</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>8</td>\n      <td>2007</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>175000</td>\n    </tr>\n    <tr>\n      <th>1456</th>\n      <td>1457</td>\n      <td>20</td>\n      <td>RL</td>\n      <td>85.0</td>\n      <td>13175</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>MnPrv</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2010</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>210000</td>\n    </tr>\n    <tr>\n      <th>1457</th>\n      <td>1458</td>\n      <td>70</td>\n      <td>RL</td>\n      <td>66.0</td>\n      <td>9042</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>GdPrv</td>\n      <td>Shed</td>\n      <td>2500</td>\n      <td>5</td>\n      <td>2010</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>266500</td>\n    </tr>\n    <tr>\n      <th>1458</th>\n      <td>1459</td>\n      <td>20</td>\n      <td>RL</td>\n      <td>68.0</td>\n      <td>9717</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>4</td>\n      <td>2010</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>142125</td>\n    </tr>\n    <tr>\n      <th>1459</th>\n      <td>1460</td>\n      <td>20</td>\n      <td>RL</td>\n      <td>75.0</td>\n      <td>9937</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>6</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>147500</td>\n    </tr>\n  </tbody>\n</table>\n<p>1460 rows × 81 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# 読み込み\n",
    "df_price = pd.read_csv('train.csv')\n",
    "display(df_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "(1460, 3)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "(1460, 1)"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(934, 3) (934, 1)\n(234, 3) (234, 1)\n(292, 3) (292, 1)\n"
    }
   ],
   "source": [
    "# 抽出\n",
    "X = df_price[['LotArea', 'GrLivArea', 'YearBuilt']].values\n",
    "y = df_price['SalePrice'].values[:, np.newaxis]\n",
    "display(X.shape)\n",
    "display(y.shape)\n",
    "\n",
    "# 分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "# 標準化\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# 対数変換\n",
    "y_train = np.log(y_train)\n",
    "y_val = np.log(y_val)\n",
    "y_test = np.log(y_test)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense (Dense)                (None, 10)                40        \n_________________________________________________________________\ndense_1 (Dense)              (None, 3)                 33        \n_________________________________________________________________\ndense_2 (Dense)              (None, 1)                 4         \n=================================================================\nTotal params: 77\nTrainable params: 77\nNon-trainable params: 0\n_________________________________________________________________\nTrain on 934 samples, validate on 234 samples\nEpoch 1/100\n934/934 [==============================] - 0s 229us/sample - loss: 138.4217 - mean_absolute_error: 11.7600 - val_loss: 134.1055 - val_mean_absolute_error: 11.5757\nEpoch 2/100\n934/934 [==============================] - 0s 50us/sample - loss: 131.6338 - mean_absolute_error: 11.4657 - val_loss: 126.1695 - val_mean_absolute_error: 11.2215\nEpoch 3/100\n934/934 [==============================] - 0s 57us/sample - loss: 123.2091 - mean_absolute_error: 11.0806 - val_loss: 116.5738 - val_mean_absolute_error: 10.7705\nEpoch 4/100\n934/934 [==============================] - 0s 53us/sample - loss: 112.9951 - mean_absolute_error: 10.5843 - val_loss: 105.2224 - val_mean_absolute_error: 10.2030\nEpoch 5/100\n934/934 [==============================] - 0s 61us/sample - loss: 101.2287 - mean_absolute_error: 9.9769 - val_loss: 92.3896 - val_mean_absolute_error: 9.5092\nEpoch 6/100\n934/934 [==============================] - 0s 51us/sample - loss: 88.8682 - mean_absolute_error: 9.2815 - val_loss: 79.7284 - val_mean_absolute_error: 8.7545\nEpoch 7/100\n934/934 [==============================] - 0s 83us/sample - loss: 76.8258 - mean_absolute_error: 8.5277 - val_loss: 67.3568 - val_mean_absolute_error: 7.9189\nEpoch 8/100\n934/934 [==============================] - 0s 58us/sample - loss: 65.5877 - mean_absolute_error: 7.7451 - val_loss: 56.0955 - val_mean_absolute_error: 7.0662\nEpoch 9/100\n934/934 [==============================] - 0s 48us/sample - loss: 55.5564 - mean_absolute_error: 6.9305 - val_loss: 46.3340 - val_mean_absolute_error: 6.2313\nEpoch 10/100\n934/934 [==============================] - 0s 44us/sample - loss: 46.9744 - mean_absolute_error: 6.1537 - val_loss: 38.3033 - val_mean_absolute_error: 5.4548\nEpoch 11/100\n934/934 [==============================] - 0s 43us/sample - loss: 39.8414 - mean_absolute_error: 5.4313 - val_loss: 31.6745 - val_mean_absolute_error: 4.7283\nEpoch 12/100\n934/934 [==============================] - 0s 41us/sample - loss: 33.9242 - mean_absolute_error: 4.8100 - val_loss: 26.4122 - val_mean_absolute_error: 4.1528\nEpoch 13/100\n934/934 [==============================] - 0s 48us/sample - loss: 28.7856 - mean_absolute_error: 4.3125 - val_loss: 22.0328 - val_mean_absolute_error: 3.7144\nEpoch 14/100\n934/934 [==============================] - 0s 46us/sample - loss: 24.2838 - mean_absolute_error: 3.9061 - val_loss: 18.4057 - val_mean_absolute_error: 3.3744\nEpoch 15/100\n934/934 [==============================] - 0s 43us/sample - loss: 20.4818 - mean_absolute_error: 3.5778 - val_loss: 15.5072 - val_mean_absolute_error: 3.0996\nEpoch 16/100\n934/934 [==============================] - 0s 47us/sample - loss: 17.1293 - mean_absolute_error: 3.2576 - val_loss: 12.9470 - val_mean_absolute_error: 2.8334\nEpoch 17/100\n934/934 [==============================] - 0s 65us/sample - loss: 14.3201 - mean_absolute_error: 2.9746 - val_loss: 10.8372 - val_mean_absolute_error: 2.5845\nEpoch 18/100\n934/934 [==============================] - 0s 48us/sample - loss: 12.0502 - mean_absolute_error: 2.7166 - val_loss: 9.1575 - val_mean_absolute_error: 2.3708\nEpoch 19/100\n934/934 [==============================] - 0s 50us/sample - loss: 10.2319 - mean_absolute_error: 2.4888 - val_loss: 7.8558 - val_mean_absolute_error: 2.2045\nEpoch 20/100\n934/934 [==============================] - 0s 49us/sample - loss: 8.7989 - mean_absolute_error: 2.3044 - val_loss: 6.8037 - val_mean_absolute_error: 2.0598\nEpoch 21/100\n934/934 [==============================] - 0s 47us/sample - loss: 7.6554 - mean_absolute_error: 2.1388 - val_loss: 5.8945 - val_mean_absolute_error: 1.9188\nEpoch 22/100\n934/934 [==============================] - 0s 62us/sample - loss: 6.7069 - mean_absolute_error: 1.9908 - val_loss: 5.1880 - val_mean_absolute_error: 1.7987\nEpoch 23/100\n934/934 [==============================] - 0s 84us/sample - loss: 5.9538 - mean_absolute_error: 1.8576 - val_loss: 4.6143 - val_mean_absolute_error: 1.6939\nEpoch 24/100\n934/934 [==============================] - 0s 56us/sample - loss: 5.3306 - mean_absolute_error: 1.7438 - val_loss: 4.1201 - val_mean_absolute_error: 1.5957\nEpoch 25/100\n934/934 [==============================] - 0s 43us/sample - loss: 4.8300 - mean_absolute_error: 1.6558 - val_loss: 3.7384 - val_mean_absolute_error: 1.5198\nEpoch 26/100\n934/934 [==============================] - 0s 46us/sample - loss: 4.3946 - mean_absolute_error: 1.5602 - val_loss: 3.3888 - val_mean_absolute_error: 1.4359\nEpoch 27/100\n934/934 [==============================] - 0s 46us/sample - loss: 4.0169 - mean_absolute_error: 1.4809 - val_loss: 3.0864 - val_mean_absolute_error: 1.3618\nEpoch 28/100\n934/934 [==============================] - 0s 45us/sample - loss: 3.6800 - mean_absolute_error: 1.4030 - val_loss: 2.8214 - val_mean_absolute_error: 1.2951\nEpoch 29/100\n934/934 [==============================] - 0s 46us/sample - loss: 3.3777 - mean_absolute_error: 1.3317 - val_loss: 2.5616 - val_mean_absolute_error: 1.2287\nEpoch 30/100\n934/934 [==============================] - 0s 44us/sample - loss: 3.0922 - mean_absolute_error: 1.2696 - val_loss: 2.3240 - val_mean_absolute_error: 1.1684\nEpoch 31/100\n934/934 [==============================] - 0s 45us/sample - loss: 2.8413 - mean_absolute_error: 1.2123 - val_loss: 2.1332 - val_mean_absolute_error: 1.1159\nEpoch 32/100\n934/934 [==============================] - 0s 46us/sample - loss: 2.6247 - mean_absolute_error: 1.1611 - val_loss: 1.9544 - val_mean_absolute_error: 1.0678\nEpoch 33/100\n934/934 [==============================] - 0s 52us/sample - loss: 2.4271 - mean_absolute_error: 1.1142 - val_loss: 1.7891 - val_mean_absolute_error: 1.0221\nEpoch 34/100\n934/934 [==============================] - 0s 45us/sample - loss: 2.2460 - mean_absolute_error: 1.0669 - val_loss: 1.6354 - val_mean_absolute_error: 0.9775\nEpoch 35/100\n934/934 [==============================] - 0s 60us/sample - loss: 2.0723 - mean_absolute_error: 1.0195 - val_loss: 1.4974 - val_mean_absolute_error: 0.9305\nEpoch 36/100\n934/934 [==============================] - 0s 42us/sample - loss: 1.9166 - mean_absolute_error: 0.9825 - val_loss: 1.3721 - val_mean_absolute_error: 0.8964\nEpoch 37/100\n934/934 [==============================] - 0s 64us/sample - loss: 1.7669 - mean_absolute_error: 0.9424 - val_loss: 1.2578 - val_mean_absolute_error: 0.8558\nEpoch 38/100\n934/934 [==============================] - 0s 45us/sample - loss: 1.6430 - mean_absolute_error: 0.9102 - val_loss: 1.1544 - val_mean_absolute_error: 0.8242\nEpoch 39/100\n934/934 [==============================] - 0s 45us/sample - loss: 1.5237 - mean_absolute_error: 0.8763 - val_loss: 1.0716 - val_mean_absolute_error: 0.7932\nEpoch 40/100\n934/934 [==============================] - 0s 43us/sample - loss: 1.4147 - mean_absolute_error: 0.8492 - val_loss: 0.9921 - val_mean_absolute_error: 0.7681\nEpoch 41/100\n934/934 [==============================] - 0s 47us/sample - loss: 1.3105 - mean_absolute_error: 0.8206 - val_loss: 0.9252 - val_mean_absolute_error: 0.7418\nEpoch 42/100\n934/934 [==============================] - 0s 67us/sample - loss: 1.2107 - mean_absolute_error: 0.7864 - val_loss: 0.8561 - val_mean_absolute_error: 0.7098\nEpoch 43/100\n934/934 [==============================] - 0s 44us/sample - loss: 1.1223 - mean_absolute_error: 0.7644 - val_loss: 0.8068 - val_mean_absolute_error: 0.6933\nEpoch 44/100\n934/934 [==============================] - 0s 46us/sample - loss: 1.0442 - mean_absolute_error: 0.7343 - val_loss: 0.7437 - val_mean_absolute_error: 0.6640\nEpoch 45/100\n934/934 [==============================] - 0s 42us/sample - loss: 0.9699 - mean_absolute_error: 0.7059 - val_loss: 0.6974 - val_mean_absolute_error: 0.6417\nEpoch 46/100\n934/934 [==============================] - 0s 41us/sample - loss: 0.9056 - mean_absolute_error: 0.6842 - val_loss: 0.6550 - val_mean_absolute_error: 0.6214\nEpoch 47/100\n934/934 [==============================] - 0s 44us/sample - loss: 0.8440 - mean_absolute_error: 0.6580 - val_loss: 0.6117 - val_mean_absolute_error: 0.5981\nEpoch 48/100\n934/934 [==============================] - 0s 51us/sample - loss: 0.7908 - mean_absolute_error: 0.6362 - val_loss: 0.5690 - val_mean_absolute_error: 0.5776\nEpoch 49/100\n934/934 [==============================] - 0s 47us/sample - loss: 0.7390 - mean_absolute_error: 0.6160 - val_loss: 0.5335 - val_mean_absolute_error: 0.5597\nEpoch 50/100\n934/934 [==============================] - 0s 93us/sample - loss: 0.6949 - mean_absolute_error: 0.5967 - val_loss: 0.4990 - val_mean_absolute_error: 0.5419\nEpoch 51/100\n934/934 [==============================] - 0s 50us/sample - loss: 0.6462 - mean_absolute_error: 0.5746 - val_loss: 0.4618 - val_mean_absolute_error: 0.5205\nEpoch 52/100\n934/934 [==============================] - 0s 57us/sample - loss: 0.6073 - mean_absolute_error: 0.5554 - val_loss: 0.4318 - val_mean_absolute_error: 0.5043\nEpoch 53/100\n934/934 [==============================] - 0s 52us/sample - loss: 0.5665 - mean_absolute_error: 0.5409 - val_loss: 0.4074 - val_mean_absolute_error: 0.4909\nEpoch 54/100\n934/934 [==============================] - 0s 49us/sample - loss: 0.5318 - mean_absolute_error: 0.5289 - val_loss: 0.3791 - val_mean_absolute_error: 0.4749\nEpoch 55/100\n934/934 [==============================] - 0s 47us/sample - loss: 0.4938 - mean_absolute_error: 0.5096 - val_loss: 0.3564 - val_mean_absolute_error: 0.4609\nEpoch 56/100\n934/934 [==============================] - 0s 46us/sample - loss: 0.4628 - mean_absolute_error: 0.4946 - val_loss: 0.3337 - val_mean_absolute_error: 0.4469\nEpoch 57/100\n934/934 [==============================] - 0s 58us/sample - loss: 0.4327 - mean_absolute_error: 0.4782 - val_loss: 0.3130 - val_mean_absolute_error: 0.4322\nEpoch 58/100\n934/934 [==============================] - 0s 56us/sample - loss: 0.4063 - mean_absolute_error: 0.4661 - val_loss: 0.2919 - val_mean_absolute_error: 0.4182\nEpoch 59/100\n934/934 [==============================] - 0s 46us/sample - loss: 0.3828 - mean_absolute_error: 0.4486 - val_loss: 0.2766 - val_mean_absolute_error: 0.4067\nEpoch 60/100\n934/934 [==============================] - 0s 47us/sample - loss: 0.3578 - mean_absolute_error: 0.4379 - val_loss: 0.2606 - val_mean_absolute_error: 0.3943\nEpoch 61/100\n934/934 [==============================] - 0s 64us/sample - loss: 0.3381 - mean_absolute_error: 0.4283 - val_loss: 0.2465 - val_mean_absolute_error: 0.3853\nEpoch 62/100\n934/934 [==============================] - 0s 57us/sample - loss: 0.3201 - mean_absolute_error: 0.4159 - val_loss: 0.2337 - val_mean_absolute_error: 0.3744\nEpoch 63/100\n934/934 [==============================] - 0s 42us/sample - loss: 0.3052 - mean_absolute_error: 0.4034 - val_loss: 0.2225 - val_mean_absolute_error: 0.3646\nEpoch 64/100\n934/934 [==============================] - 0s 43us/sample - loss: 0.2895 - mean_absolute_error: 0.3967 - val_loss: 0.2130 - val_mean_absolute_error: 0.3580\nEpoch 65/100\n934/934 [==============================] - 0s 58us/sample - loss: 0.2745 - mean_absolute_error: 0.3888 - val_loss: 0.2043 - val_mean_absolute_error: 0.3512\nEpoch 66/100\n934/934 [==============================] - 0s 50us/sample - loss: 0.2626 - mean_absolute_error: 0.3782 - val_loss: 0.1956 - val_mean_absolute_error: 0.3428\nEpoch 67/100\n934/934 [==============================] - 0s 48us/sample - loss: 0.2510 - mean_absolute_error: 0.3710 - val_loss: 0.1890 - val_mean_absolute_error: 0.3374\nEpoch 68/100\n934/934 [==============================] - 0s 44us/sample - loss: 0.2404 - mean_absolute_error: 0.3646 - val_loss: 0.1837 - val_mean_absolute_error: 0.3336\nEpoch 69/100\n934/934 [==============================] - 0s 47us/sample - loss: 0.2312 - mean_absolute_error: 0.3608 - val_loss: 0.1781 - val_mean_absolute_error: 0.3292\nEpoch 70/100\n934/934 [==============================] - 0s 43us/sample - loss: 0.2231 - mean_absolute_error: 0.3533 - val_loss: 0.1717 - val_mean_absolute_error: 0.3233\nEpoch 71/100\n934/934 [==============================] - 0s 46us/sample - loss: 0.2142 - mean_absolute_error: 0.3481 - val_loss: 0.1654 - val_mean_absolute_error: 0.3179\nEpoch 72/100\n934/934 [==============================] - 0s 53us/sample - loss: 0.2074 - mean_absolute_error: 0.3428 - val_loss: 0.1617 - val_mean_absolute_error: 0.3137\nEpoch 73/100\n934/934 [==============================] - 0s 47us/sample - loss: 0.2000 - mean_absolute_error: 0.3346 - val_loss: 0.1568 - val_mean_absolute_error: 0.3097\nEpoch 74/100\n934/934 [==============================] - 0s 45us/sample - loss: 0.1926 - mean_absolute_error: 0.3286 - val_loss: 0.1523 - val_mean_absolute_error: 0.3045\nEpoch 75/100\n934/934 [==============================] - 0s 43us/sample - loss: 0.1862 - mean_absolute_error: 0.3254 - val_loss: 0.1486 - val_mean_absolute_error: 0.3022\nEpoch 76/100\n934/934 [==============================] - 0s 43us/sample - loss: 0.1817 - mean_absolute_error: 0.3222 - val_loss: 0.1446 - val_mean_absolute_error: 0.2980\nEpoch 77/100\n934/934 [==============================] - 0s 44us/sample - loss: 0.1751 - mean_absolute_error: 0.3150 - val_loss: 0.1409 - val_mean_absolute_error: 0.2940\nEpoch 78/100\n934/934 [==============================] - 0s 43us/sample - loss: 0.1703 - mean_absolute_error: 0.3109 - val_loss: 0.1374 - val_mean_absolute_error: 0.2911\nEpoch 79/100\n934/934 [==============================] - 0s 49us/sample - loss: 0.1642 - mean_absolute_error: 0.3068 - val_loss: 0.1350 - val_mean_absolute_error: 0.2878\nEpoch 80/100\n934/934 [==============================] - 0s 42us/sample - loss: 0.1598 - mean_absolute_error: 0.3027 - val_loss: 0.1318 - val_mean_absolute_error: 0.2845\nEpoch 81/100\n934/934 [==============================] - 0s 60us/sample - loss: 0.1550 - mean_absolute_error: 0.2987 - val_loss: 0.1283 - val_mean_absolute_error: 0.2816\nEpoch 82/100\n934/934 [==============================] - 0s 44us/sample - loss: 0.1521 - mean_absolute_error: 0.2959 - val_loss: 0.1254 - val_mean_absolute_error: 0.2786\nEpoch 83/100\n934/934 [==============================] - 0s 46us/sample - loss: 0.1475 - mean_absolute_error: 0.2920 - val_loss: 0.1232 - val_mean_absolute_error: 0.2757\nEpoch 84/100\n934/934 [==============================] - 0s 43us/sample - loss: 0.1431 - mean_absolute_error: 0.2881 - val_loss: 0.1197 - val_mean_absolute_error: 0.2724\nEpoch 85/100\n934/934 [==============================] - 0s 51us/sample - loss: 0.1398 - mean_absolute_error: 0.2844 - val_loss: 0.1169 - val_mean_absolute_error: 0.2686\nEpoch 86/100\n934/934 [==============================] - 0s 42us/sample - loss: 0.1368 - mean_absolute_error: 0.2820 - val_loss: 0.1154 - val_mean_absolute_error: 0.2679\nEpoch 87/100\n934/934 [==============================] - 0s 44us/sample - loss: 0.1336 - mean_absolute_error: 0.2788 - val_loss: 0.1122 - val_mean_absolute_error: 0.2639\nEpoch 88/100\n934/934 [==============================] - 0s 78us/sample - loss: 0.1309 - mean_absolute_error: 0.2762 - val_loss: 0.1104 - val_mean_absolute_error: 0.2623\nEpoch 89/100\n934/934 [==============================] - 0s 40us/sample - loss: 0.1281 - mean_absolute_error: 0.2723 - val_loss: 0.1079 - val_mean_absolute_error: 0.2585\nEpoch 90/100\n934/934 [==============================] - 0s 57us/sample - loss: 0.1249 - mean_absolute_error: 0.2688 - val_loss: 0.1057 - val_mean_absolute_error: 0.2549\nEpoch 91/100\n934/934 [==============================] - 0s 42us/sample - loss: 0.1224 - mean_absolute_error: 0.2662 - val_loss: 0.1033 - val_mean_absolute_error: 0.2528\nEpoch 92/100\n934/934 [==============================] - 0s 43us/sample - loss: 0.1209 - mean_absolute_error: 0.2638 - val_loss: 0.1015 - val_mean_absolute_error: 0.2514\nEpoch 93/100\n934/934 [==============================] - 0s 44us/sample - loss: 0.1174 - mean_absolute_error: 0.2605 - val_loss: 0.0994 - val_mean_absolute_error: 0.2462\nEpoch 94/100\n934/934 [==============================] - 0s 73us/sample - loss: 0.1154 - mean_absolute_error: 0.2577 - val_loss: 0.0978 - val_mean_absolute_error: 0.2441\nEpoch 95/100\n934/934 [==============================] - 0s 43us/sample - loss: 0.1132 - mean_absolute_error: 0.2552 - val_loss: 0.0958 - val_mean_absolute_error: 0.2424\nEpoch 96/100\n934/934 [==============================] - 0s 47us/sample - loss: 0.1111 - mean_absolute_error: 0.2525 - val_loss: 0.0935 - val_mean_absolute_error: 0.2393\nEpoch 97/100\n934/934 [==============================] - 0s 44us/sample - loss: 0.1091 - mean_absolute_error: 0.2506 - val_loss: 0.0923 - val_mean_absolute_error: 0.2386\nEpoch 98/100\n934/934 [==============================] - 0s 45us/sample - loss: 0.1078 - mean_absolute_error: 0.2482 - val_loss: 0.0908 - val_mean_absolute_error: 0.2356\nEpoch 99/100\n934/934 [==============================] - 0s 53us/sample - loss: 0.1054 - mean_absolute_error: 0.2479 - val_loss: 0.0885 - val_mean_absolute_error: 0.2340\nEpoch 100/100\n934/934 [==============================] - 0s 72us/sample - loss: 0.1034 - mean_absolute_error: 0.2434 - val_loss: 0.0872 - val_mean_absolute_error: 0.2307\n"
    }
   ],
   "source": [
    "# モデル構築\n",
    "K.clear_session()\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(10, activation = tf.nn.relu, input_shape=(X_train.shape[1],)))\n",
    "model.add(tf.keras.layers.Dense(3, activation = tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "model.summary()\n",
    "\n",
    "# コンパイル\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=tf.train.AdamOptimizer(learning_rate=0.001),\n",
    "              metrics=['mae'])\n",
    "\n",
    "# 学習\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=32,\n",
    "                    epochs=100,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "y_pred_log [12.249089 11.586033 11.580055 12.089891 11.426519]\ny_test [12.20918779 11.79810441 11.60823564 12.16525065 11.38509209]\nmae 0.24865404265604582\n"
    }
   ],
   "source": [
    "y_pred_log = model.predict(X_test)\n",
    "print(\"y_pred_log\", y_pred_log.flatten()[:5])\n",
    "print(\"y_test\", y_test.flatten()[:5])\n",
    "print(\"mae\", mean_absolute_error(y_test, y_pred_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 375.2875 248.518125\" width=\"375.2875pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.518125 \r\nL 375.2875 248.518125 \r\nL 375.2875 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 33.2875 224.64 \r\nL 368.0875 224.64 \r\nL 368.0875 7.2 \r\nL 33.2875 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m1860a6632c\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"48.505682\" xlink:href=\"#m1860a6632c\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n      </defs>\r\n      <g transform=\"translate(45.324432 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"109.993285\" xlink:href=\"#m1860a6632c\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 20 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      </defs>\r\n      <g transform=\"translate(103.630785 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"171.480888\" xlink:href=\"#m1860a6632c\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 40 -->\r\n      <defs>\r\n       <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n      </defs>\r\n      <g transform=\"translate(165.118388 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"232.968492\" xlink:href=\"#m1860a6632c\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 60 -->\r\n      <defs>\r\n       <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n      </defs>\r\n      <g transform=\"translate(226.605992 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-54\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"294.456095\" xlink:href=\"#m1860a6632c\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 80 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n      </defs>\r\n      <g transform=\"translate(288.093595 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-56\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"355.943698\" xlink:href=\"#m1860a6632c\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 100 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g transform=\"translate(346.399948 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"ma9ea1b3428\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#ma9ea1b3428\" y=\"214.880967\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(19.925 218.680186)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#ma9ea1b3428\" y=\"186.302022\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(13.5625 190.101241)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#ma9ea1b3428\" y=\"157.723077\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 40 -->\r\n      <g transform=\"translate(13.5625 161.522296)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#ma9ea1b3428\" y=\"129.144133\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 60 -->\r\n      <g transform=\"translate(13.5625 132.943352)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-54\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#ma9ea1b3428\" y=\"100.565188\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 80 -->\r\n      <g transform=\"translate(13.5625 104.364407)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-56\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#ma9ea1b3428\" y=\"71.986243\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 100 -->\r\n      <g transform=\"translate(7.2 75.785462)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#ma9ea1b3428\" y=\"43.407299\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 120 -->\r\n      <g transform=\"translate(7.2 47.206517)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_8\">\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#ma9ea1b3428\" y=\"14.828354\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- 140 -->\r\n      <g transform=\"translate(7.2 18.627573)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_15\">\r\n    <path clip-path=\"url(#p4cb5f2b026)\" d=\"M 48.505682 17.083636 \r\nL 51.580062 26.783153 \r\nL 54.654442 38.821613 \r\nL 57.728822 53.416961 \r\nL 60.803202 70.230548 \r\nL 63.877583 87.892967 \r\nL 66.951963 105.10094 \r\nL 70.026343 121.159574 \r\nL 73.100723 135.493824 \r\nL 76.175103 147.757009 \r\nL 79.249483 157.949662 \r\nL 82.323864 166.405006 \r\nL 85.398244 173.747854 \r\nL 88.472624 180.180646 \r\nL 91.547004 185.613506 \r\nL 94.621384 190.40416 \r\nL 97.695764 194.418292 \r\nL 100.770145 197.661806 \r\nL 103.844525 200.260084 \r\nL 106.918905 202.307792 \r\nL 109.993285 203.941773 \r\nL 113.067665 205.297184 \r\nL 116.142045 206.373317 \r\nL 119.216426 207.26377 \r\nL 122.290806 207.979086 \r\nL 125.365186 208.601278 \r\nL 128.439566 209.141012 \r\nL 131.513946 209.62239 \r\nL 134.588326 210.054409 \r\nL 137.662707 210.462315 \r\nL 140.737087 210.820864 \r\nL 143.811467 211.130389 \r\nL 146.885847 211.412796 \r\nL 149.960227 211.671527 \r\nL 153.034607 211.919724 \r\nL 156.108988 212.142221 \r\nL 159.183368 212.356182 \r\nL 162.257748 212.533152 \r\nL 165.332128 212.703609 \r\nL 168.406508 212.859473 \r\nL 171.480888 213.008328 \r\nL 174.555269 213.15098 \r\nL 177.629649 213.277224 \r\nL 180.704029 213.388831 \r\nL 183.778409 213.495029 \r\nL 186.852789 213.586868 \r\nL 189.927169 213.67498 \r\nL 193.00155 213.750974 \r\nL 196.07593 213.825046 \r\nL 199.15031 213.888051 \r\nL 202.22469 213.957617 \r\nL 205.29907 214.013196 \r\nL 208.37345 214.071441 \r\nL 211.447831 214.121003 \r\nL 214.522211 214.175375 \r\nL 217.596591 214.219634 \r\nL 220.670971 214.262663 \r\nL 223.745351 214.300451 \r\nL 226.819731 214.333918 \r\nL 229.894112 214.369654 \r\nL 232.968492 214.397886 \r\nL 236.042872 214.42361 \r\nL 239.117252 214.444837 \r\nL 242.191632 214.467314 \r\nL 245.266012 214.488746 \r\nL 248.340393 214.50576 \r\nL 251.414773 214.522361 \r\nL 254.489153 214.537506 \r\nL 257.563533 214.550582 \r\nL 260.637913 214.562179 \r\nL 263.712293 214.574866 \r\nL 266.786674 214.584673 \r\nL 269.861054 214.595176 \r\nL 272.935434 214.605784 \r\nL 276.009814 214.614839 \r\nL 279.084194 214.621367 \r\nL 282.158574 214.63082 \r\nL 285.232955 214.6376 \r\nL 288.307335 214.646349 \r\nL 291.381715 214.652633 \r\nL 294.456095 214.659531 \r\nL 297.530475 214.663641 \r\nL 300.604855 214.670203 \r\nL 303.679236 214.67654 \r\nL 306.753616 214.681144 \r\nL 309.827996 214.68546 \r\nL 312.902376 214.689994 \r\nL 315.976756 214.69398 \r\nL 319.051136 214.697888 \r\nL 322.125517 214.702468 \r\nL 325.199897 214.706065 \r\nL 328.274277 214.708246 \r\nL 331.348657 214.713262 \r\nL 334.423037 214.716005 \r\nL 337.497417 214.719176 \r\nL 340.571798 214.722226 \r\nL 343.646178 214.725069 \r\nL 346.720558 214.726897 \r\nL 349.794938 214.730308 \r\nL 352.869318 214.733171 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_16\">\r\n    <path clip-path=\"url(#p4cb5f2b026)\" d=\"M 48.505682 23.251217 \r\nL 51.580062 34.591366 \r\nL 54.654442 48.303146 \r\nL 57.728822 64.523767 \r\nL 60.803202 82.861153 \r\nL 63.877583 100.953354 \r\nL 66.951963 118.631671 \r\nL 70.026343 134.723446 \r\nL 73.100723 148.672178 \r\nL 76.175103 160.14764 \r\nL 79.249483 169.619831 \r\nL 82.323864 177.139324 \r\nL 85.398244 183.397291 \r\nL 88.472624 188.580206 \r\nL 91.547004 192.721943 \r\nL 94.621384 196.380342 \r\nL 97.695764 199.395154 \r\nL 100.770145 201.795405 \r\nL 103.844525 203.6554 \r\nL 106.918905 205.158791 \r\nL 109.993285 206.45806 \r\nL 113.067665 207.467621 \r\nL 116.142045 208.287426 \r\nL 119.216426 208.993529 \r\nL 122.290806 209.538935 \r\nL 125.365186 210.038496 \r\nL 128.439566 210.470732 \r\nL 131.513946 210.849371 \r\nL 134.588326 211.220539 \r\nL 137.662707 211.560027 \r\nL 140.737087 211.832678 \r\nL 143.811467 212.088182 \r\nL 146.885847 212.324441 \r\nL 149.960227 212.544013 \r\nL 153.034607 212.7413 \r\nL 156.108988 212.920379 \r\nL 159.183368 213.083609 \r\nL 162.257748 213.231422 \r\nL 165.332128 213.349705 \r\nL 168.406508 213.463292 \r\nL 171.480888 213.558908 \r\nL 174.555269 213.657646 \r\nL 177.629649 213.728125 \r\nL 180.704029 213.818267 \r\nL 183.778409 213.884449 \r\nL 186.852789 213.94496 \r\nL 189.927169 214.006823 \r\nL 193.00155 214.067851 \r\nL 196.07593 214.118685 \r\nL 199.15031 214.167949 \r\nL 202.22469 214.221127 \r\nL 205.29907 214.263934 \r\nL 208.37345 214.298822 \r\nL 211.447831 214.339207 \r\nL 214.522211 214.37175 \r\nL 217.596591 214.404074 \r\nL 220.670971 214.433726 \r\nL 223.745351 214.46381 \r\nL 226.819731 214.485733 \r\nL 229.894112 214.508535 \r\nL 232.968492 214.5288 \r\nL 236.042872 214.547061 \r\nL 239.117252 214.562999 \r\nL 242.191632 214.576553 \r\nL 245.266012 214.589012 \r\nL 248.340393 214.60147 \r\nL 251.414773 214.610839 \r\nL 254.489153 214.618448 \r\nL 257.563533 214.626457 \r\nL 260.637913 214.63557 \r\nL 263.712293 214.644574 \r\nL 266.786674 214.649914 \r\nL 269.861054 214.656979 \r\nL 272.935434 214.663389 \r\nL 276.009814 214.668623 \r\nL 279.084194 214.674363 \r\nL 282.158574 214.679694 \r\nL 285.232955 214.684641 \r\nL 288.307335 214.688115 \r\nL 291.381715 214.692593 \r\nL 294.456095 214.697667 \r\nL 297.530475 214.701802 \r\nL 300.604855 214.704906 \r\nL 303.679236 214.709877 \r\nL 306.753616 214.713867 \r\nL 309.827996 214.716111 \r\nL 312.902376 214.720572 \r\nL 315.976756 214.72314 \r\nL 319.051136 214.726827 \r\nL 322.125517 214.729874 \r\nL 325.199897 214.733345 \r\nL 328.274277 214.735944 \r\nL 331.348657 214.738931 \r\nL 334.423037 214.741267 \r\nL 337.497417 214.744048 \r\nL 340.571798 214.747381 \r\nL 343.646178 214.749031 \r\nL 346.720558 214.75125 \r\nL 349.794938 214.754444 \r\nL 352.869318 214.756364 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 33.2875 224.64 \r\nL 33.2875 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 368.0875 224.64 \r\nL 368.0875 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 33.2875 224.64 \r\nL 368.0875 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 33.2875 7.2 \r\nL 368.0875 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"legend_1\">\r\n    <g id=\"patch_7\">\r\n     <path d=\"M 289.946875 44.834375 \r\nL 361.0875 44.834375 \r\nQ 363.0875 44.834375 363.0875 42.834375 \r\nL 363.0875 14.2 \r\nQ 363.0875 12.2 361.0875 12.2 \r\nL 289.946875 12.2 \r\nQ 287.946875 12.2 287.946875 14.2 \r\nL 287.946875 42.834375 \r\nQ 287.946875 44.834375 289.946875 44.834375 \r\nz\r\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\r\n    </g>\r\n    <g id=\"line2d_17\">\r\n     <path d=\"M 291.946875 20.298437 \r\nL 311.946875 20.298437 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_18\"/>\r\n    <g id=\"text_15\">\r\n     <!-- loss -->\r\n     <defs>\r\n      <path d=\"M 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\n\" id=\"DejaVuSans-108\"/>\r\n      <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n      <path d=\"M 44.28125 53.078125 \r\nL 44.28125 44.578125 \r\nQ 40.484375 46.53125 36.375 47.5 \r\nQ 32.28125 48.484375 27.875 48.484375 \r\nQ 21.1875 48.484375 17.84375 46.4375 \r\nQ 14.5 44.390625 14.5 40.28125 \r\nQ 14.5 37.15625 16.890625 35.375 \r\nQ 19.28125 33.59375 26.515625 31.984375 \r\nL 29.59375 31.296875 \r\nQ 39.15625 29.25 43.1875 25.515625 \r\nQ 47.21875 21.78125 47.21875 15.09375 \r\nQ 47.21875 7.46875 41.1875 3.015625 \r\nQ 35.15625 -1.421875 24.609375 -1.421875 \r\nQ 20.21875 -1.421875 15.453125 -0.5625 \r\nQ 10.6875 0.296875 5.421875 2 \r\nL 5.421875 11.28125 \r\nQ 10.40625 8.6875 15.234375 7.390625 \r\nQ 20.0625 6.109375 24.8125 6.109375 \r\nQ 31.15625 6.109375 34.5625 8.28125 \r\nQ 37.984375 10.453125 37.984375 14.40625 \r\nQ 37.984375 18.0625 35.515625 20.015625 \r\nQ 33.0625 21.96875 24.703125 23.78125 \r\nL 21.578125 24.515625 \r\nQ 13.234375 26.265625 9.515625 29.90625 \r\nQ 5.8125 33.546875 5.8125 39.890625 \r\nQ 5.8125 47.609375 11.28125 51.796875 \r\nQ 16.75 56 26.8125 56 \r\nQ 31.78125 56 36.171875 55.265625 \r\nQ 40.578125 54.546875 44.28125 53.078125 \r\nz\r\n\" id=\"DejaVuSans-115\"/>\r\n     </defs>\r\n     <g transform=\"translate(319.946875 23.798437)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"line2d_19\">\r\n     <path d=\"M 291.946875 34.976562 \r\nL 311.946875 34.976562 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_20\"/>\r\n    <g id=\"text_16\">\r\n     <!-- val_loss -->\r\n     <defs>\r\n      <path d=\"M 2.984375 54.6875 \r\nL 12.5 54.6875 \r\nL 29.59375 8.796875 \r\nL 46.6875 54.6875 \r\nL 56.203125 54.6875 \r\nL 35.6875 0 \r\nL 23.484375 0 \r\nz\r\n\" id=\"DejaVuSans-118\"/>\r\n      <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n      <path d=\"M 50.984375 -16.609375 \r\nL 50.984375 -23.578125 \r\nL -0.984375 -23.578125 \r\nL -0.984375 -16.609375 \r\nz\r\n\" id=\"DejaVuSans-95\"/>\r\n     </defs>\r\n     <g transform=\"translate(319.946875 38.476562)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-118\"/>\r\n      <use x=\"59.179688\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"120.458984\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"148.242188\" xlink:href=\"#DejaVuSans-95\"/>\r\n      <use x=\"198.242188\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"226.025391\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"287.207031\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"339.306641\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p4cb5f2b026\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"33.2875\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "plt.plot(range(len(history.history['loss'])), history.history['loss'], label='loss')\n",
    "plt.plot(range(len(history.history['val_loss'])), history.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題6】\n",
    "#### MNISTをKerasで学習\n",
    "TensorFlowによるMNISTデータセットによる画像の多値分類をKerasに書き換えてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(48000, 28, 28, 1)\n(12000, 28, 28, 1)\n(10000, 28, 28, 1)\n"
    }
   ],
   "source": [
    "# 読み込み\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.astype(np.float)[:, :, :, np.newaxis]\n",
    "X_test = X_test.astype(np.float)[:, :, :, np.newaxis]\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(48000, 10)\n(12000, 10)\n(10000, 10)\n"
    }
   ],
   "source": [
    "y_train = np.identity(10)[y_train]\n",
    "y_val = np.identity(10)[y_val]\n",
    "y_test = np.identity(10)[y_test]\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d (Conv2D)              (None, 26, 26, 32)        320       \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 24, 24, 16)        4624      \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 12, 12, 16)        0         \n_________________________________________________________________\nflatten (Flatten)            (None, 2304)              0         \n_________________________________________________________________\ndense (Dense)                (None, 10)                23050     \n=================================================================\nTotal params: 27,994\nTrainable params: 27,994\nNon-trainable params: 0\n_________________________________________________________________\nTrain on 48000 samples, validate on 12000 samples\nEpoch 1/5\n48000/48000 [==============================] - 19s 405us/sample - loss: 0.1603 - acc: 0.9517 - val_loss: 0.0760 - val_acc: 0.9758\nEpoch 2/5\n48000/48000 [==============================] - 20s 410us/sample - loss: 0.0604 - acc: 0.9811 - val_loss: 0.0727 - val_acc: 0.9783\nEpoch 3/5\n48000/48000 [==============================] - 20s 412us/sample - loss: 0.0418 - acc: 0.9868 - val_loss: 0.0710 - val_acc: 0.9790\nEpoch 4/5\n48000/48000 [==============================] - 20s 426us/sample - loss: 0.0343 - acc: 0.9886 - val_loss: 0.0626 - val_acc: 0.9819\nEpoch 5/5\n48000/48000 [==============================] - 20s 411us/sample - loss: 0.0278 - acc: 0.9911 - val_loss: 0.0638 - val_acc: 0.9826\n"
    }
   ],
   "source": [
    "# モデル構築\n",
    "import tensorflow.keras.layers as layers\n",
    "K.clear_session()\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Conv2D(filters=32, kernel_size=(3, 3), activation = tf.nn.relu, input_shape=X_train.shape[1:]))\n",
    "model.add(layers.Conv2D(filters=16, kernel_size=(3, 3), activation = tf.nn.relu))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(10, activation = tf.nn.softmax))\n",
    "model.summary()\n",
    "\n",
    "# コンパイル\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=tf.train.AdamOptimizer(learning_rate=0.01),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 学習\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=5,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "y_pred_proba [[4.50540889e-11 8.54929463e-14 9.51129042e-09 1.44093992e-05\n  1.16815330e-13 1.22431687e-13 3.65508365e-18 9.99985576e-01\n  3.11721671e-08 4.77674067e-10]\n [1.02692976e-09 1.78671198e-05 9.99982119e-01 1.29712005e-11\n  1.75582869e-13 3.11191570e-14 5.25786028e-08 1.65300729e-13\n  3.00675351e-09 5.25417123e-12]\n [1.34502659e-06 9.99050319e-01 4.49115796e-05 2.72551506e-06\n  4.45721053e-05 2.98599093e-06 1.49851885e-05 4.99534326e-05\n  7.87993835e-04 2.57664283e-07]\n [9.99986768e-01 1.93506280e-10 1.19158028e-06 2.60786437e-09\n  2.05728501e-09 4.84825013e-10 2.04021447e-07 1.07303713e-05\n  2.39675728e-08 1.09373923e-06]\n [1.00283764e-08 6.36121433e-11 3.45532769e-11 3.02338741e-08\n  9.99974728e-01 1.89901372e-10 2.08444262e-09 7.70580755e-08\n  3.28072787e-07 2.47830139e-05]]\ny_pred [7 2 1 ... 4 5 6]\ny_test [7 2 1 ... 4 5 6]\ntest_acc 0.9837\n"
    }
   ],
   "source": [
    "y_pred_proba = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "print(\"y_pred_proba\", y_pred_proba[:5])\n",
    "print(\"y_pred\", y_pred)\n",
    "print(\"y_test\", np.argmax(y_test, axis=1))\n",
    "print(\"test_acc\", accuracy_score(np.argmax(y_test, axis=1), y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題7】\n",
    "#### （アドバンス課題）PyTorchへの書き換え\n",
    "4種類の問題をPyTorchに書き換えてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras: (N, H, W, C)\n",
    "\n",
    "PyTorch: (N, C, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "      Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm  \\\n0      1            5.1           3.5            1.4           0.2   \n1      2            4.9           3.0            1.4           0.2   \n2      3            4.7           3.2            1.3           0.2   \n3      4            4.6           3.1            1.5           0.2   \n4      5            5.0           3.6            1.4           0.2   \n..   ...            ...           ...            ...           ...   \n145  146            6.7           3.0            5.2           2.3   \n146  147            6.3           2.5            5.0           1.9   \n147  148            6.5           3.0            5.2           2.0   \n148  149            6.2           3.4            5.4           2.3   \n149  150            5.9           3.0            5.1           1.8   \n\n            Species  \n0       Iris-setosa  \n1       Iris-setosa  \n2       Iris-setosa  \n3       Iris-setosa  \n4       Iris-setosa  \n..              ...  \n145  Iris-virginica  \n146  Iris-virginica  \n147  Iris-virginica  \n148  Iris-virginica  \n149  Iris-virginica  \n\n[150 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>SepalLengthCm</th>\n      <th>SepalWidthCm</th>\n      <th>PetalLengthCm</th>\n      <th>PetalWidthCm</th>\n      <th>Species</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>5.1</td>\n      <td>3.5</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>4.9</td>\n      <td>3.0</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>4.7</td>\n      <td>3.2</td>\n      <td>1.3</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>4.6</td>\n      <td>3.1</td>\n      <td>1.5</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>5.0</td>\n      <td>3.6</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>146</td>\n      <td>6.7</td>\n      <td>3.0</td>\n      <td>5.2</td>\n      <td>2.3</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>146</th>\n      <td>147</td>\n      <td>6.3</td>\n      <td>2.5</td>\n      <td>5.0</td>\n      <td>1.9</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>148</td>\n      <td>6.5</td>\n      <td>3.0</td>\n      <td>5.2</td>\n      <td>2.0</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>148</th>\n      <td>149</td>\n      <td>6.2</td>\n      <td>3.4</td>\n      <td>5.4</td>\n      <td>2.3</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>149</th>\n      <td>150</td>\n      <td>5.9</td>\n      <td>3.0</td>\n      <td>5.1</td>\n      <td>1.8</td>\n      <td>Iris-virginica</td>\n    </tr>\n  </tbody>\n</table>\n<p>150 rows × 6 columns</p>\n</div>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "(100, 4)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "(100, 1)"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(64, 4) (64, 1)\n(16, 4) (16, 1)\n(20, 4) (20, 1)\n"
    }
   ],
   "source": [
    "# 【問題3】Iris（2値分類）\n",
    "## 読み込み\n",
    "df_iris = pd.read_csv('Iris.csv')\n",
    "display(df_iris)\n",
    "X = np.array(df_iris[(df_iris['Species'] == 'Iris-versicolor')|(df_iris['Species'] == 'Iris-virginica')].iloc[:, 1:5])\n",
    "y = np.array(df_iris[(df_iris['Species'] == 'Iris-versicolor')|(df_iris['Species'] == 'Iris-virginica')].iloc[:, 5])\n",
    "y[y=='Iris-versicolor'] = 0\n",
    "y[y=='Iris-virginica'] = 1\n",
    "y = y.astype(np.int)[:, np.newaxis]\n",
    "display(X.shape)\n",
    "display(y.shape)\n",
    "\n",
    "## 分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "## 標準化\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "NNfor3(\n  (layer_1): Linear(in_features=4, out_features=10, bias=True)\n  (layer_2): Linear(in_features=10, out_features=5, bias=True)\n  (layer_output): Linear(in_features=5, out_features=1, bias=True)\n)\n"
    }
   ],
   "source": [
    "# PyTorchでモデル構築\n",
    "n_features = 4\n",
    "n_nodes_1 = 10\n",
    "activation_1 = nn.ReLU()\n",
    "n_nodes_2 = 5\n",
    "activation_2 = nn.ReLU()\n",
    "n_output = 1\n",
    "activation_output = nn.Sigmoid()\n",
    "\n",
    "## ネットワーク\n",
    "class NNfor3(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(NNfor3, self).__init__()\n",
    "        self.layer_1 = nn.Linear(\n",
    "            n_features,\n",
    "            n_nodes_1)\n",
    "        self.layer_2 = nn.Linear(\n",
    "            n_nodes_1,\n",
    "            n_nodes_2)\n",
    "        self.layer_output = nn.Linear(\n",
    "            n_nodes_2,\n",
    "            n_output)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = activation_1(self.layer_1(X))\n",
    "        X = activation_2(self.layer_2(X))\n",
    "        X = activation_output(self.layer_output(X))\n",
    "\n",
    "        return X\n",
    "\n",
    "## インスタンス作成\n",
    "model = NNfor3()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ミニバッチ\n",
    "batch_size = 1\n",
    "\n",
    "## numpy配列をテンソルに変換\n",
    "t_X_train = torch.from_numpy(X_train).float()\n",
    "t_y_train = torch.from_numpy(y_train).float()\n",
    "t_X_valid = torch.from_numpy(X_val).float()\n",
    "t_y_valid = torch.from_numpy(y_val).float()\n",
    "\n",
    "## 特徴量とラベルをデータセットにまとめる\n",
    "dataset_train = TensorDataset(t_X_train, t_y_train)\n",
    "dataset_valid = TensorDataset(t_X_valid, t_y_valid)\n",
    "\n",
    "# データローダを作成\n",
    "loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "loader_valid = DataLoader(dataset_valid, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## オプティマイザ\n",
    "lr = 0.003\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=lr\n",
    ")\n",
    "\n",
    "## 損失関数\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 学習(train)\n",
    "def train_step(X_train, y_train):\n",
    "    ## 訓練モード    \n",
    "    model.train()\n",
    "\n",
    "    ## 訓練データに対して予測\n",
    "    y_train_pred = model(X_train)\n",
    "\n",
    "    ## 勾配初期化\n",
    "    optimizer.zero_grad()\n",
    "    ## 損失の取得\n",
    "    loss = criterion(y_train_pred, y_train)\n",
    "    ## 勾配を計算\n",
    "    loss.backward()\n",
    "\n",
    "    ## パラメータ更新\n",
    "    optimizer.step()\n",
    "\n",
    "    ## 正解率の確認\n",
    "    with torch.no_grad():\n",
    "        y_train_pred_binary = torch.where(y_train_pred>0.5, torch.ones(1), torch.zeros(1))\n",
    "        acc = (y_train_pred_binary == y_train).sum()\n",
    "\n",
    "    return (loss.item(), acc.item())\n",
    "\n",
    "## 評価(validation)\n",
    "def valid_step(X_val, y_val):\n",
    "    ## 評価モード\n",
    "    model.eval()\n",
    "\n",
    "    ## 予測\n",
    "    y_val_pred = model(X_val)\n",
    "\n",
    "    ## 損失計算\n",
    "    loss = criterion(y_val_pred, y_val)\n",
    "\n",
    "    ## 正解率計算\n",
    "    with torch.no_grad():\n",
    "        y_val_pred_binary = torch.where(y_val_pred>0.5, torch.ones(1), torch.zeros(1))\n",
    "        acc = (y_val_pred_binary == y_val).sum()\n",
    "\n",
    "    return (loss.item(), acc.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch1/10: [loss:0.4673, acc:0.8281, val_loss:0.2662, val_acc:1.0000]\nepoch2/10: [loss:0.3414, acc:0.9375, val_loss:0.2008, val_acc:1.0000]\nepoch3/10: [loss:0.1950, acc:0.9531, val_loss:0.0794, val_acc:1.0000]\nepoch4/10: [loss:0.1125, acc:0.9688, val_loss:0.0461, val_acc:1.0000]\nepoch5/10: [loss:0.0822, acc:0.9688, val_loss:0.0392, val_acc:1.0000]\nepoch6/10: [loss:0.0672, acc:0.9844, val_loss:0.0341, val_acc:1.0000]\nepoch7/10: [loss:0.0559, acc:0.9844, val_loss:0.0297, val_acc:1.0000]\nepoch8/10: [loss:0.0482, acc:0.9844, val_loss:0.0255, val_acc:1.0000]\nepoch9/10: [loss:0.0434, acc:0.9844, val_loss:0.0243, val_acc:1.0000]\nepoch10/10: [loss:0.0372, acc:0.9844, val_loss:0.0238, val_acc:1.0000]\nDone!\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "OrderedDict([('layer_1.weight',\n              tensor([[-0.3253,  0.9297,  0.3127,  1.5018],\n                      [-0.6653,  1.2888, -0.6017, -0.8112],\n                      [ 0.2163,  0.1430, -0.1449, -0.8373],\n                      [-0.3236,  1.1077,  0.5097, -0.2879],\n                      [-0.2199,  0.6093, -1.0773, -1.2354],\n                      [ 0.0216,  0.8115,  0.2423,  0.3984],\n                      [-1.0375, -0.6394,  2.1395,  0.3024],\n                      [-0.5329, -0.4405,  1.2749,  0.3070],\n                      [ 0.8695, -0.4825, -1.1003, -1.0965],\n                      [-0.0246, -0.4182,  0.8111, -0.2089]])),\n             ('layer_1.bias',\n              tensor([ 0.3441, -0.0395,  0.0076,  0.0793,  0.3370, -0.1762,  0.3806,  0.3751,\n                       0.0079, -0.1390])),\n             ('layer_2.weight',\n              tensor([[ 0.7094, -0.6308, -0.8411,  0.3480,  0.2334, -0.5125,  0.4507,  0.9466,\n                       -0.3672,  0.2688],\n                      [ 0.2643,  0.0674, -0.2163, -0.6054,  0.3484, -0.3392, -0.6753, -0.2904,\n                        0.0371, -1.0881],\n                      [ 0.3968, -0.0486, -0.8297, -1.5373,  0.3235, -0.4110,  0.0438,  0.7733,\n                        0.4242,  0.0162],\n                      [ 0.2687,  0.3942, -0.3796,  0.3949,  1.2079, -0.6717, -0.5447,  0.3158,\n                        1.1171,  0.3218],\n                      [-0.7774,  0.2454, -0.3085, -0.3619,  0.2312,  0.1414, -0.8785, -0.1092,\n                       -0.0712,  0.1316]])),\n             ('layer_2.bias',\n              tensor([ 0.3303, -0.1944,  0.1274, -0.0523, -0.1612])),\n             ('layer_output.weight',\n              tensor([[ 1.3503,  0.0697,  0.7864, -1.6552,  0.0469]])),\n             ('layer_output.bias', tensor([0.2616]))])"
     },
     "metadata": {}
    }
   ],
   "source": [
    "## パラメータ初期化関数\n",
    "def initializer(layer):\n",
    "    if type(layer) == nn.Linear:\n",
    "        nn.init.kaiming_normal_(layer.weight)\n",
    "        layer.bias.data.fill_(0.0)\n",
    "\n",
    "## パラメータ初期化する\n",
    "model.apply(initializer)\n",
    "\n",
    "n_epoch = 10\n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "# 1エポック内1データあたりの損失・正解率の平均\n",
    "avg_loss = 0.0\n",
    "avg_acc = 0.0\n",
    "avg_val_loss = 0.0\n",
    "avg_val_acc = 0.0\n",
    "\n",
    "## 学習する\n",
    "for epoch in range(n_epoch):\n",
    "    ## epoch内のloss, accの保存\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    total_val_loss = 0.0\n",
    "    total_val_acc = 0.0\n",
    "    ## 累計データ数の保存\n",
    "    total_train = 0\n",
    "    total_val = 0\n",
    "\n",
    "    ## 訓練\n",
    "    for X_train, y_train in loader_train:\n",
    "       loss, acc = train_step(X_train, y_train)\n",
    "       total_loss += loss\n",
    "       total_acc += acc\n",
    "       total_train += len(y_train)\n",
    "\n",
    "    ## 評価\n",
    "    for X_val, y_val in loader_valid:\n",
    "        val_loss, val_acc = valid_step(X_val, y_val)\n",
    "        total_val_loss += val_loss\n",
    "        total_val_acc += val_acc\n",
    "        total_val += len(y_val)\n",
    "\n",
    "    # 平均計算\n",
    "    avg_loss = total_loss/total_train\n",
    "    avg_val_loss = total_val_loss/total_val\n",
    "    avg_acc = total_acc/total_train\n",
    "    avg_val_acc = total_val_acc/total_val\n",
    "\n",
    "    print(f'epoch{epoch+1}/{n_epoch}: [loss:{avg_loss:.4f}, acc:{avg_acc:.4f}, val_loss:{avg_val_loss:.4f}, val_acc:{avg_val_acc:.4f}]')\n",
    "\n",
    "    # リストに追加\n",
    "    train_history.append(avg_loss)\n",
    "    val_history.append(avg_val_loss)\n",
    "\n",
    "print('Done!')\n",
    "display(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "y_pred_proba tensor([[1.8434e-02],\n        [9.9915e-01],\n        [2.0774e-02],\n        [9.9970e-01],\n        [9.8462e-01],\n        [9.9928e-01],\n        [8.5120e-03],\n        [9.3961e-01],\n        [9.9970e-01],\n        [9.9507e-01],\n        [9.9738e-01],\n        [9.9759e-01],\n        [9.9959e-01],\n        [1.0636e-02],\n        [4.6226e-06],\n        [8.8159e-05],\n        [5.5456e-01],\n        [7.1062e-04],\n        [9.8060e-01],\n        [2.4978e-03]], grad_fn=<SigmoidBackward>)\ny_pred [0 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 1 0 1 0]\ny_test [0 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 0 0 0 0]\n"
    }
   ],
   "source": [
    "# 予測\n",
    "## Tensor変換\n",
    "t_X_test = torch.from_numpy(X_test).float()\n",
    "t_y_test = torch.from_numpy(y_test).float()\n",
    "y_pred_proba = model(t_X_test)\n",
    "y_pred = torch.where(y_pred_proba>0.5, torch.ones(1), torch.zeros(1))\n",
    "print(\"y_pred_proba\", y_pred_proba)\n",
    "print(\"y_pred\", y_pred.flatten().int().detach().numpy())\n",
    "print(\"y_test\", y_test.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 372.103125 248.518125\" width=\"372.103125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.518125 \r\nL 372.103125 248.518125 \r\nL 372.103125 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 30.103125 224.64 \r\nL 364.903125 224.64 \r\nL 364.903125 7.2 \r\nL 30.103125 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m86a11adf6f\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"45.321307\" xlink:href=\"#m86a11adf6f\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n      </defs>\r\n      <g transform=\"translate(42.140057 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"112.95767\" xlink:href=\"#m86a11adf6f\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 2 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      </defs>\r\n      <g transform=\"translate(109.77642 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"180.594034\" xlink:href=\"#m86a11adf6f\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 4 -->\r\n      <defs>\r\n       <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n      </defs>\r\n      <g transform=\"translate(177.412784 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"248.230398\" xlink:href=\"#m86a11adf6f\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 6 -->\r\n      <defs>\r\n       <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n      </defs>\r\n      <g transform=\"translate(245.049148 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"315.866761\" xlink:href=\"#m86a11adf6f\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 8 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n      </defs>\r\n      <g transform=\"translate(312.685511 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-56\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_6\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m995963c6d3\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m995963c6d3\" y=\"180.781147\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 0.1 -->\r\n      <defs>\r\n       <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 184.580366)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-49\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m995963c6d3\" y=\"136.208934\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0.2 -->\r\n      <g transform=\"translate(7.2 140.008153)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m995963c6d3\" y=\"91.636722\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0.3 -->\r\n      <defs>\r\n       <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 95.435941)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-51\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m995963c6d3\" y=\"47.064509\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 0.4 -->\r\n      <g transform=\"translate(7.2 50.863728)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_10\">\r\n    <path clip-path=\"url(#pf056ce2ae4)\" d=\"M 45.321307 17.083636 \r\nL 79.139489 73.172133 \r\nL 112.95767 138.417712 \r\nL 146.775852 175.221475 \r\nL 180.594034 188.71648 \r\nL 214.412216 195.418754 \r\nL 248.230398 200.432071 \r\nL 282.04858 203.867871 \r\nL 315.866761 206.003834 \r\nL 349.684943 208.751995 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_11\">\r\n    <path clip-path=\"url(#pf056ce2ae4)\" d=\"M 45.321307 106.709228 \r\nL 79.139489 135.834418 \r\nL 112.95767 189.966017 \r\nL 146.775852 204.791236 \r\nL 180.594034 207.901214 \r\nL 214.412216 210.161648 \r\nL 248.230398 212.116304 \r\nL 282.04858 213.980144 \r\nL 315.866761 214.523366 \r\nL 349.684943 214.756364 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 30.103125 224.64 \r\nL 30.103125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 364.903125 224.64 \r\nL 364.903125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 30.103125 224.64 \r\nL 364.903125 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 30.103125 7.2 \r\nL 364.903125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"legend_1\">\r\n    <g id=\"patch_7\">\r\n     <path d=\"M 286.7625 44.834375 \r\nL 357.903125 44.834375 \r\nQ 359.903125 44.834375 359.903125 42.834375 \r\nL 359.903125 14.2 \r\nQ 359.903125 12.2 357.903125 12.2 \r\nL 286.7625 12.2 \r\nQ 284.7625 12.2 284.7625 14.2 \r\nL 284.7625 42.834375 \r\nQ 284.7625 44.834375 286.7625 44.834375 \r\nz\r\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\r\n    </g>\r\n    <g id=\"line2d_12\">\r\n     <path d=\"M 288.7625 20.298437 \r\nL 308.7625 20.298437 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_13\"/>\r\n    <g id=\"text_10\">\r\n     <!-- loss -->\r\n     <defs>\r\n      <path d=\"M 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\n\" id=\"DejaVuSans-108\"/>\r\n      <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n      <path d=\"M 44.28125 53.078125 \r\nL 44.28125 44.578125 \r\nQ 40.484375 46.53125 36.375 47.5 \r\nQ 32.28125 48.484375 27.875 48.484375 \r\nQ 21.1875 48.484375 17.84375 46.4375 \r\nQ 14.5 44.390625 14.5 40.28125 \r\nQ 14.5 37.15625 16.890625 35.375 \r\nQ 19.28125 33.59375 26.515625 31.984375 \r\nL 29.59375 31.296875 \r\nQ 39.15625 29.25 43.1875 25.515625 \r\nQ 47.21875 21.78125 47.21875 15.09375 \r\nQ 47.21875 7.46875 41.1875 3.015625 \r\nQ 35.15625 -1.421875 24.609375 -1.421875 \r\nQ 20.21875 -1.421875 15.453125 -0.5625 \r\nQ 10.6875 0.296875 5.421875 2 \r\nL 5.421875 11.28125 \r\nQ 10.40625 8.6875 15.234375 7.390625 \r\nQ 20.0625 6.109375 24.8125 6.109375 \r\nQ 31.15625 6.109375 34.5625 8.28125 \r\nQ 37.984375 10.453125 37.984375 14.40625 \r\nQ 37.984375 18.0625 35.515625 20.015625 \r\nQ 33.0625 21.96875 24.703125 23.78125 \r\nL 21.578125 24.515625 \r\nQ 13.234375 26.265625 9.515625 29.90625 \r\nQ 5.8125 33.546875 5.8125 39.890625 \r\nQ 5.8125 47.609375 11.28125 51.796875 \r\nQ 16.75 56 26.8125 56 \r\nQ 31.78125 56 36.171875 55.265625 \r\nQ 40.578125 54.546875 44.28125 53.078125 \r\nz\r\n\" id=\"DejaVuSans-115\"/>\r\n     </defs>\r\n     <g transform=\"translate(316.7625 23.798437)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"line2d_14\">\r\n     <path d=\"M 288.7625 34.976562 \r\nL 308.7625 34.976562 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_15\"/>\r\n    <g id=\"text_11\">\r\n     <!-- val_loss -->\r\n     <defs>\r\n      <path d=\"M 2.984375 54.6875 \r\nL 12.5 54.6875 \r\nL 29.59375 8.796875 \r\nL 46.6875 54.6875 \r\nL 56.203125 54.6875 \r\nL 35.6875 0 \r\nL 23.484375 0 \r\nz\r\n\" id=\"DejaVuSans-118\"/>\r\n      <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n      <path d=\"M 50.984375 -16.609375 \r\nL 50.984375 -23.578125 \r\nL -0.984375 -23.578125 \r\nL -0.984375 -16.609375 \r\nz\r\n\" id=\"DejaVuSans-95\"/>\r\n     </defs>\r\n     <g transform=\"translate(316.7625 38.476562)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-118\"/>\r\n      <use x=\"59.179688\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"120.458984\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"148.242188\" xlink:href=\"#DejaVuSans-95\"/>\r\n      <use x=\"198.242188\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"226.025391\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"287.207031\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"339.306641\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"pf056ce2ae4\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"30.103125\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "plt.plot(train_history, label=\"loss\")\n",
    "plt.plot(val_history, label=\"val_loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "(150, 4)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "(150,)"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(96, 4) (96,)\n(24, 4) (24,)\n(30, 4) (30,)\n"
    }
   ],
   "source": [
    "# 【問題4】Iris（多値分類）をKerasで学習\n",
    "# 抽出\n",
    "X = np.array(df_iris.iloc[:, 1:5])\n",
    "y = np.array(df_iris.iloc[:, 5])\n",
    "y[y=='Iris-versicolor'] = 0\n",
    "y[y=='Iris-virginica'] = 1\n",
    "y[y=='Iris-setosa'] = 2\n",
    "y = y.astype(np.int)\n",
    "display(X.shape)\n",
    "display(y.shape)\n",
    "\n",
    "# 分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "# 標準化\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "NNfor4(\n  (layer): Sequential(\n    (0): Linear(in_features=4, out_features=100, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=100, out_features=10, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=10, out_features=3, bias=True)\n  )\n)\n"
    }
   ],
   "source": [
    "# PyTorchでモデル構築\n",
    "n_features = 4\n",
    "n_nodes_1 = 100\n",
    "activation_1 = nn.ReLU()\n",
    "n_nodes_2 = 10\n",
    "activation_2 = nn.ReLU()\n",
    "n_output = 3\n",
    "\n",
    "## ネットワーク\n",
    "class NNfor4(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(NNfor4, self).__init__()\n",
    "        self.layer = nn.Sequential(nn.Linear(n_features, n_nodes_1), activation_1,\n",
    "                                   nn.Linear(n_nodes_1, n_nodes_2), activation_2,\n",
    "                                   nn.Linear(n_nodes_2, n_output))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.layer(X)\n",
    "\n",
    "        return X\n",
    "\n",
    "## インスタンス作成\n",
    "model = NNfor4()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ミニバッチ\n",
    "batch_size = 1\n",
    "\n",
    "## numpy配列をテンソルに変換\n",
    "t_X_train = torch.from_numpy(X_train).float()\n",
    "t_y_train = torch.from_numpy(y_train).long()\n",
    "t_X_valid = torch.from_numpy(X_val).float()\n",
    "t_y_valid = torch.from_numpy(y_val).long()\n",
    "\n",
    "## 特徴量とラベルをデータセットにまとめる\n",
    "dataset_train = TensorDataset(t_X_train, t_y_train)\n",
    "dataset_valid = TensorDataset(t_X_valid, t_y_valid)\n",
    "\n",
    "# データローダを作成\n",
    "loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "loader_valid = DataLoader(dataset_valid, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## オプティマイザ\n",
    "lr = 0.003\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=lr\n",
    ")\n",
    "\n",
    "## 損失関数\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 学習(train)\n",
    "def train_step(X_train, y_train):\n",
    "    ## 訓練モード    \n",
    "    model.train()\n",
    "\n",
    "    ## 訓練データに対して予測\n",
    "    y_train_pred = model(X_train)\n",
    "\n",
    "    ## 勾配初期化\n",
    "    optimizer.zero_grad()\n",
    "    ## 損失の取得\n",
    "    loss = criterion(y_train_pred, y_train)\n",
    "    ## 勾配を計算\n",
    "    loss.backward()\n",
    "\n",
    "    ## パラメータ更新\n",
    "    optimizer.step()\n",
    "\n",
    "    ## 正解率の確認\n",
    "    with torch.no_grad():\n",
    "        y_train_pred_num = torch.argmax(y_train_pred, axis=1)\n",
    "        acc = (y_train_pred_num == y_train).sum()\n",
    "\n",
    "    return (loss.item(), acc.item())\n",
    "\n",
    "## 評価(validation)\n",
    "def valid_step(X_val, y_val):\n",
    "    ## 評価モード\n",
    "    model.eval()\n",
    "\n",
    "    ## 予測\n",
    "    y_val_pred = model(X_val)\n",
    "\n",
    "    ## 損失計算\n",
    "    loss = criterion(y_val_pred, y_val)\n",
    "\n",
    "    ## 正解率計算\n",
    "    with torch.no_grad():\n",
    "        y_val_pred_num = torch.argmax(y_val_pred, axis=1)\n",
    "        acc = (y_val_pred_num == y_val).sum()\n",
    "\n",
    "    return (loss.item(), acc.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch1/10: [loss:0.6057, acc:0.7083, val_loss:0.4128, val_acc:0.7917]\nepoch2/10: [loss:0.2744, acc:0.8854, val_loss:0.5539, val_acc:0.7917]\nepoch3/10: [loss:0.2076, acc:0.9062, val_loss:0.3603, val_acc:0.7917]\nepoch4/10: [loss:0.1780, acc:0.9375, val_loss:0.3715, val_acc:0.7917]\nepoch5/10: [loss:0.1537, acc:0.9479, val_loss:0.2894, val_acc:0.8333]\nepoch6/10: [loss:0.1341, acc:0.9375, val_loss:0.3411, val_acc:0.8333]\nepoch7/10: [loss:0.1063, acc:0.9792, val_loss:0.3074, val_acc:0.8750]\nepoch8/10: [loss:0.1179, acc:0.9688, val_loss:0.3837, val_acc:0.8333]\nepoch9/10: [loss:0.0980, acc:0.9375, val_loss:0.3270, val_acc:0.8750]\nepoch10/10: [loss:0.0684, acc:0.9792, val_loss:0.1587, val_acc:0.9167]\nDone!\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "OrderedDict([('layer.0.weight',\n              tensor([[-1.1076, -0.5473, -2.5131, -1.1941],\n                      [-1.0936,  1.0659, -0.3457, -0.2277],\n                      [-1.2841,  1.5103,  0.0321, -0.3944],\n                      [-0.0496, -0.9203,  0.3740,  0.5851],\n                      [ 1.1549, -0.9402,  0.3671, -0.8421],\n                      [ 0.0369, -1.2486, -0.3897, -0.0233],\n                      [ 0.8064, -0.3237, -0.4111, -0.0503],\n                      [ 0.2424, -0.4440,  1.0226, -0.8194],\n                      [-0.1386,  1.8678,  0.5962, -0.9876],\n                      [-0.6963, -0.5591, -1.3045,  0.2960],\n                      [ 0.4134, -0.7867, -0.1550,  0.5922],\n                      [-0.2110, -0.0495,  0.8068,  0.3654],\n                      [-0.3371,  1.6197,  0.7204,  0.4925],\n                      [ 0.0292, -0.8977,  0.8912,  0.0422],\n                      [-0.4211,  0.7321, -0.0048,  0.6710],\n                      [-0.3052, -1.4168,  0.5796,  0.8645],\n                      [-0.6697,  0.0270,  1.3864,  1.2381],\n                      [-0.1830, -0.3573, -0.2701,  0.7162],\n                      [-0.5441, -0.4823,  0.6689, -0.9239],\n                      [ 0.4303,  0.2104, -0.5494, -0.2352],\n                      [-0.1342, -0.4297, -0.3321, -0.4075],\n                      [ 0.1186, -0.2117,  0.5670, -0.7110],\n                      [-0.2169,  0.1263, -0.3948, -1.0826],\n                      [-1.0580,  0.2090,  0.6503, -0.2913],\n                      [-0.6280, -0.2354,  0.3898,  1.2601],\n                      [-0.0905, -0.0029,  0.2798,  0.1289],\n                      [-0.5217, -0.2500,  0.4311, -0.8568],\n                      [-0.0516, -0.2173,  0.4751,  0.3899],\n                      [-0.2244, -0.5365, -1.7452,  0.1112],\n                      [-0.1651, -0.1231, -0.2220, -0.7271],\n                      [ 0.5047, -0.7673, -0.3459,  0.2025],\n                      [-0.9045,  0.3369, -0.1997,  0.0448],\n                      [ 0.2277,  0.6536, -0.9422, -0.3556],\n                      [-2.3350, -0.5971, -0.2194, -0.7677],\n                      [ 0.6228,  0.2260,  0.2288, -0.4063],\n                      [ 0.5409, -0.5935, -0.4529, -0.2581],\n                      [ 0.1063, -0.5003, -0.3697, -0.7426],\n                      [ 0.5230,  0.0891, -1.8309, -0.4678],\n                      [ 0.7101, -0.3058,  0.4443,  0.3287],\n                      [-0.2492, -0.3444,  0.3937, -0.4554],\n                      [ 0.1824,  0.7813, -0.0550, -0.7088],\n                      [ 1.3137, -0.6740,  0.1229,  0.4610],\n                      [-0.3950, -0.2226,  0.2145, -0.0382],\n                      [ 0.8273, -0.1006,  0.0760, -1.4204],\n                      [ 0.1013, -0.0423, -0.7559,  0.3283],\n                      [-0.9104, -0.1442, -0.1911,  0.1219],\n                      [ 0.3413, -0.9850,  0.1533,  0.5717],\n                      [ 0.6821,  0.3497,  0.5294,  0.3455],\n                      [-0.0086, -0.4395,  0.5715,  0.6867],\n                      [-1.0775, -0.1618, -0.2416, -0.6266],\n                      [ 0.3904,  0.4326,  0.2296,  0.5733],\n                      [-0.0859,  0.0825,  0.0072, -0.2645],\n                      [ 0.2565, -0.0525,  0.8228,  0.8093],\n                      [ 0.3150, -0.2751,  1.7060, -0.4010],\n                      [ 0.4549, -0.9218, -0.3454,  0.5665],\n                      [-0.3027, -0.0541,  0.9016,  0.4924],\n                      [-0.8479,  0.6141, -0.7109,  0.7122],\n                      [ 0.6511, -0.5129, -0.3988,  0.2097],\n                      [-0.2662,  0.0202,  0.5762,  0.2572],\n                      [-0.3852,  0.0360,  0.6223,  0.1725],\n                      [-0.5283, -1.3412,  0.2793, -0.2138],\n                      [-0.6086,  0.6441, -0.2150, -0.8755],\n                      [-0.4416,  0.2167,  0.8331,  1.4022],\n                      [ 0.3669, -1.1309,  0.6949, -0.3104],\n                      [-0.0482, -0.0170, -0.8013,  1.0397],\n                      [ 0.6150, -0.2339, -0.4417,  0.1551],\n                      [-0.5792,  0.8475,  1.1999, -0.1349],\n                      [-0.1596,  1.0997, -0.7427,  0.7993],\n                      [-0.1383, -1.1638, -0.6847, -0.3701],\n                      [ 0.9909,  1.8081, -0.8377, -1.3058],\n                      [ 0.0170, -0.2472,  0.3982,  0.3193],\n                      [ 0.7965,  0.9157, -0.1068,  0.1934],\n                      [-1.2908, -0.6056, -1.1937, -0.4218],\n                      [-0.3884, -0.3984, -0.5672, -1.3030],\n                      [ 0.8942,  0.6849,  0.7135, -0.8450],\n                      [ 0.4294,  0.3511, -0.0622,  0.3299],\n                      [ 0.8863, -0.5880, -0.7881, -0.2393],\n                      [-0.1606, -0.5289,  1.4866, -0.8440],\n                      [-0.7425, -1.1317,  0.5972, -0.0132],\n                      [-0.1036, -0.1894,  0.9739,  0.9270],\n                      [ 1.0606,  0.6445, -1.1271,  0.9050],\n                      [ 1.2032,  0.4310, -0.1378, -0.1033],\n                      [-0.4107, -0.5199, -0.6293,  0.9188],\n                      [ 0.5651, -0.4926, -1.1977, -0.8830],\n                      [ 0.4927, -0.5416,  0.1359, -1.7063],\n                      [ 0.1116, -0.3486, -1.1760,  0.0524],\n                      [ 0.4975,  1.2086,  0.2783, -1.9339],\n                      [-0.2226, -0.2958, -1.5634, -0.5092],\n                      [ 0.5518, -0.5712, -0.4957, -0.4929],\n                      [-0.3667,  0.1062, -0.3101, -0.3191],\n                      [ 0.3679,  0.8167, -0.5937,  0.6487],\n                      [-0.8761,  0.0049, -0.1259,  0.1254],\n                      [-0.6443, -0.9470,  0.6796, -0.5423],\n                      [-0.6983, -1.4410, -0.0942, -1.5660],\n                      [ 0.1152, -0.9406,  0.3944, -0.5977],\n                      [-0.2338, -0.5608, -0.8401, -0.1346],\n                      [-0.7276, -1.3840,  0.5677, -0.8600],\n                      [ 0.1762, -0.3871, -0.2457, -0.6483],\n                      [-0.4313, -0.2615,  0.5996, -0.0799],\n                      [ 0.3992, -0.4261,  0.3129,  1.8874]])),\n             ('layer.0.bias',\n              tensor([-0.1491,  0.3257,  0.2780, -0.2862, -0.0353, -0.0266,  0.1240,  0.4063,\n                      -0.1109,  0.1211,  0.2251, -0.2636,  0.1056,  0.1189, -0.0024, -0.2223,\n                      -0.1440, -0.1852,  0.2787,  0.0023, -0.0609,  0.3014,  0.1092,  0.2100,\n                       0.0926, -0.1317,  0.2575, -0.2668,  0.1268,  0.1491, -0.0943, -0.0723,\n                       0.0346, -0.1236,  0.0573,  0.2634, -0.1646, -0.0181, -0.2415, -0.0047,\n                       0.0469,  0.2233,  0.0177,  0.3100, -0.0552, -0.0581, -0.0888,  0.0696,\n                      -0.3107, -0.0126, -0.1039,  0.4050,  0.1520, -0.1132,  0.1671, -0.2458,\n                       0.1340,  0.1842, -0.0754, -0.0356,  0.2423, -0.0098, -0.2371,  0.3227,\n                      -0.0785, -0.0329,  0.2163, -0.0306,  0.1047,  0.1009, -0.2759,  0.0866,\n                       0.1544,  0.1549,  0.0929,  0.0892,  0.3371,  0.1637, -0.2872, -0.2529,\n                      -0.0800, -0.0473, -0.1725,  0.1372, -0.1374,  0.0905, -0.0008,  0.0183,\n                      -0.0189, -0.0824,  0.0161,  0.1389,  0.1367,  0.2387, -0.0799,  0.0275,\n                       0.2883, -0.0178,  0.0021, -0.3366])),\n             ('layer.2.weight',\n              tensor([[-9.5682e-03, -8.3343e-03, -1.3495e-02,  1.5626e-01,  3.5197e-02,\n                       -1.9727e-02,  1.7415e-01,  3.9285e-01,  7.6694e-02, -2.6623e-01,\n                        4.3248e-01,  1.8064e-01,  9.4128e-03,  1.1125e-01, -2.6914e-04,\n                        2.1172e-01,  5.6037e-01,  2.7105e-01, -5.9598e-02, -4.8151e-02,\n                       -1.0661e-01,  1.1582e-01, -1.8175e-01,  2.2547e-02,  3.7784e-01,\n                        3.0728e-01,  2.3500e-01,  1.8496e-01,  4.3462e-02, -4.2687e-02,\n                        8.6918e-02,  5.6455e-02, -7.9120e-02,  2.3482e-02,  2.3015e-02,\n                       -3.9522e-02, -4.0200e-02,  4.4719e-02,  3.2234e-02,  2.1171e-02,\n                        7.9530e-02,  1.6363e-01,  2.2464e-01, -5.5303e-02,  6.4749e-02,\n                        1.5002e-01,  1.2812e-01,  3.4997e-01,  3.7460e-01,  1.6193e-01,\n                        1.0114e-01, -9.8916e-02,  2.0827e-01,  1.6258e-01,  4.5552e-01,\n                        2.6200e-01,  1.0904e-01,  1.5649e-01, -2.5976e-02,  2.2880e-01,\n                        2.0146e-01,  1.7453e-01,  9.1739e-02,  3.5601e-01,  3.6330e-01,\n                       -4.8485e-03,  4.0759e-01,  2.9977e-02,  1.3430e-01, -1.2671e-01,\n                        8.0028e-02, -2.4459e-02, -1.3671e-01, -1.2861e-02,  3.1467e-01,\n                        4.4967e-02, -2.1534e-01,  2.6698e-01, -7.3480e-02,  2.2613e-01,\n                        1.1167e-01,  1.6106e-01,  1.6500e-01, -1.6553e-01,  5.1648e-02,\n                       -8.8216e-02,  1.2252e-01, -1.0194e-01, -1.1822e-01, -9.5401e-02,\n                       -5.1358e-02,  1.9141e-01,  2.1797e-01, -1.4114e-01,  9.4227e-02,\n                        6.4517e-02,  1.8194e-01, -5.8981e-02,  1.9442e-01,  1.1739e-01],\n                      [ 2.8970e-01,  5.4780e-02,  1.6681e-01, -3.6838e-01, -7.9239e-02,\n                       -5.1861e-03, -1.3375e-01, -3.0558e-01,  2.1566e-01, -7.4217e-02,\n                       -1.9844e-01, -3.4229e-01,  1.2268e-01, -1.2730e-01,  1.2852e-01,\n                       -6.0597e-02,  5.8551e-02, -6.8463e-02,  1.6243e-01, -1.1062e-01,\n                       -2.0888e-01, -1.1527e-01,  1.1753e-01,  1.1309e-01, -2.5822e-01,\n                        1.6858e-01,  1.4917e-01,  2.2154e-01,  4.7695e-02,  4.9341e-02,\n                       -1.2025e-01,  1.4235e-01,  2.5264e-01,  5.4832e-02,  1.8335e-01,\n                       -1.5809e-01,  2.6277e-01,  2.0093e-02, -8.3362e-02,  7.6539e-02,\n                        2.4301e-01,  5.8541e-02, -1.7124e-01, -1.8726e-01,  1.7043e-01,\n                        2.9296e-01, -6.9804e-02, -1.2315e-01,  1.5122e-01,  5.4111e-02,\n                        3.5001e-02,  7.0325e-02,  1.9452e-02, -2.0578e-01,  2.2575e-01,\n                        6.2022e-03, -1.0603e-01, -2.6970e-01, -2.0713e-02, -2.5809e-01,\n                       -1.3019e-01, -8.5218e-02,  2.2019e-01, -6.7639e-02, -2.6959e-02,\n                        3.5232e-02, -1.5017e-01, -1.8953e-01, -9.4059e-02,  3.2308e-01,\n                        3.4383e-02, -3.0903e-01,  1.2133e-01,  1.3831e-01,  3.6458e-01,\n                       -5.0074e-02, -8.9075e-02, -2.8196e-01, -9.3619e-03,  3.1979e-03,\n                        5.4941e-02,  1.9637e-01, -4.3410e-02,  1.1238e-01,  6.0711e-02,\n                       -1.1661e-01,  2.1863e-01,  3.0992e-01,  6.0898e-02,  3.1849e-01,\n                        8.3554e-02, -4.8422e-02,  8.2789e-02,  3.2577e-02, -1.6279e-02,\n                       -1.3480e-01,  1.0173e-01,  3.2682e-01, -1.3943e-01,  3.5612e-02],\n                      [-4.4026e-02, -2.8301e-01, -4.8161e-01,  1.3019e-01,  1.7926e-02,\n                        2.6711e-01, -2.0504e-01, -1.2658e-02, -3.9162e-02,  5.9747e-03,\n                       -4.1559e-02,  2.4077e-01,  1.9263e-01, -2.3297e-01,  7.6670e-02,\n                        4.0781e-01,  2.7125e-01,  1.4528e-01,  4.4121e-02, -9.4257e-02,\n                        5.5521e-02,  2.2065e-02, -4.2635e-01, -1.3730e-03,  3.3195e-01,\n                        2.0679e-01, -2.2242e-02, -2.8024e-02, -1.1896e-01, -2.3748e-01,\n                       -1.2417e-01, -1.2890e-01, -1.1246e-01,  9.6841e-02,  3.4429e-01,\n                        3.4686e-02,  3.2172e-02, -2.9666e-01,  5.5363e-02,  1.4640e-01,\n                        1.0061e-01, -1.9228e-01,  2.4131e-01, -2.3944e-01,  4.3059e-02,\n                        1.8005e-02,  1.9033e-01,  4.5309e-02,  3.8706e-01,  3.8719e-02,\n                        2.1296e-01, -6.0822e-02,  2.3563e-01,  2.7119e-01,  1.4998e-01,\n                        4.1408e-01,  6.2164e-03,  1.5723e-01,  2.6409e-01,  6.1267e-02,\n                        5.9035e-02, -1.7039e-01,  3.5666e-01, -7.4775e-02,  3.2835e-01,\n                        2.8503e-02,  9.3283e-02,  2.2174e-01,  2.1426e-01, -5.4100e-02,\n                        3.7120e-01, -1.6368e-01, -1.1919e-02,  2.4428e-01, -8.1195e-02,\n                       -2.0065e-01, -1.6702e-01,  2.1504e-01, -5.2660e-02,  9.6719e-02,\n                       -7.5825e-02, -2.9020e-01,  1.5964e-01, -1.2052e-01, -4.7588e-02,\n                       -4.1485e-01, -1.5346e-01,  4.2391e-02, -1.1982e-01, -9.5054e-02,\n                        1.3795e-01, -2.9426e-02,  7.6301e-02,  7.4098e-02,  1.8674e-01,\n                        3.9112e-02, -8.8191e-02, -1.4769e-01,  2.4295e-01, -6.4162e-02],\n                      [ 1.3595e-03, -5.4246e-02, -1.7904e-01,  1.1826e-01,  1.7608e-01,\n                        1.1439e-01, -2.1095e-02,  1.2573e-01, -5.5400e-02,  2.4877e-01,\n                        1.2418e-03, -3.9051e-02, -1.0828e-01, -1.8150e-01,  1.2487e-01,\n                       -2.0165e-01,  6.2238e-02, -2.2816e-01, -2.1980e-01, -4.9301e-02,\n                       -2.1872e-01,  1.6256e-01, -9.9538e-02, -1.9926e-01,  1.4005e-01,\n                        5.8324e-02, -2.5612e-02,  1.4229e-02,  5.0730e-02,  7.9373e-02,\n                        2.7069e-02,  3.0176e-01, -1.1583e-01, -8.5565e-02, -1.8936e-01,\n                        3.9956e-02,  2.5516e-01,  2.8016e-01,  1.9015e-01, -1.2589e-02,\n                        6.4441e-02, -2.2513e-01,  7.6643e-02, -2.1379e-01,  1.2654e-01,\n                       -1.9034e-01, -1.8133e-01, -2.8807e-01,  1.8603e-01,  3.0247e-01,\n                       -2.1898e-02,  5.4642e-02, -4.8293e-02, -2.4765e-01, -4.9999e-02,\n                        1.3977e-01, -9.5785e-02, -1.0923e-01, -6.5857e-02, -1.1170e-01,\n                       -1.4694e-01,  4.7831e-02, -2.6767e-01,  1.5826e-01, -1.5238e-01,\n                        1.0940e-01,  1.5603e-01, -1.6287e-01,  6.7092e-02,  1.2252e-01,\n                       -1.8789e-01, -2.2495e-01,  6.6180e-02, -1.2660e-01,  3.1984e-02,\n                        2.0146e-01, -7.7892e-02,  2.4025e-01,  1.4326e-01, -2.0331e-01,\n                       -1.6104e-01, -5.2672e-02,  7.1995e-02, -2.9134e-01,  3.5555e-02,\n                        1.5152e-01, -1.6590e-01, -1.1707e-01,  3.0641e-02,  2.8915e-02,\n                        7.3203e-02,  1.7598e-01, -3.9146e-01, -3.9533e-02, -9.5245e-02,\n                       -4.0701e-02, -7.1691e-02, -6.6541e-02, -1.2813e-01, -1.6481e-01],\n                      [ 3.2271e-01,  1.4038e-01,  2.3893e-01, -3.3073e-01, -3.0480e-02,\n                       -4.5860e-02, -2.3013e-02,  4.0489e-01,  6.6138e-02,  2.2803e-01,\n                        1.0783e-01, -2.6276e-01,  1.7902e-01, -1.3490e-01, -2.6544e-02,\n                       -2.8823e-01,  6.4997e-02, -1.4593e-01,  2.0723e-02,  1.0294e-01,\n                        5.7212e-02,  1.8172e-01,  1.3845e-01,  2.2914e-02,  1.0746e-01,\n                       -9.7202e-02,  2.6561e-01, -2.8120e-01,  1.4876e-02, -3.6015e-02,\n                       -8.3735e-02,  2.2568e-02, -2.1854e-02,  1.3717e-01,  1.2093e-01,\n                        2.8600e-01,  2.2102e-01,  2.9647e-01, -2.3880e-01, -8.9382e-02,\n                        7.2793e-02,  2.8545e-01, -6.0279e-02,  5.6137e-01, -3.8294e-02,\n                        1.6496e-01, -1.2341e-01,  2.6095e-01, -1.3371e-01,  2.7764e-01,\n                        3.8733e-02,  4.8905e-01, -5.6297e-02,  5.0143e-02,  1.3114e-01,\n                       -1.5608e-01,  1.5732e-01,  1.1397e-01, -1.8848e-01,  1.9052e-01,\n                        1.2512e-01,  1.4631e-01, -3.0390e-01,  2.6871e-01, -1.1065e-01,\n                        6.2018e-02,  2.6806e-01, -2.2639e-01,  4.4607e-01,  4.9921e-02,\n                       -3.7972e-02,  9.8839e-02,  4.2189e-02,  1.3681e-01,  9.9190e-02,\n                        2.6444e-01,  5.2275e-01,  1.6630e-02, -3.1179e-01, -8.8439e-02,\n                        2.1800e-02,  6.4992e-02, -9.9421e-03,  4.7880e-01,  1.4072e-01,\n                       -1.0993e-01,  2.8423e-02,  2.4653e-01,  1.0799e-01,  2.6430e-02,\n                        1.5260e-01,  2.3852e-01, -2.2945e-02,  2.3795e-02,  1.7438e-01,\n                        8.3767e-02,  5.2080e-02, -6.6589e-02,  4.1434e-02, -8.4294e-02],\n                      [-2.7370e-01, -1.9044e-01, -5.0114e-01,  2.1409e-02, -2.8669e-01,\n                       -3.9308e-02, -2.3914e-01, -2.0386e-01, -4.6089e-04, -8.5528e-02,\n                       -8.0557e-02,  1.0881e-01,  1.2775e-01, -3.2651e-02, -4.0957e-03,\n                        7.8448e-04,  1.3745e-01,  3.2722e-01, -2.2385e-01, -3.5039e-01,\n                       -5.1919e-02, -1.4034e-01,  3.0884e-02, -2.3851e-02,  7.4292e-02,\n                        1.6359e-01, -1.7837e-01,  6.6646e-01,  6.9965e-02, -1.4563e-01,\n                       -1.2949e-01,  4.2105e-02, -1.3527e-01,  2.3951e-01, -3.3698e-01,\n                       -1.3923e-01,  1.0415e-02, -3.0220e-02,  1.1952e-01, -1.0163e-02,\n                       -1.7685e-01,  5.0126e-02,  1.4001e-01,  1.4692e-01,  3.9665e-04,\n                        1.1495e-01, -3.0760e-02,  2.9815e-01, -4.1719e-02,  3.5173e-02,\n                        8.0716e-02, -2.8294e-01, -1.6846e-01,  2.6673e-01,  4.3633e-02,\n                        2.8317e-01, -6.7106e-02, -1.1367e-01, -5.6383e-02,  1.7068e-01,\n                        4.2931e-02, -1.8808e-01,  2.8137e-01,  6.7862e-03,  2.3799e-01,\n                       -7.7559e-02,  8.1848e-02,  1.3786e-01, -3.8114e-01, -2.9089e-02,\n                        3.1384e-01, -5.6899e-02,  2.9420e-02, -2.8364e-01, -1.9722e-01,\n                        1.0685e-01, -2.0378e-01,  3.5456e-02,  1.1696e-01,  2.3812e-01,\n                        1.5847e-01,  3.2450e-01,  2.4283e-01, -3.8750e-01, -1.2209e-01,\n                       -1.9160e-01, -8.5447e-02, -1.2449e-01, -7.1904e-02,  1.9330e-01,\n                       -3.6432e-02, -3.5626e-02, -1.2708e-01, -1.0595e-01,  1.6362e-01,\n                        1.3702e-01,  3.9726e-04, -1.8442e-01,  4.1180e-02,  8.1723e-02],\n                      [ 8.4952e-02,  3.5765e-01,  5.0615e-01,  3.0376e-02, -2.2787e-01,\n                       -1.3330e-01, -2.1006e-01,  5.0557e-02, -1.7287e-01,  1.7002e-01,\n                       -1.7303e-01,  4.2174e-02,  1.9470e-01, -2.0074e-01,  2.7396e-01,\n                       -3.3308e-02,  8.0942e-02,  1.4174e-01,  1.6878e-01, -6.7674e-02,\n                       -1.3579e-01,  3.0493e-01,  3.2416e-01,  1.7226e-01,  7.2447e-02,\n                        3.1308e-02,  1.2931e-01, -3.5056e-02, -5.4178e-02,  2.9318e-01,\n                        8.4078e-02, -7.0947e-02,  7.1612e-03,  1.5706e-01,  2.8731e-02,\n                        1.4717e-01, -2.6067e-02,  7.1671e-02, -1.1480e-01,  2.1716e-01,\n                        1.2455e-01,  1.3705e-01,  1.1589e-02,  2.9239e-01,  3.1011e-01,\n                        1.3256e-02,  9.6588e-03,  1.6333e-01,  8.8988e-02,  1.9316e-01,\n                        1.0629e-02,  3.4743e-01, -1.4678e-01, -7.8132e-03, -1.8245e-01,\n                        4.8504e-02,  5.6991e-02, -2.6748e-02,  1.6127e-01, -5.4184e-02,\n                        9.7203e-02, -1.6422e-01, -1.5026e-01, -7.4986e-02, -3.1084e-01,\n                       -5.7718e-02,  2.5072e-01,  1.8722e-01, -3.2370e-02,  4.6061e-02,\n                       -2.4728e-01, -1.1622e-01,  1.9257e-02,  3.1159e-01, -1.6847e-01,\n                       -1.1521e-01, -1.4749e-01,  2.0329e-01, -4.0715e-02,  4.8391e-02,\n                       -3.1501e-02,  5.7051e-03, -1.9697e-01,  3.1071e-01, -1.7556e-01,\n                       -1.1896e-01, -8.3278e-02, -1.4589e-02,  7.3506e-02,  2.8502e-01,\n                        1.5449e-01, -1.9301e-01,  1.1207e-01, -5.0903e-02, -1.6719e-01,\n                        2.0147e-02,  2.1590e-02, -6.1226e-02,  7.7747e-02, -2.3362e-01],\n                      [-3.2406e-01,  1.0389e-01,  1.7167e-01, -2.4073e-01, -8.1120e-02,\n                        9.8230e-02,  1.2320e-01,  2.6939e-03, -1.4412e-01,  1.3479e-02,\n                       -1.9636e-02, -3.9453e-01,  3.5557e-01,  1.9643e-01,  2.3034e-01,\n                        7.2320e-02, -3.0684e-01, -1.8824e-01,  3.0573e-01, -6.7432e-02,\n                       -1.0471e-01,  8.0760e-02, -1.1871e-02,  2.8015e-01, -2.0400e-02,\n                       -8.0868e-03,  8.4046e-03, -6.5295e-01,  2.4791e-01,  2.1691e-01,\n                       -2.0697e-01,  4.6604e-02,  2.6860e-02, -9.7279e-02, -1.5692e-02,\n                       -1.5907e-02, -2.5823e-01, -1.7340e-01, -3.1298e-01,  8.0630e-03,\n                        1.1684e-01,  1.2219e-01,  1.2245e-01,  3.3195e-01, -1.2127e-01,\n                       -3.1885e-02,  1.1289e-01,  6.5923e-02, -2.4916e-01, -1.0383e-01,\n                       -1.1361e-01,  2.2103e-01,  2.2718e-01,  3.8142e-03,  2.3891e-01,\n                       -1.6524e-01,  1.3844e-01,  3.7020e-02,  1.2558e-01, -1.4459e-01,\n                        1.5821e-01,  3.8308e-02, -1.9260e-01,  2.5563e-01, -8.8723e-02,\n                        5.4880e-03,  2.5842e-01, -2.1486e-01,  1.3289e-01,  2.2376e-01,\n                       -5.1247e-01,  2.9250e-02,  1.7160e-01,  4.1743e-02, -6.8470e-02,\n                       -8.7306e-02,  2.0462e-01,  1.9282e-02,  1.7008e-02, -8.5098e-02,\n                        1.1877e-01,  9.2137e-02, -1.1190e-01, -3.9372e-02, -1.0111e-01,\n                        2.8088e-01,  1.0983e-01,  7.2851e-03,  1.3635e-02, -3.4712e-02,\n                       -1.6673e-02,  4.1376e-01,  4.2464e-02,  1.4761e-01, -1.0419e-01,\n                        2.0873e-02,  3.2214e-01,  2.8404e-02,  2.1106e-02, -2.6099e-01],\n                      [ 2.1440e-01, -3.7220e-02, -1.0112e-01, -1.1396e-01,  1.4796e-01,\n                       -9.0336e-03, -8.7883e-02, -5.7656e-02,  7.1034e-02,  3.7246e-03,\n                       -2.0262e-02, -7.7452e-02,  1.3380e-01, -2.0321e-01,  3.3346e-01,\n                       -2.4691e-02, -1.9009e-01,  6.6951e-02, -1.5719e-01,  2.2269e-01,\n                        2.7241e-01, -1.1729e-01, -2.1848e-01, -4.0759e-01, -7.8680e-02,\n                        1.0574e-01,  7.2273e-02,  2.2251e-01, -8.3787e-02, -2.2389e-01,\n                       -1.9912e-01, -5.4458e-02,  1.9706e-01, -3.6311e-02,  9.4362e-02,\n                        6.8675e-02,  3.1331e-01,  1.6522e-01,  1.7917e-01, -6.8684e-02,\n                       -8.4816e-02, -1.5429e-01,  3.4736e-02,  6.0918e-02,  1.6182e-01,\n                        2.7498e-01, -5.8944e-02, -8.8309e-02,  1.6793e-01,  3.4578e-01,\n                        3.1747e-02,  1.7168e-01, -7.3722e-02,  8.0609e-02, -7.0761e-02,\n                        1.6073e-01, -4.6351e-02, -1.3643e-01,  1.7300e-01, -4.3976e-02,\n                        1.0169e-01,  4.6755e-01,  6.6143e-02,  2.6085e-02, -3.9731e-02,\n                       -2.0468e-01,  1.1755e-01,  2.0081e-01, -2.3818e-02, -8.9160e-02,\n                        1.1081e-02, -1.9819e-01,  2.1917e-02,  6.5812e-02, -3.3573e-01,\n                       -1.0151e-03, -2.8494e-01, -2.3757e-01,  5.2432e-03, -4.8358e-02,\n                       -9.9365e-02, -8.5102e-02,  1.7978e-01, -1.1160e-01,  7.6833e-02,\n                        2.3694e-01,  2.2137e-01,  3.2372e-01, -5.1774e-03,  1.5293e-01,\n                        1.3532e-01, -1.6034e-02, -5.5088e-02, -1.2653e-01, -2.3486e-02,\n                        6.4615e-02, -7.2757e-02,  7.0692e-03,  8.0196e-02,  1.2334e-01],\n                      [-1.3150e-01, -3.7500e-01, -9.0391e-02,  2.8621e-03, -1.2384e-01,\n                       -1.5108e-01, -1.3334e-01, -1.5726e-01, -1.9018e-01, -1.7974e-02,\n                       -8.3336e-02, -1.1089e-01,  5.6810e-02,  1.4420e-01, -1.3202e-01,\n                       -1.7676e-01, -5.9072e-02, -2.3732e-01,  1.1355e-01, -1.7296e-01,\n                        3.1512e-04, -7.5606e-02, -4.9202e-03,  6.4720e-03, -1.8192e-01,\n                       -1.4407e-01,  1.5487e-01,  2.6020e-01, -4.1399e-02,  7.0994e-02,\n                       -4.5338e-02, -2.3898e-01, -5.8467e-02, -1.0862e-01,  9.0669e-02,\n                       -1.4804e-01,  9.8770e-02,  1.1308e-01,  5.6210e-02, -1.2505e-01,\n                        4.3502e-02, -2.3500e-01,  1.6609e-02, -5.5834e-03,  1.6862e-01,\n                        1.6467e-01,  5.0075e-02, -7.0372e-02,  2.0221e-02,  1.1291e-01,\n                        2.8789e-01, -1.1777e-01, -6.4039e-02,  8.7511e-02, -3.1489e-01,\n                       -3.2842e-02, -2.0160e-01,  1.3374e-01,  7.3937e-02,  1.4098e-01,\n                        4.7210e-02, -2.4173e-01,  9.9088e-02,  1.1386e-01, -1.1622e-01,\n                       -3.9606e-01, -2.8511e-02, -1.8129e-01, -2.1287e-02,  1.2316e-01,\n                       -3.7876e-02,  9.7432e-02,  4.0323e-02, -4.7977e-02,  9.1515e-02,\n                       -8.8059e-03, -1.1065e-01, -3.0930e-03,  1.0924e-01, -3.6364e-01,\n                       -1.9253e-01,  1.3135e-02,  5.8793e-03,  2.8116e-01, -1.4308e-01,\n                       -2.3828e-01, -1.1344e-01, -4.5269e-02, -1.3051e-01,  1.2817e-01,\n                        1.4069e-01, -1.0337e-01,  9.2611e-02,  4.4239e-02, -9.4434e-02,\n                        1.2204e-01,  6.4549e-02, -5.4417e-02, -2.3268e-01, -5.6743e-02]])),\n             ('layer.2.bias',\n              tensor([ 0.1129, -0.0833, -0.1298, -0.0353,  0.3814, -0.1623,  0.1592,  0.2660,\n                      -0.0680, -0.0355])),\n             ('layer.4.weight',\n              tensor([[ 0.5316, -0.4122,  0.3145, -0.3607,  0.1501,  0.2083,  0.6012,  0.6767,\n                       -1.4046, -0.0067],\n                      [ 0.6051, -0.0841,  0.6481,  0.2857, -0.5676,  1.2966, -0.2701, -0.0762,\n                        0.3766,  0.5626],\n                      [-0.3601,  0.2446, -0.1116, -0.6776, -0.0481, -0.2266,  1.1304, -0.0782,\n                        0.1398, -0.0223]])),\n             ('layer.4.bias', tensor([ 0.2651, -0.1899, -0.1945]))])"
     },
     "metadata": {}
    }
   ],
   "source": [
    "## パラメータ初期化関数\n",
    "def initializer(layer):\n",
    "    if type(layer) == nn.Linear:\n",
    "        nn.init.kaiming_normal_(layer.weight)\n",
    "        layer.bias.data.fill_(0.0)\n",
    "\n",
    "## パラメータ初期化する\n",
    "model.apply(initializer)\n",
    "\n",
    "n_epoch = 10\n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "# 1エポック内1データあたりの損失・正解率の平均\n",
    "avg_loss = 0.0\n",
    "avg_acc = 0.0\n",
    "avg_val_loss = 0.0\n",
    "avg_val_acc = 0.0\n",
    "\n",
    "## 学習する\n",
    "for epoch in range(n_epoch):\n",
    "    ## epoch内のloss, accの保存\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    total_val_loss = 0.0\n",
    "    total_val_acc = 0.0\n",
    "    ## 累計データ数の保存\n",
    "    total_train = 0\n",
    "    total_val = 0\n",
    "\n",
    "    ## 訓練\n",
    "    for X_train, y_train in loader_train:\n",
    "       loss, acc = train_step(X_train, y_train)\n",
    "       total_loss += loss\n",
    "       total_acc += acc\n",
    "       total_train += len(y_train)\n",
    "\n",
    "    ## 評価\n",
    "    for X_val, y_val in loader_valid:\n",
    "        val_loss, val_acc = valid_step(X_val, y_val)\n",
    "        total_val_loss += val_loss\n",
    "        total_val_acc += val_acc\n",
    "        total_val += len(y_val)\n",
    "\n",
    "    # 平均計算\n",
    "    avg_loss = total_loss/total_train\n",
    "    avg_val_loss = total_val_loss/total_val\n",
    "    avg_acc = total_acc/total_train\n",
    "    avg_val_acc = total_val_acc/total_val\n",
    "\n",
    "    print(f'epoch{epoch+1}/{n_epoch}: [loss:{avg_loss:.4f}, acc:{avg_acc:.4f}, val_loss:{avg_val_loss:.4f}, val_acc:{avg_val_acc:.4f}]')\n",
    "\n",
    "    # リストに追加\n",
    "    train_history.append(avg_loss)\n",
    "    val_history.append(avg_val_loss)\n",
    "\n",
    "print('Done!')\n",
    "display(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "y_pred_proba tensor([[ 8.8669, 16.0525, -5.6637],\n        [ 9.1296,  0.9972, -2.7766],\n        [ 0.2960, -7.7022, 11.5775],\n        [ 7.9015,  9.3087, -5.0144],\n        [-0.9783, -8.1259, 10.5113],\n        [ 8.5634, 17.2853, -5.0567],\n        [-1.5081, -7.8409, 10.8348],\n        [ 3.7218,  0.2999, -1.8674],\n        [ 4.9030,  1.1658, -2.5326],\n        [ 3.7552, -0.0425, -1.4140],\n        [ 7.9741,  7.4858, -3.8038],\n        [ 3.3341,  0.6245, -1.0894],\n        [ 5.2571,  1.3996, -1.6078],\n        [ 4.4762,  2.0847, -2.3805],\n        [ 4.2931,  2.2187, -1.6832],\n        [ 0.1225, -9.1684, 10.3560],\n        [ 3.8073,  2.7471, -1.7633],\n        [ 7.3845,  2.8001, -1.3620],\n        [ 0.2480, -8.7702,  9.7242],\n        [-0.7660, -7.2338, 10.6860],\n        [ 6.8963, 11.7918, -4.3810],\n        [ 4.7444,  2.4002, -0.4108],\n        [ 0.2447, -8.1074, 10.7448],\n        [ 0.9963, -9.7264, 10.9862],\n        [ 5.2220,  7.4732, -3.3119],\n        [-1.6683, -9.0754, 13.2181],\n        [ 0.4474, -6.8421, 10.4896],\n        [ 3.4734,  0.0276, -1.3389],\n        [ 8.7885, -0.4477,  0.2944],\n        [-0.8836, -7.2856,  9.8036]], grad_fn=<AddmmBackward>)\ny_pred [1 0 2 1 2 1 2 0 0 0 0 0 0 0 0 2 0 0 2 2 1 0 2 2 1 2 2 0 0 2]\ny_test [1 0 2 1 2 1 2 0 0 0 1 0 0 0 0 2 0 0 2 2 1 0 2 2 1 2 2 0 0 2]\nacc:  0.9666666666666667\n"
    }
   ],
   "source": [
    "# 予測\n",
    "## Tensor変換\n",
    "t_X_test = torch.from_numpy(X_test).float()\n",
    "y_pred_proba = model(t_X_test)\n",
    "y_pred = torch.argmax(y_pred_proba, axis=1)\n",
    "print(\"y_pred_proba\", y_pred_proba)\n",
    "print(\"y_pred\", y_pred.flatten().int().detach().numpy())\n",
    "print(\"y_test\", y_test.flatten())\n",
    "print(\"acc: \", accuracy_score(y_test.flatten(),  y_pred.flatten().int().detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 372.103125 248.518125\" width=\"372.103125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.518125 \r\nL 372.103125 248.518125 \r\nL 372.103125 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 30.103125 224.64 \r\nL 364.903125 224.64 \r\nL 364.903125 7.2 \r\nL 30.103125 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"mc1d2d0dd2b\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"45.321307\" xlink:href=\"#mc1d2d0dd2b\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n      </defs>\r\n      <g transform=\"translate(42.140057 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"112.95767\" xlink:href=\"#mc1d2d0dd2b\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 2 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      </defs>\r\n      <g transform=\"translate(109.77642 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"180.594034\" xlink:href=\"#mc1d2d0dd2b\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 4 -->\r\n      <defs>\r\n       <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n      </defs>\r\n      <g transform=\"translate(177.412784 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"248.230398\" xlink:href=\"#mc1d2d0dd2b\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 6 -->\r\n      <defs>\r\n       <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n      </defs>\r\n      <g transform=\"translate(245.049148 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"315.866761\" xlink:href=\"#mc1d2d0dd2b\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 8 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n      </defs>\r\n      <g transform=\"translate(312.685511 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-56\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_6\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m897e389e6e\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m897e389e6e\" y=\"203.124669\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 0.1 -->\r\n      <defs>\r\n       <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 206.923888)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-49\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m897e389e6e\" y=\"166.339372\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0.2 -->\r\n      <g transform=\"translate(7.2 170.13859)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m897e389e6e\" y=\"129.554074\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0.3 -->\r\n      <defs>\r\n       <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 133.353293)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-51\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m897e389e6e\" y=\"92.768776\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 0.4 -->\r\n      <g transform=\"translate(7.2 96.567995)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m897e389e6e\" y=\"55.983479\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 0.5 -->\r\n      <defs>\r\n       <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 59.782697)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m897e389e6e\" y=\"19.198181\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 0.6 -->\r\n      <g transform=\"translate(7.2 22.9974)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_12\">\r\n    <path clip-path=\"url(#p6795bdb859)\" d=\"M 45.321307 17.083636 \r\nL 79.139489 138.956034 \r\nL 112.95767 163.542413 \r\nL 146.775852 174.43404 \r\nL 180.594034 183.366849 \r\nL 214.412216 190.58416 \r\nL 248.230398 200.812548 \r\nL 282.04858 196.530933 \r\nL 315.866761 203.845444 \r\nL 349.684943 214.756364 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_13\">\r\n    <path clip-path=\"url(#p6795bdb859)\" d=\"M 45.321307 88.053486 \r\nL 79.139489 36.170861 \r\nL 112.95767 107.379694 \r\nL 146.775852 103.249407 \r\nL 180.594034 133.454955 \r\nL 214.412216 114.427382 \r\nL 248.230398 126.822826 \r\nL 282.04858 98.76154 \r\nL 315.866761 119.606619 \r\nL 349.684943 181.540502 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 30.103125 224.64 \r\nL 30.103125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 364.903125 224.64 \r\nL 364.903125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 30.103125 224.64 \r\nL 364.903125 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 30.103125 7.2 \r\nL 364.903125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"legend_1\">\r\n    <g id=\"patch_7\">\r\n     <path d=\"M 286.7625 44.834375 \r\nL 357.903125 44.834375 \r\nQ 359.903125 44.834375 359.903125 42.834375 \r\nL 359.903125 14.2 \r\nQ 359.903125 12.2 357.903125 12.2 \r\nL 286.7625 12.2 \r\nQ 284.7625 12.2 284.7625 14.2 \r\nL 284.7625 42.834375 \r\nQ 284.7625 44.834375 286.7625 44.834375 \r\nz\r\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\r\n    </g>\r\n    <g id=\"line2d_14\">\r\n     <path d=\"M 288.7625 20.298437 \r\nL 308.7625 20.298437 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_15\"/>\r\n    <g id=\"text_12\">\r\n     <!-- loss -->\r\n     <defs>\r\n      <path d=\"M 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\n\" id=\"DejaVuSans-108\"/>\r\n      <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n      <path d=\"M 44.28125 53.078125 \r\nL 44.28125 44.578125 \r\nQ 40.484375 46.53125 36.375 47.5 \r\nQ 32.28125 48.484375 27.875 48.484375 \r\nQ 21.1875 48.484375 17.84375 46.4375 \r\nQ 14.5 44.390625 14.5 40.28125 \r\nQ 14.5 37.15625 16.890625 35.375 \r\nQ 19.28125 33.59375 26.515625 31.984375 \r\nL 29.59375 31.296875 \r\nQ 39.15625 29.25 43.1875 25.515625 \r\nQ 47.21875 21.78125 47.21875 15.09375 \r\nQ 47.21875 7.46875 41.1875 3.015625 \r\nQ 35.15625 -1.421875 24.609375 -1.421875 \r\nQ 20.21875 -1.421875 15.453125 -0.5625 \r\nQ 10.6875 0.296875 5.421875 2 \r\nL 5.421875 11.28125 \r\nQ 10.40625 8.6875 15.234375 7.390625 \r\nQ 20.0625 6.109375 24.8125 6.109375 \r\nQ 31.15625 6.109375 34.5625 8.28125 \r\nQ 37.984375 10.453125 37.984375 14.40625 \r\nQ 37.984375 18.0625 35.515625 20.015625 \r\nQ 33.0625 21.96875 24.703125 23.78125 \r\nL 21.578125 24.515625 \r\nQ 13.234375 26.265625 9.515625 29.90625 \r\nQ 5.8125 33.546875 5.8125 39.890625 \r\nQ 5.8125 47.609375 11.28125 51.796875 \r\nQ 16.75 56 26.8125 56 \r\nQ 31.78125 56 36.171875 55.265625 \r\nQ 40.578125 54.546875 44.28125 53.078125 \r\nz\r\n\" id=\"DejaVuSans-115\"/>\r\n     </defs>\r\n     <g transform=\"translate(316.7625 23.798437)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"line2d_16\">\r\n     <path d=\"M 288.7625 34.976562 \r\nL 308.7625 34.976562 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_17\"/>\r\n    <g id=\"text_13\">\r\n     <!-- val_loss -->\r\n     <defs>\r\n      <path d=\"M 2.984375 54.6875 \r\nL 12.5 54.6875 \r\nL 29.59375 8.796875 \r\nL 46.6875 54.6875 \r\nL 56.203125 54.6875 \r\nL 35.6875 0 \r\nL 23.484375 0 \r\nz\r\n\" id=\"DejaVuSans-118\"/>\r\n      <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n      <path d=\"M 50.984375 -16.609375 \r\nL 50.984375 -23.578125 \r\nL -0.984375 -23.578125 \r\nL -0.984375 -16.609375 \r\nz\r\n\" id=\"DejaVuSans-95\"/>\r\n     </defs>\r\n     <g transform=\"translate(316.7625 38.476562)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-118\"/>\r\n      <use x=\"59.179688\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"120.458984\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"148.242188\" xlink:href=\"#DejaVuSans-95\"/>\r\n      <use x=\"198.242188\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"226.025391\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"287.207031\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"339.306641\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p6795bdb859\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"30.103125\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "plt.plot(train_history, label=\"loss\")\n",
    "plt.plot(val_history, label=\"val_loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "        Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n0        1          60       RL         65.0     8450   Pave   NaN      Reg   \n1        2          20       RL         80.0     9600   Pave   NaN      Reg   \n2        3          60       RL         68.0    11250   Pave   NaN      IR1   \n3        4          70       RL         60.0     9550   Pave   NaN      IR1   \n4        5          60       RL         84.0    14260   Pave   NaN      IR1   \n...    ...         ...      ...          ...      ...    ...   ...      ...   \n1455  1456          60       RL         62.0     7917   Pave   NaN      Reg   \n1456  1457          20       RL         85.0    13175   Pave   NaN      Reg   \n1457  1458          70       RL         66.0     9042   Pave   NaN      Reg   \n1458  1459          20       RL         68.0     9717   Pave   NaN      Reg   \n1459  1460          20       RL         75.0     9937   Pave   NaN      Reg   \n\n     LandContour Utilities  ... PoolArea PoolQC  Fence MiscFeature MiscVal  \\\n0            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n1            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n2            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n3            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n4            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n...          ...       ...  ...      ...    ...    ...         ...     ...   \n1455         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n1456         Lvl    AllPub  ...        0    NaN  MnPrv         NaN       0   \n1457         Lvl    AllPub  ...        0    NaN  GdPrv        Shed    2500   \n1458         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n1459         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n\n     MoSold YrSold  SaleType  SaleCondition  SalePrice  \n0         2   2008        WD         Normal     208500  \n1         5   2007        WD         Normal     181500  \n2         9   2008        WD         Normal     223500  \n3         2   2006        WD        Abnorml     140000  \n4        12   2008        WD         Normal     250000  \n...     ...    ...       ...            ...        ...  \n1455      8   2007        WD         Normal     175000  \n1456      2   2010        WD         Normal     210000  \n1457      5   2010        WD         Normal     266500  \n1458      4   2010        WD         Normal     142125  \n1459      6   2008        WD         Normal     147500  \n\n[1460 rows x 81 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>MSSubClass</th>\n      <th>MSZoning</th>\n      <th>LotFrontage</th>\n      <th>LotArea</th>\n      <th>Street</th>\n      <th>Alley</th>\n      <th>LotShape</th>\n      <th>LandContour</th>\n      <th>Utilities</th>\n      <th>...</th>\n      <th>PoolArea</th>\n      <th>PoolQC</th>\n      <th>Fence</th>\n      <th>MiscFeature</th>\n      <th>MiscVal</th>\n      <th>MoSold</th>\n      <th>YrSold</th>\n      <th>SaleType</th>\n      <th>SaleCondition</th>\n      <th>SalePrice</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>65.0</td>\n      <td>8450</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>208500</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>20</td>\n      <td>RL</td>\n      <td>80.0</td>\n      <td>9600</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>5</td>\n      <td>2007</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>181500</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>68.0</td>\n      <td>11250</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>9</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>223500</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>70</td>\n      <td>RL</td>\n      <td>60.0</td>\n      <td>9550</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2006</td>\n      <td>WD</td>\n      <td>Abnorml</td>\n      <td>140000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>84.0</td>\n      <td>14260</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>12</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>250000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1455</th>\n      <td>1456</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>62.0</td>\n      <td>7917</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>8</td>\n      <td>2007</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>175000</td>\n    </tr>\n    <tr>\n      <th>1456</th>\n      <td>1457</td>\n      <td>20</td>\n      <td>RL</td>\n      <td>85.0</td>\n      <td>13175</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>MnPrv</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2010</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>210000</td>\n    </tr>\n    <tr>\n      <th>1457</th>\n      <td>1458</td>\n      <td>70</td>\n      <td>RL</td>\n      <td>66.0</td>\n      <td>9042</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>GdPrv</td>\n      <td>Shed</td>\n      <td>2500</td>\n      <td>5</td>\n      <td>2010</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>266500</td>\n    </tr>\n    <tr>\n      <th>1458</th>\n      <td>1459</td>\n      <td>20</td>\n      <td>RL</td>\n      <td>68.0</td>\n      <td>9717</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>4</td>\n      <td>2010</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>142125</td>\n    </tr>\n    <tr>\n      <th>1459</th>\n      <td>1460</td>\n      <td>20</td>\n      <td>RL</td>\n      <td>75.0</td>\n      <td>9937</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>6</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>147500</td>\n    </tr>\n  </tbody>\n</table>\n<p>1460 rows × 81 columns</p>\n</div>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "(1460, 3)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "(1460, 1)"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(934, 3) (934, 1)\n(234, 3) (234, 1)\n(292, 3) (292, 1)\n"
    }
   ],
   "source": [
    "# 【問題5】House PricesをKerasで学習\n",
    "# 読み込み\n",
    "df_price = pd.read_csv('train.csv')\n",
    "display(df_price)\n",
    "# 抽出\n",
    "X = df_price[['LotArea', 'GrLivArea', 'YearBuilt']].values\n",
    "y = df_price['SalePrice'].values[:, np.newaxis]\n",
    "display(X.shape)\n",
    "display(y.shape)\n",
    "\n",
    "# 分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "# 標準化\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# 対数変換\n",
    "y_train = np.log(y_train)\n",
    "y_val = np.log(y_val)\n",
    "y_test = np.log(y_test)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "NNfor5(\n  (layer): Sequential(\n    (0): Linear(in_features=3, out_features=10, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=10, out_features=3, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=3, out_features=1, bias=True)\n  )\n)\n"
    }
   ],
   "source": [
    "# PyTorchでモデル構築\n",
    "n_features = 3\n",
    "n_nodes_1 = 10\n",
    "activation_1 = nn.ReLU()\n",
    "n_nodes_2 = 3\n",
    "activation_2 = nn.ReLU()\n",
    "n_output = 1\n",
    "\n",
    "## ネットワーク\n",
    "class NNfor5(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(NNfor5, self).__init__()\n",
    "        self.layer = nn.Sequential(nn.Linear(n_features, n_nodes_1), activation_1,\n",
    "                                   nn.Linear(n_nodes_1, n_nodes_2), activation_2,\n",
    "                                   nn.Linear(n_nodes_2, n_output))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.layer(X)\n",
    "\n",
    "        return X\n",
    "\n",
    "## インスタンス作成\n",
    "model = NNfor5()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ミニバッチ\n",
    "batch_size = 32\n",
    "\n",
    "## numpy配列をテンソルに変換\n",
    "t_X_train = torch.from_numpy(X_train).float()\n",
    "t_y_train = torch.from_numpy(y_train).float()\n",
    "t_X_valid = torch.from_numpy(X_val).float()\n",
    "t_y_valid = torch.from_numpy(y_val).float()\n",
    "\n",
    "## 特徴量とラベルをデータセットにまとめる\n",
    "dataset_train = TensorDataset(t_X_train, t_y_train)\n",
    "dataset_valid = TensorDataset(t_X_valid, t_y_valid)\n",
    "\n",
    "# データローダを作成\n",
    "loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "loader_valid = DataLoader(dataset_valid, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "## オプティマイザ\n",
    "lr = 0.001\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=lr\n",
    ")\n",
    "\n",
    "## 損失関数\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 学習(train)\n",
    "def train_step(X_train, y_train):\n",
    "    ## 訓練モード    \n",
    "    model.train()\n",
    "\n",
    "    ## 訓練データに対して予測\n",
    "    y_train_pred = model(X_train)\n",
    "\n",
    "    ## 勾配初期化\n",
    "    optimizer.zero_grad()\n",
    "    ## 損失の取得\n",
    "    loss = criterion(y_train_pred, y_train)\n",
    "    ## 勾配を計算\n",
    "    loss.backward()\n",
    "\n",
    "    ## パラメータ更新\n",
    "    optimizer.step()\n",
    "\n",
    "    ## 平均絶対誤差計算\n",
    "    with torch.no_grad():\n",
    "        mae = mean_absolute_error(y_train.detach().numpy(), y_train_pred.detach().numpy())\n",
    "\n",
    "    return (loss.item(), mae)\n",
    "\n",
    "## 評価(validation)\n",
    "def valid_step(X_val, y_val):\n",
    "    ## 評価モード\n",
    "    model.eval()\n",
    "\n",
    "    ## 予測\n",
    "    y_val_pred = model(X_val)\n",
    "\n",
    "    ## 損失計算\n",
    "    loss = criterion(y_val_pred, y_val)\n",
    "\n",
    "    ## 平均絶対誤差計算\n",
    "    with torch.no_grad():\n",
    "        mae = mean_absolute_error(y_val.detach().numpy(), y_val_pred.detach().numpy())\n",
    "\n",
    "    return (loss.item(), mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch1/100: [loss:5.3401, mae:12.8789, val_loss:5.5395, val_mae:12.7182]\nepoch2/100: [loss:5.1025, mae:12.5943, val_loss:5.3187, val_mae:12.4658]\nepoch3/100: [loss:4.9292, mae:12.3815, val_loss:5.1555, val_mae:12.2747]\nepoch4/100: [loss:4.7952, mae:12.2130, val_loss:5.0300, val_mae:12.1250]\nepoch5/100: [loss:4.7071, mae:12.1006, val_loss:4.9441, val_mae:12.0210]\nepoch6/100: [loss:4.6305, mae:12.0017, val_loss:4.8753, val_mae:11.9370]\nepoch7/100: [loss:4.5694, mae:11.9219, val_loss:4.8213, val_mae:11.8703]\nepoch8/100: [loss:4.5227, mae:11.8603, val_loss:4.7780, val_mae:11.8164]\nepoch9/100: [loss:4.4798, mae:11.8036, val_loss:4.7410, val_mae:11.7703]\nepoch10/100: [loss:4.4479, mae:11.7608, val_loss:4.7100, val_mae:11.7315]\nepoch11/100: [loss:4.4199, mae:11.7239, val_loss:4.6830, val_mae:11.6976]\nepoch12/100: [loss:4.3976, mae:11.6942, val_loss:4.6574, val_mae:11.6656]\nepoch13/100: [loss:4.3709, mae:11.6586, val_loss:4.6321, val_mae:11.6337]\nepoch14/100: [loss:4.3521, mae:11.6334, val_loss:4.6072, val_mae:11.6023]\nepoch15/100: [loss:4.3373, mae:11.6134, val_loss:4.5829, val_mae:11.5716]\nepoch16/100: [loss:4.3111, mae:11.5783, val_loss:4.5589, val_mae:11.5412]\nepoch17/100: [loss:4.2827, mae:11.5402, val_loss:4.5353, val_mae:11.5112]\nepoch18/100: [loss:4.2701, mae:11.5228, val_loss:4.5119, val_mae:11.4814]\nepoch19/100: [loss:4.2432, mae:11.4866, val_loss:4.4887, val_mae:11.4518]\nepoch20/100: [loss:4.2237, mae:11.4602, val_loss:4.4657, val_mae:11.4224]\nepoch21/100: [loss:4.2031, mae:11.4322, val_loss:4.4429, val_mae:11.3931]\nepoch22/100: [loss:4.1742, mae:11.3927, val_loss:4.4201, val_mae:11.3639]\nepoch23/100: [loss:4.1606, mae:11.3742, val_loss:4.3975, val_mae:11.3348]\nepoch24/100: [loss:4.1329, mae:11.3360, val_loss:4.3749, val_mae:11.3056]\nepoch25/100: [loss:4.1156, mae:11.3125, val_loss:4.3525, val_mae:11.2765]\nepoch26/100: [loss:4.0913, mae:11.2789, val_loss:4.3301, val_mae:11.2475]\nepoch27/100: [loss:4.0758, mae:11.2575, val_loss:4.3079, val_mae:11.2185]\nepoch28/100: [loss:4.0546, mae:11.2283, val_loss:4.2857, val_mae:11.1895]\nepoch29/100: [loss:4.0379, mae:11.2050, val_loss:4.2635, val_mae:11.1605]\nepoch30/100: [loss:4.0096, mae:11.1655, val_loss:4.2414, val_mae:11.1316]\nepoch31/100: [loss:3.9905, mae:11.1389, val_loss:4.2195, val_mae:11.1026]\nepoch32/100: [loss:3.9701, mae:11.1095, val_loss:4.1976, val_mae:11.0737]\nepoch33/100: [loss:3.9503, mae:11.0826, val_loss:4.1757, val_mae:11.0449]\nepoch34/100: [loss:3.9292, mae:11.0531, val_loss:4.1540, val_mae:11.0160]\nepoch35/100: [loss:3.9117, mae:11.0284, val_loss:4.1323, val_mae:10.9872]\nepoch36/100: [loss:3.8885, mae:10.9953, val_loss:4.1106, val_mae:10.9583]\nepoch37/100: [loss:3.8656, mae:10.9631, val_loss:4.0891, val_mae:10.9295]\nepoch38/100: [loss:3.8516, mae:10.9431, val_loss:4.0676, val_mae:10.9008]\nepoch39/100: [loss:3.8295, mae:10.9116, val_loss:4.0462, val_mae:10.8720]\nepoch40/100: [loss:3.8033, mae:10.8742, val_loss:4.0249, val_mae:10.8433]\nepoch41/100: [loss:3.7854, mae:10.8486, val_loss:4.0037, val_mae:10.8146]\nepoch42/100: [loss:3.7657, mae:10.8204, val_loss:3.9825, val_mae:10.7860]\nepoch43/100: [loss:3.7511, mae:10.7991, val_loss:3.9614, val_mae:10.7573]\nepoch44/100: [loss:3.7294, mae:10.7679, val_loss:3.9404, val_mae:10.7287]\nepoch45/100: [loss:3.7136, mae:10.7449, val_loss:3.9194, val_mae:10.7000]\nepoch46/100: [loss:3.6837, mae:10.7015, val_loss:3.8985, val_mae:10.6714]\nepoch47/100: [loss:3.6698, mae:10.6815, val_loss:3.8777, val_mae:10.6429]\nepoch48/100: [loss:3.6507, mae:10.6535, val_loss:3.8570, val_mae:10.6143]\nepoch49/100: [loss:3.6347, mae:10.6300, val_loss:3.8363, val_mae:10.5858]\nepoch50/100: [loss:3.6090, mae:10.5923, val_loss:3.8157, val_mae:10.5573]\nepoch51/100: [loss:3.5905, mae:10.5653, val_loss:3.7951, val_mae:10.5288]\nepoch52/100: [loss:3.5716, mae:10.5373, val_loss:3.7747, val_mae:10.5004]\nepoch53/100: [loss:3.5544, mae:10.5117, val_loss:3.7543, val_mae:10.4719]\nepoch54/100: [loss:3.5354, mae:10.4834, val_loss:3.7340, val_mae:10.4435]\nepoch55/100: [loss:3.5149, mae:10.4531, val_loss:3.7137, val_mae:10.4151]\nepoch56/100: [loss:3.4992, mae:10.4298, val_loss:3.6935, val_mae:10.3867]\nepoch57/100: [loss:3.4745, mae:10.3928, val_loss:3.6734, val_mae:10.3584]\nepoch58/100: [loss:3.4574, mae:10.3671, val_loss:3.6534, val_mae:10.3300]\nepoch59/100: [loss:3.4419, mae:10.3438, val_loss:3.6334, val_mae:10.3017]\nepoch60/100: [loss:3.4229, mae:10.3149, val_loss:3.6135, val_mae:10.2734]\nepoch61/100: [loss:3.4045, mae:10.2875, val_loss:3.5937, val_mae:10.2452]\nepoch62/100: [loss:3.3842, mae:10.2564, val_loss:3.5739, val_mae:10.2169]\nepoch63/100: [loss:3.3703, mae:10.2353, val_loss:3.5542, val_mae:10.1887]\nepoch64/100: [loss:3.3472, mae:10.2001, val_loss:3.5346, val_mae:10.1605]\nepoch65/100: [loss:3.3280, mae:10.1710, val_loss:3.5151, val_mae:10.1323]\nepoch66/100: [loss:3.3090, mae:10.1420, val_loss:3.4956, val_mae:10.1041]\nepoch67/100: [loss:3.2905, mae:10.1136, val_loss:3.4762, val_mae:10.0760]\nepoch68/100: [loss:3.2720, mae:10.0847, val_loss:3.4568, val_mae:10.0479]\nepoch69/100: [loss:3.2540, mae:10.0573, val_loss:3.4375, val_mae:10.0198]\nepoch70/100: [loss:3.2336, mae:10.0257, val_loss:3.4183, val_mae:9.9917]\nepoch71/100: [loss:3.2192, mae:10.0030, val_loss:3.3992, val_mae:9.9637]\nepoch72/100: [loss:3.2008, mae:9.9745, val_loss:3.3801, val_mae:9.9356]\nepoch73/100: [loss:3.1869, mae:9.9523, val_loss:3.3611, val_mae:9.9076]\nepoch74/100: [loss:3.1643, mae:9.9175, val_loss:3.3421, val_mae:9.8796]\nepoch75/100: [loss:3.1473, mae:9.8906, val_loss:3.3233, val_mae:9.8516]\nepoch76/100: [loss:3.1288, mae:9.8613, val_loss:3.3044, val_mae:9.8236]\nepoch77/100: [loss:3.1100, mae:9.8318, val_loss:3.2857, val_mae:9.7956]\nepoch78/100: [loss:3.0943, mae:9.8067, val_loss:3.2670, val_mae:9.7677]\nepoch79/100: [loss:3.0765, mae:9.7785, val_loss:3.2484, val_mae:9.7398]\nepoch80/100: [loss:3.0571, mae:9.7478, val_loss:3.2298, val_mae:9.7119]\nepoch81/100: [loss:3.0420, mae:9.7235, val_loss:3.2113, val_mae:9.6840]\nepoch82/100: [loss:3.0160, mae:9.6811, val_loss:3.1929, val_mae:9.6561]\nepoch83/100: [loss:3.0040, mae:9.6622, val_loss:3.1746, val_mae:9.6283]\nepoch84/100: [loss:2.9926, mae:9.6440, val_loss:3.1563, val_mae:9.6005]\nepoch85/100: [loss:2.9710, mae:9.6092, val_loss:3.1380, val_mae:9.5726]\nepoch86/100: [loss:2.9554, mae:9.5840, val_loss:3.1199, val_mae:9.5448]\nepoch87/100: [loss:2.9347, mae:9.5501, val_loss:3.1017, val_mae:9.5170]\nepoch88/100: [loss:2.9234, mae:9.5317, val_loss:3.0837, val_mae:9.4893]\nepoch89/100: [loss:2.8970, mae:9.4884, val_loss:3.0657, val_mae:9.4615]\nepoch90/100: [loss:2.8878, mae:9.4734, val_loss:3.0478, val_mae:9.4338]\nepoch91/100: [loss:2.8694, mae:9.4432, val_loss:3.0299, val_mae:9.4061]\nepoch92/100: [loss:2.8485, mae:9.4086, val_loss:3.0122, val_mae:9.3784]\nepoch93/100: [loss:2.8327, mae:9.3823, val_loss:2.9944, val_mae:9.3507]\nepoch94/100: [loss:2.8166, mae:9.3557, val_loss:2.9768, val_mae:9.3230]\nepoch95/100: [loss:2.8084, mae:9.3418, val_loss:2.9592, val_mae:9.2954]\nepoch96/100: [loss:2.7898, mae:9.3109, val_loss:2.9416, val_mae:9.2677]\nepoch97/100: [loss:2.7707, mae:9.2787, val_loss:2.9241, val_mae:9.2400]\nepoch98/100: [loss:2.7549, mae:9.2525, val_loss:2.9067, val_mae:9.2124]\nepoch99/100: [loss:2.7304, mae:9.2109, val_loss:2.8893, val_mae:9.1848]\nepoch100/100: [loss:2.7182, mae:9.1902, val_loss:2.8720, val_mae:9.1573]\nDone!\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "OrderedDict([('layer.0.weight',\n              tensor([[ 0.9988,  0.1069, -0.4494],\n                      [-1.1587,  1.1230, -0.0718],\n                      [ 0.2415, -0.2311,  0.2049],\n                      [-0.6205, -0.1775,  0.1778],\n                      [ 0.7147, -0.4227, -0.2417],\n                      [ 1.1904,  0.1519, -1.3252],\n                      [-1.1406,  0.6419, -0.6399],\n                      [ 0.9555,  0.0020,  1.0503],\n                      [-0.5445, -0.0126,  0.2438],\n                      [ 0.1470,  0.1282, -1.2353]])),\n             ('layer.0.bias',\n              tensor([-0.2013,  0.2293, -0.1345, -0.1657, -0.1099,  0.2163, -0.1580,  0.1288,\n                      -0.1908, -0.2086])),\n             ('layer.2.weight',\n              tensor([[ 1.6222e-01,  1.3885e-01,  2.7765e-01,  2.4732e-01,  1.0349e-03,\n                       -1.1514e+00, -2.5825e-01, -8.1617e-01,  1.7965e-01, -4.0384e-01],\n                      [-4.5934e-01, -4.3629e-01, -8.0271e-02, -2.3992e-01,  4.1152e-01,\n                       -2.6186e-01,  3.4637e-01, -1.1698e-02,  8.2060e-01, -6.0078e-02],\n                      [ 1.6788e-01, -3.3839e-01, -1.1664e-01,  2.9762e-01, -3.5209e-02,\n                       -5.2931e-01,  4.1281e-01, -3.2076e-02,  2.2502e-02,  4.0384e-01]])),\n             ('layer.2.bias', tensor([-0.2223, -0.1939, -0.1871])),\n             ('layer.4.weight', tensor([[-1.5318, -0.4502, -0.6037]])),\n             ('layer.4.bias', tensor([2.8472]))])"
     },
     "metadata": {}
    }
   ],
   "source": [
    "## パラメータ初期化関数\n",
    "def initializer(layer):\n",
    "    if type(layer) == nn.Linear:\n",
    "        nn.init.kaiming_normal_(layer.weight)\n",
    "        layer.bias.data.fill_(0.0)\n",
    "\n",
    "## パラメータ初期化する\n",
    "model.apply(initializer)\n",
    "\n",
    "n_epoch = 100\n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "# 1エポック内1データあたりの損失・正解率の平均\n",
    "avg_loss = 0.0\n",
    "avg_mae = 0.0\n",
    "avg_val_loss = 0.0\n",
    "avg_val_mae = 0.0\n",
    "\n",
    "## 学習する\n",
    "for epoch in range(n_epoch):\n",
    "    ## epoch内のloss, accの保存\n",
    "    total_loss = 0.0\n",
    "    total_mae = 0.0\n",
    "    total_val_loss = 0.0\n",
    "    total_val_mae = 0.0\n",
    "    ## 累計データ数の保存\n",
    "    total_train = 0\n",
    "    total_val = 0\n",
    "\n",
    "    ## 訓練\n",
    "    for X_train, y_train in loader_train:\n",
    "       loss, mae = train_step(X_train, y_train)\n",
    "       total_loss += loss\n",
    "       total_mae += mae\n",
    "       total_train += len(y_train)\n",
    "\n",
    "    ## 評価\n",
    "    for X_val, y_val in loader_valid:\n",
    "        val_loss, val_mae = valid_step(X_val, y_val)\n",
    "        total_val_loss += val_loss\n",
    "        total_val_mae += val_mae\n",
    "        total_val += len(y_val)\n",
    "\n",
    "    # 平均計算\n",
    "    avg_loss = total_loss/total_train\n",
    "    avg_val_loss = total_val_loss/total_val\n",
    "    avg_mae = total_mae/len(loader_train)\n",
    "    avg_val_mae = total_val_mae/len(loader_valid)\n",
    "\n",
    "    print(f'epoch{epoch+1}/{n_epoch}: [loss:{avg_loss:.4f}, mae:{avg_mae:.4f}, val_loss:{avg_val_loss:.4f}, val_mae:{avg_val_mae:.4f}]')\n",
    "\n",
    "    # リストに追加\n",
    "    train_history.append(avg_loss)\n",
    "    val_history.append(avg_val_loss)\n",
    "\n",
    "print('Done!')\n",
    "display(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "y_pred_log [2.8472073 2.8472073 2.8472073 2.8472073 2.8472073]\ny_test [12.20918779 11.79810441 11.60823564 12.16525065 11.38509209]\nmae 9.17968304521033\n"
    }
   ],
   "source": [
    "# 予測\n",
    "## Tensor変換\n",
    "t_X_test = torch.from_numpy(X_test).float()\n",
    "y_pred_log = model(t_X_test).detach().numpy()\n",
    "print(\"y_pred_log\", y_pred_log.flatten()[:5])\n",
    "print(\"y_test\", y_test.flatten()[:5])\n",
    "print(\"mae\", mean_absolute_error(y_test, y_pred_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 372.103125 248.518125\" width=\"372.103125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.518125 \r\nL 372.103125 248.518125 \r\nL 372.103125 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 30.103125 224.64 \r\nL 364.903125 224.64 \r\nL 364.903125 7.2 \r\nL 30.103125 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m231030e285\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"45.321307\" xlink:href=\"#m231030e285\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n      </defs>\r\n      <g transform=\"translate(42.140057 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"106.80891\" xlink:href=\"#m231030e285\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 20 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      </defs>\r\n      <g transform=\"translate(100.44641 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"168.296513\" xlink:href=\"#m231030e285\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 40 -->\r\n      <defs>\r\n       <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n      </defs>\r\n      <g transform=\"translate(161.934013 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"229.784117\" xlink:href=\"#m231030e285\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 60 -->\r\n      <defs>\r\n       <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n      </defs>\r\n      <g transform=\"translate(223.421617 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-54\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"291.27172\" xlink:href=\"#m231030e285\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 80 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n      </defs>\r\n      <g transform=\"translate(284.90922 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-56\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"352.759323\" xlink:href=\"#m231030e285\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 100 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g transform=\"translate(343.215573 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m8a41e29c0b\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m8a41e29c0b\" y=\"195.012068\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 3.0 -->\r\n      <defs>\r\n       <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n       <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 198.811287)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m8a41e29c0b\" y=\"159.980538\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 3.5 -->\r\n      <defs>\r\n       <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 163.779757)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m8a41e29c0b\" y=\"124.949009\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 4.0 -->\r\n      <g transform=\"translate(7.2 128.748228)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m8a41e29c0b\" y=\"89.917479\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 4.5 -->\r\n      <g transform=\"translate(7.2 93.716698)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m8a41e29c0b\" y=\"54.88595\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 5.0 -->\r\n      <g transform=\"translate(7.2 58.685168)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m8a41e29c0b\" y=\"19.85442\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 5.5 -->\r\n      <g transform=\"translate(7.2 23.653639)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_13\">\r\n    <path clip-path=\"url(#pe5d9ae3c97)\" d=\"M 45.321307 31.060831 \r\nL 48.395687 47.707302 \r\nL 51.470067 59.842946 \r\nL 54.544447 69.238274 \r\nL 57.618827 75.410062 \r\nL 60.693208 80.772043 \r\nL 63.767588 85.056634 \r\nL 66.841968 88.326272 \r\nL 69.916348 91.335527 \r\nL 72.990728 93.568311 \r\nL 76.065108 95.528554 \r\nL 79.139489 97.094829 \r\nL 82.213869 98.961184 \r\nL 85.288249 100.279496 \r\nL 88.362629 101.314517 \r\nL 91.437009 103.155202 \r\nL 94.511389 105.14115 \r\nL 97.58577 106.027144 \r\nL 100.66015 107.908408 \r\nL 103.73453 109.272971 \r\nL 106.80891 110.716824 \r\nL 109.88329 112.741097 \r\nL 112.95767 113.695126 \r\nL 116.032051 115.635473 \r\nL 119.106431 116.848927 \r\nL 122.180811 118.553276 \r\nL 125.255191 119.638332 \r\nL 128.329571 121.123145 \r\nL 131.403951 122.291828 \r\nL 134.478332 124.276707 \r\nL 137.552712 125.614239 \r\nL 140.627092 127.04425 \r\nL 143.701472 128.428831 \r\nL 146.775852 129.906022 \r\nL 149.850232 131.132174 \r\nL 152.924613 132.761751 \r\nL 155.998993 134.362617 \r\nL 159.073373 135.34438 \r\nL 162.147753 136.897398 \r\nL 165.222133 138.729093 \r\nL 168.296513 139.984718 \r\nL 171.370894 141.363931 \r\nL 174.445274 142.384425 \r\nL 177.519654 143.906362 \r\nL 180.594034 145.015572 \r\nL 183.668414 147.113385 \r\nL 186.742794 148.080464 \r\nL 189.817175 149.420832 \r\nL 192.891555 150.544686 \r\nL 195.965935 152.345743 \r\nL 199.040315 153.64175 \r\nL 202.114695 154.967263 \r\nL 205.189075 156.17093 \r\nL 208.263456 157.503163 \r\nL 211.337836 158.939857 \r\nL 214.412216 160.034483 \r\nL 217.486596 161.766352 \r\nL 220.560976 162.966422 \r\nL 223.635356 164.050719 \r\nL 226.709737 165.385607 \r\nL 229.784117 166.674119 \r\nL 232.858497 168.095445 \r\nL 235.932877 169.067389 \r\nL 239.007257 170.682817 \r\nL 242.081637 172.032952 \r\nL 245.156018 173.361147 \r\nL 248.230398 174.656773 \r\nL 251.304778 175.955163 \r\nL 254.379158 177.215951 \r\nL 257.453538 178.642089 \r\nL 260.527918 179.653765 \r\nL 263.602299 180.943635 \r\nL 266.676679 181.920275 \r\nL 269.751059 183.498932 \r\nL 272.825439 184.693726 \r\nL 275.899819 185.988469 \r\nL 278.974199 187.303853 \r\nL 282.04858 188.406868 \r\nL 285.12296 189.6549 \r\nL 288.19734 191.009326 \r\nL 291.27172 192.067045 \r\nL 294.3461 193.891317 \r\nL 297.42048 194.732651 \r\nL 300.494861 195.529974 \r\nL 303.569241 197.04332 \r\nL 306.643621 198.133564 \r\nL 309.718001 199.587125 \r\nL 312.792381 200.381687 \r\nL 315.866761 202.225538 \r\nL 318.941142 202.873708 \r\nL 322.015522 204.160014 \r\nL 325.089902 205.624856 \r\nL 328.164282 206.735056 \r\nL 331.238662 207.860933 \r\nL 334.313042 208.434961 \r\nL 337.387423 209.736294 \r\nL 340.461803 211.079288 \r\nL 343.536183 212.183513 \r\nL 346.610563 213.902722 \r\nL 349.684943 214.756364 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_14\">\r\n    <path clip-path=\"url(#pe5d9ae3c97)\" d=\"M 45.321307 17.083636 \r\nL 48.395687 32.553813 \r\nL 51.470067 43.994266 \r\nL 54.544447 52.782622 \r\nL 57.618827 58.805453 \r\nL 60.693208 63.623634 \r\nL 63.767588 67.406194 \r\nL 66.841968 70.442886 \r\nL 69.916348 73.032447 \r\nL 72.990728 75.201258 \r\nL 76.065108 77.097107 \r\nL 79.139489 78.886062 \r\nL 82.213869 80.664025 \r\nL 85.288249 82.406552 \r\nL 88.362629 84.111612 \r\nL 91.437009 85.7892 \r\nL 94.511389 87.446531 \r\nL 97.58577 89.085618 \r\nL 100.66015 90.709209 \r\nL 103.73453 92.3195 \r\nL 106.80891 93.920247 \r\nL 109.88329 95.512823 \r\nL 112.95767 97.096832 \r\nL 116.032051 98.681566 \r\nL 119.106431 100.254085 \r\nL 122.180811 101.81937 \r\nL 125.255191 103.379108 \r\nL 128.329571 104.935447 \r\nL 131.403951 106.485845 \r\nL 134.478332 108.032407 \r\nL 137.552712 109.572827 \r\nL 140.627092 111.106937 \r\nL 143.701472 112.636327 \r\nL 146.775852 114.162159 \r\nL 149.850232 115.681579 \r\nL 152.924613 117.197455 \r\nL 155.998993 118.706579 \r\nL 159.073373 120.209864 \r\nL 162.147753 121.709295 \r\nL 165.222133 123.204871 \r\nL 168.296513 124.693193 \r\nL 171.370894 126.175031 \r\nL 174.445274 127.653453 \r\nL 177.519654 129.126655 \r\nL 180.594034 130.596771 \r\nL 183.668414 132.060416 \r\nL 186.742794 133.517854 \r\nL 189.817175 134.971489 \r\nL 192.891555 136.420217 \r\nL 195.965935 137.864138 \r\nL 199.040315 139.301757 \r\nL 202.114695 140.734862 \r\nL 205.189075 142.163785 \r\nL 208.263456 143.587195 \r\nL 211.337836 145.006389 \r\nL 214.412216 146.419935 \r\nL 217.486596 147.829936 \r\nL 220.560976 149.233384 \r\nL 223.635356 150.632854 \r\nL 226.709737 152.0283 \r\nL 229.784117 153.416944 \r\nL 232.858497 154.801457 \r\nL 235.932877 156.179916 \r\nL 239.007257 157.555999 \r\nL 242.081637 158.925727 \r\nL 245.156018 160.290397 \r\nL 248.230398 161.650115 \r\nL 251.304778 163.005646 \r\nL 254.379158 164.356222 \r\nL 257.453538 165.701622 \r\nL 260.527918 167.042574 \r\nL 263.602299 168.379533 \r\nL 266.676679 169.712387 \r\nL 269.751059 171.0403 \r\nL 272.825439 172.363301 \r\nL 275.899819 173.681853 \r\nL 278.974199 174.995608 \r\nL 282.04858 176.304661 \r\nL 285.12296 177.609006 \r\nL 288.19734 178.909532 \r\nL 291.27172 180.204742 \r\nL 294.3461 181.496405 \r\nL 297.42048 182.781452 \r\nL 300.494861 184.062716 \r\nL 303.569241 185.341093 \r\nL 306.643621 186.614499 \r\nL 309.718001 187.883978 \r\nL 312.792381 189.14729 \r\nL 315.866761 190.407816 \r\nL 318.941142 191.662278 \r\nL 322.015522 192.913857 \r\nL 325.089902 194.160075 \r\nL 328.164282 195.402 \r\nL 331.238662 196.639327 \r\nL 334.313042 197.871983 \r\nL 337.387423 199.10261 \r\nL 340.461803 200.328718 \r\nL 343.536183 201.549787 \r\nL 346.610563 202.766189 \r\nL 349.684943 203.977052 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 30.103125 224.64 \r\nL 30.103125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 364.903125 224.64 \r\nL 364.903125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 30.103125 224.64 \r\nL 364.903125 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 30.103125 7.2 \r\nL 364.903125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"legend_1\">\r\n    <g id=\"patch_7\">\r\n     <path d=\"M 286.7625 44.834375 \r\nL 357.903125 44.834375 \r\nQ 359.903125 44.834375 359.903125 42.834375 \r\nL 359.903125 14.2 \r\nQ 359.903125 12.2 357.903125 12.2 \r\nL 286.7625 12.2 \r\nQ 284.7625 12.2 284.7625 14.2 \r\nL 284.7625 42.834375 \r\nQ 284.7625 44.834375 286.7625 44.834375 \r\nz\r\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\r\n    </g>\r\n    <g id=\"line2d_15\">\r\n     <path d=\"M 288.7625 20.298437 \r\nL 308.7625 20.298437 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_16\"/>\r\n    <g id=\"text_13\">\r\n     <!-- loss -->\r\n     <defs>\r\n      <path d=\"M 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\n\" id=\"DejaVuSans-108\"/>\r\n      <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n      <path d=\"M 44.28125 53.078125 \r\nL 44.28125 44.578125 \r\nQ 40.484375 46.53125 36.375 47.5 \r\nQ 32.28125 48.484375 27.875 48.484375 \r\nQ 21.1875 48.484375 17.84375 46.4375 \r\nQ 14.5 44.390625 14.5 40.28125 \r\nQ 14.5 37.15625 16.890625 35.375 \r\nQ 19.28125 33.59375 26.515625 31.984375 \r\nL 29.59375 31.296875 \r\nQ 39.15625 29.25 43.1875 25.515625 \r\nQ 47.21875 21.78125 47.21875 15.09375 \r\nQ 47.21875 7.46875 41.1875 3.015625 \r\nQ 35.15625 -1.421875 24.609375 -1.421875 \r\nQ 20.21875 -1.421875 15.453125 -0.5625 \r\nQ 10.6875 0.296875 5.421875 2 \r\nL 5.421875 11.28125 \r\nQ 10.40625 8.6875 15.234375 7.390625 \r\nQ 20.0625 6.109375 24.8125 6.109375 \r\nQ 31.15625 6.109375 34.5625 8.28125 \r\nQ 37.984375 10.453125 37.984375 14.40625 \r\nQ 37.984375 18.0625 35.515625 20.015625 \r\nQ 33.0625 21.96875 24.703125 23.78125 \r\nL 21.578125 24.515625 \r\nQ 13.234375 26.265625 9.515625 29.90625 \r\nQ 5.8125 33.546875 5.8125 39.890625 \r\nQ 5.8125 47.609375 11.28125 51.796875 \r\nQ 16.75 56 26.8125 56 \r\nQ 31.78125 56 36.171875 55.265625 \r\nQ 40.578125 54.546875 44.28125 53.078125 \r\nz\r\n\" id=\"DejaVuSans-115\"/>\r\n     </defs>\r\n     <g transform=\"translate(316.7625 23.798437)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"line2d_17\">\r\n     <path d=\"M 288.7625 34.976562 \r\nL 308.7625 34.976562 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_18\"/>\r\n    <g id=\"text_14\">\r\n     <!-- val_loss -->\r\n     <defs>\r\n      <path d=\"M 2.984375 54.6875 \r\nL 12.5 54.6875 \r\nL 29.59375 8.796875 \r\nL 46.6875 54.6875 \r\nL 56.203125 54.6875 \r\nL 35.6875 0 \r\nL 23.484375 0 \r\nz\r\n\" id=\"DejaVuSans-118\"/>\r\n      <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n      <path d=\"M 50.984375 -16.609375 \r\nL 50.984375 -23.578125 \r\nL -0.984375 -23.578125 \r\nL -0.984375 -16.609375 \r\nz\r\n\" id=\"DejaVuSans-95\"/>\r\n     </defs>\r\n     <g transform=\"translate(316.7625 38.476562)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-118\"/>\r\n      <use x=\"59.179688\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"120.458984\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"148.242188\" xlink:href=\"#DejaVuSans-95\"/>\r\n      <use x=\"198.242188\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"226.025391\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"287.207031\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"339.306641\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"pe5d9ae3c97\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"30.103125\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "plt.plot(train_history, label=\"loss\")\n",
    "plt.plot(val_history, label=\"val_loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(48000, 1, 28, 28)\n(12000, 1, 28, 28)\n(10000, 1, 28, 28)\n"
    }
   ],
   "source": [
    "# 【問題6】MNISTをKerasで学習\n",
    "# 読み込み\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.astype(np.float)[:, np.newaxis, :, :]\n",
    "X_test = X_test.astype(np.float)[:, np.newaxis, :, :]\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "NNfor6(\n  (layer): Sequential(\n    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1))\n    (3): ReLU()\n    (4): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n    (5): Flatten()\n    (6): Linear(in_features=2304, out_features=10, bias=True)\n  )\n)\n"
    }
   ],
   "source": [
    "# PyTorchでモデル構築\n",
    "n_channels_1 = 1\n",
    "n_channels_2 = 32\n",
    "activation_1 = nn.ReLU()\n",
    "n_channels_3 = 16\n",
    "activation_2 = nn.ReLU()\n",
    "n_features = int(((X_train.shape[2]-4)/2)*((X_train.shape[3]-4)/2)*n_channels_3)\n",
    "n_output = 10\n",
    "\n",
    "## ネットワーク\n",
    "class NNfor6(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(NNfor6, self).__init__()\n",
    "        self.layer = nn.Sequential(nn.Conv2d(n_channels_1, n_channels_2, kernel_size=(3, 3)), activation_1,\n",
    "                                   nn.Conv2d(n_channels_2, n_channels_3, kernel_size=(3, 3)), activation_2,\n",
    "                                   nn.MaxPool2d(kernel_size=(2, 2)),\n",
    "                                   nn.Flatten(),\n",
    "                                   nn.Linear(n_features, n_output))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.layer(X)\n",
    "\n",
    "        return X\n",
    "\n",
    "## インスタンス作成\n",
    "model = NNfor6()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ミニバッチ\n",
    "batch_size = 128\n",
    "\n",
    "## numpy配列をテンソルに変換\n",
    "t_X_train = torch.from_numpy(X_train).float()\n",
    "t_y_train = torch.from_numpy(y_train).long()\n",
    "t_X_valid = torch.from_numpy(X_val).float()\n",
    "t_y_valid = torch.from_numpy(y_val).long()\n",
    "\n",
    "## 特徴量とラベルをデータセットにまとめる\n",
    "dataset_train = TensorDataset(t_X_train, t_y_train)\n",
    "dataset_valid = TensorDataset(t_X_valid, t_y_valid)\n",
    "\n",
    "# データローダを作成\n",
    "loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "loader_valid = DataLoader(dataset_valid, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "## オプティマイザ\n",
    "lr = 0.001\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=lr\n",
    ")\n",
    "\n",
    "## 損失関数\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 学習(train)\n",
    "def train_step(X_train, y_train):\n",
    "    ## 訓練モード    \n",
    "    model.train()\n",
    "\n",
    "    ## 訓練データに対して予測\n",
    "    y_train_pred = model(X_train)\n",
    "\n",
    "    ## 勾配初期化\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    ## 損失の取得\n",
    "    loss = criterion(y_train_pred, y_train)\n",
    "    ## 勾配を計算\n",
    "    loss.backward()\n",
    "\n",
    "    ## パラメータ更新\n",
    "    optimizer.step()\n",
    "\n",
    "    ## 正解数の確認\n",
    "    with torch.no_grad():\n",
    "        y_train_pred_num = torch.argmax(y_train_pred, axis=1)\n",
    "        acc = (y_train_pred_num == y_train).sum()\n",
    "\n",
    "    return (loss.item(), acc.item())\n",
    "\n",
    "## 評価(validation)\n",
    "def valid_step(X_val, y_val):\n",
    "    ## 評価モード\n",
    "    model.eval()\n",
    "\n",
    "    ## 予測\n",
    "    y_val_pred = model(X_val)\n",
    "\n",
    "    ## 損失計算\n",
    "    loss = criterion(y_val_pred, y_val)\n",
    "\n",
    "    ## 正解数計算\n",
    "    with torch.no_grad():\n",
    "        y_val_pred_num = torch.argmax(y_val_pred, axis=1)\n",
    "        acc = (y_val_pred_num == y_val).sum()\n",
    "\n",
    "    return (loss.item(), acc.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch1/5: [loss:0.0025, acc:0.9085, val_loss:0.0009, val_acc:0.9665]\nepoch2/5: [loss:0.0007, acc:0.9731, val_loss:0.0007, val_acc:0.9718]\nepoch3/5: [loss:0.0005, acc:0.9810, val_loss:0.0006, val_acc:0.9784]\nepoch4/5: [loss:0.0004, acc:0.9849, val_loss:0.0005, val_acc:0.9802]\nepoch5/5: [loss:0.0003, acc:0.9862, val_loss:0.0005, val_acc:0.9813]\nDone!\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "OrderedDict([('layer.0.weight',\n              tensor([[[[ 5.8681e-02, -3.6123e-01, -9.7741e-02],\n                        [ 2.1393e-01,  5.9777e-02, -5.4325e-01],\n                        [-1.4735e-01, -6.5735e-02, -2.0590e-01]]],\n              \n              \n                      [[[-3.0938e-01, -1.1926e-01, -4.2475e-01],\n                        [ 3.8840e-01, -1.7485e-01, -5.8645e-02],\n                        [-4.5875e-02,  2.7256e-01,  3.8000e-01]]],\n              \n              \n                      [[[-3.5250e-01,  8.1558e-02, -6.2249e-02],\n                        [-3.8734e-01, -3.0012e-01,  3.5035e-01],\n                        [ 1.6982e-01,  3.5481e-01,  6.7878e-02]]],\n              \n              \n                      [[[-9.9385e-02, -2.8839e-01,  6.6879e-02],\n                        [ 6.9024e-02,  8.8140e-02,  2.2886e-01],\n                        [ 1.2908e-01,  4.1476e-01,  2.5727e-01]]],\n              \n              \n                      [[[ 2.1999e-01,  1.8043e-01,  1.5434e-01],\n                        [-1.6646e-01,  3.2139e-01,  3.8710e-01],\n                        [-2.6582e-01, -9.8099e-03,  2.5789e-01]]],\n              \n              \n                      [[[-1.2231e-01,  2.8970e-01, -3.0303e-01],\n                        [ 1.9174e-01,  4.3019e-01, -2.2126e-01],\n                        [ 3.2960e-01,  7.5169e-02,  2.1355e-01]]],\n              \n              \n                      [[[-2.3185e-01, -2.8604e-01, -3.4926e-01],\n                        [-3.3522e-01, -1.3709e-01, -4.1304e-01],\n                        [-1.5655e-02,  1.6453e-01, -5.1080e-02]]],\n              \n              \n                      [[[ 3.2322e-01, -3.9675e-02, -4.3706e-01],\n                        [ 3.0969e-01, -7.4978e-02, -7.1631e-02],\n                        [ 4.7540e-01,  2.9288e-01,  3.6910e-02]]],\n              \n              \n                      [[[-5.3831e-01, -1.2955e-01,  3.2158e-01],\n                        [-3.2606e-01,  2.3799e-01,  1.5082e-01],\n                        [-1.8222e-01, -9.3079e-02,  2.0684e-01]]],\n              \n              \n                      [[[-9.2776e-02,  1.5594e-01,  1.5384e-01],\n                        [-4.5063e-01, -3.7405e-01, -3.7500e-01],\n                        [-7.8926e-02, -4.0114e-03, -3.0440e-01]]],\n              \n              \n                      [[[ 3.5843e-01,  6.8122e-02,  4.2210e-01],\n                        [-7.1174e-02, -2.7179e-01,  2.4357e-01],\n                        [-2.2301e-01, -1.3225e-01, -1.9334e-01]]],\n              \n              \n                      [[[ 5.0676e-02,  4.9790e-02,  2.1580e-01],\n                        [ 2.5772e-01,  1.0682e-01,  1.9003e-01],\n                        [-1.0961e-01, -3.0803e-02, -1.1250e-01]]],\n              \n              \n                      [[[-1.3058e-01, -1.9739e-01,  2.5497e-01],\n                        [-1.5658e-01,  9.0660e-02,  3.3387e-01],\n                        [-4.1745e-01, -9.6414e-02,  4.2771e-01]]],\n              \n              \n                      [[[-4.5858e-01,  1.2864e-01, -2.3698e-01],\n                        [-2.7185e-01,  5.7508e-02, -1.0090e-01],\n                        [-2.0725e-02,  3.4260e-01,  4.3223e-01]]],\n              \n              \n                      [[[-1.6926e-01, -2.2594e-01, -8.2278e-02],\n                        [-3.4688e-01, -5.1746e-02, -9.9012e-02],\n                        [-1.1004e-03, -9.1255e-02,  2.1292e-01]]],\n              \n              \n                      [[[-2.1590e-01, -1.6525e-01,  1.0950e-01],\n                        [-2.7906e-01, -1.9944e-01, -2.8407e-01],\n                        [ 1.1862e-01, -1.8780e-01,  1.9114e-02]]],\n              \n              \n                      [[[ 1.6758e-01,  2.3003e-01, -8.3108e-02],\n                        [ 4.2526e-01, -1.1406e-01, -4.6107e-01],\n                        [ 2.1422e-01,  9.5158e-02, -2.1944e-01]]],\n              \n              \n                      [[[-2.0601e-01, -3.3402e-01,  1.4908e-01],\n                        [ 2.6326e-01,  1.7780e-01,  3.8178e-01],\n                        [ 3.1760e-01,  3.8923e-01,  3.7485e-01]]],\n              \n              \n                      [[[ 3.7180e-01, -1.9754e-01,  2.6537e-01],\n                        [ 3.2564e-01,  9.1359e-02,  1.0575e-01],\n                        [-1.3411e-01,  1.8267e-01,  2.0921e-01]]],\n              \n              \n                      [[[ 1.5960e-02,  3.7007e-01,  5.1010e-01],\n                        [ 1.4489e-01, -9.8587e-02, -1.1673e-01],\n                        [-3.2189e-01, -4.4586e-01, -2.0259e-01]]],\n              \n              \n                      [[[-2.6713e-01, -4.5290e-01, -3.6102e-01],\n                        [-1.1692e-01,  8.6870e-02,  4.3020e-01],\n                        [ 3.9013e-01,  3.8622e-01, -1.2089e-01]]],\n              \n              \n                      [[[-4.3195e-01, -3.7291e-01, -1.6662e-01],\n                        [-2.5693e-01,  1.4078e-01,  1.9722e-01],\n                        [ 3.7419e-01,  1.4544e-01,  1.5215e-01]]],\n              \n              \n                      [[[-1.8871e-01,  3.8175e-01,  4.4722e-01],\n                        [-3.2044e-01,  2.4828e-01,  2.2461e-01],\n                        [-1.0800e-01, -4.2764e-02, -1.9029e-01]]],\n              \n              \n                      [[[-5.5163e-02,  3.4430e-01, -8.9483e-02],\n                        [ 2.4176e-01,  1.0837e-01,  2.6393e-02],\n                        [-8.4600e-02,  3.2032e-01, -1.6947e-01]]],\n              \n              \n                      [[[-4.8451e-02,  3.4126e-01,  7.4440e-02],\n                        [-3.0185e-01, -3.7135e-02, -2.3693e-01],\n                        [-2.5361e-01, -3.5177e-01, -3.2266e-01]]],\n              \n              \n                      [[[ 2.7258e-01, -1.6412e-01, -6.9357e-02],\n                        [ 2.9559e-01,  1.7088e-01, -3.1005e-01],\n                        [ 3.2433e-01, -1.7465e-01, -3.6494e-01]]],\n              \n              \n                      [[[ 1.1484e-01,  3.0530e-02,  3.1819e-01],\n                        [-1.1967e-01, -2.4136e-01,  2.2304e-01],\n                        [ 1.8918e-01, -2.7747e-01,  4.3893e-02]]],\n              \n              \n                      [[[-1.6989e-01,  4.0012e-01,  2.6439e-01],\n                        [ 2.3532e-01,  3.3891e-01,  1.3428e-01],\n                        [-3.8184e-01, -5.0451e-01, -3.7302e-01]]],\n              \n              \n                      [[[ 4.0824e-01, -1.8083e-01,  1.4820e-01],\n                        [-4.1033e-02, -1.6500e-01, -5.2481e-01],\n                        [ 2.1343e-01, -3.3200e-01,  1.0452e-01]]],\n              \n              \n                      [[[-8.4066e-02,  1.9742e-01,  2.2477e-01],\n                        [-5.1273e-01,  2.3076e-01,  1.0594e-01],\n                        [-1.6094e-01, -2.9183e-01,  4.0292e-01]]],\n              \n              \n                      [[[ 1.3759e-01,  2.7797e-01,  2.0466e-01],\n                        [ 2.1190e-05, -3.5006e-01, -2.8985e-01],\n                        [ 3.3663e-01, -7.0232e-02,  1.4296e-01]]],\n              \n              \n                      [[[ 2.1662e-01,  1.0418e-01,  2.4599e-01],\n                        [ 1.0513e-01,  9.1867e-02, -3.4433e-01],\n                        [ 3.7481e-01,  2.5652e-02, -1.2514e-01]]]])),\n             ('layer.0.bias',\n              tensor([ 0.3615,  0.0557,  0.1783, -0.1578, -0.0475, -0.0925,  0.3629, -0.1695,\n                       0.3020,  0.3239,  0.1533,  0.0553,  0.1879,  0.2021,  0.3201, -0.1849,\n                      -0.1378, -0.1240, -0.2993,  0.0664,  0.1961,  0.2501,  0.2100,  0.0324,\n                       0.1448, -0.0160,  0.1361,  0.0390,  0.2375, -0.1650, -0.0053,  0.2879])),\n             ('layer.2.weight',\n              tensor([[[[-1.5954e-02,  2.7163e-02, -5.1806e-03],\n                        [ 5.4170e-02,  1.7064e-02,  4.9079e-02],\n                        [ 4.6804e-02,  1.6252e-02, -9.6389e-03]],\n              \n                       [[-4.3641e-02, -1.6530e-02,  8.9189e-02],\n                        [-9.4350e-02, -6.4151e-02,  7.2973e-03],\n                        [ 4.6865e-02,  4.0298e-02,  2.4643e-02]],\n              \n                       [[ 3.6534e-02, -4.2704e-02, -6.7637e-02],\n                        [-1.6865e-02, -6.9072e-03, -2.0646e-02],\n                        [-1.1772e-01,  2.6735e-02,  9.9271e-03]],\n              \n                       ...,\n              \n                       [[-5.5182e-03,  2.0410e-01, -5.8634e-02],\n                        [-7.5058e-02,  2.1141e-01,  1.1328e-01],\n                        [ 2.9694e-02,  3.4307e-03,  1.2755e-01]],\n              \n                       [[ 1.5748e-02,  1.2137e-02,  7.1631e-02],\n                        [ 4.5315e-02, -8.3696e-03, -3.4598e-02],\n                        [ 3.2417e-02, -4.9383e-02,  4.4571e-02]],\n              \n                       [[ 7.7828e-02,  7.4402e-04,  7.6138e-02],\n                        [ 8.4473e-02,  2.9791e-02, -2.5837e-02],\n                        [ 4.9797e-02,  4.9382e-02,  3.9680e-03]]],\n              \n              \n                      [[[-4.9496e-03, -6.3953e-02, -3.3740e-02],\n                        [ 2.5145e-02, -3.5839e-02, -9.7973e-02],\n                        [-8.0330e-02, -9.4827e-02, -6.1644e-05]],\n              \n                       [[ 5.4880e-03,  5.5443e-02,  2.7921e-02],\n                        [ 1.9647e-01,  1.4598e-01,  1.9679e-01],\n                        [ 7.4647e-02,  1.2456e-01,  1.7228e-01]],\n              \n                       [[-1.5377e-01,  1.0234e-02,  5.0719e-02],\n                        [-8.7906e-03, -4.3881e-02,  6.1916e-03],\n                        [-6.8808e-02, -1.0837e-01, -2.5759e-02]],\n              \n                       ...,\n              \n                       [[ 6.0524e-02, -1.2948e-02, -6.9444e-02],\n                        [ 1.6729e-02, -1.3854e-01, -7.2459e-02],\n                        [ 8.2710e-04, -5.7346e-02, -1.6629e-01]],\n              \n                       [[ 1.6485e-01,  3.7530e-02,  1.0346e-01],\n                        [ 1.6587e-02,  2.2578e-02, -4.7918e-02],\n                        [-7.2573e-02, -1.0506e-04, -6.0285e-02]],\n              \n                       [[ 6.2108e-02,  5.2202e-02, -2.9441e-02],\n                        [ 6.5801e-02,  1.3934e-02, -8.1794e-03],\n                        [-1.7476e-02,  7.5902e-02, -2.4029e-02]]],\n              \n              \n                      [[[-5.0244e-02,  4.1844e-02, -7.3696e-03],\n                        [ 4.3040e-02, -6.0756e-02, -2.9961e-02],\n                        [-2.0326e-02, -1.7155e-02, -4.1468e-02]],\n              \n                       [[-5.5947e-02,  2.3580e-02, -3.5729e-02],\n                        [ 1.4847e-02, -1.7119e-02, -9.8963e-03],\n                        [-6.8440e-02, -6.5104e-02,  2.2200e-02]],\n              \n                       [[ 2.9206e-02, -5.9941e-02, -5.6144e-02],\n                        [ 3.2040e-02,  3.4265e-02, -2.9910e-02],\n                        [-2.2977e-02,  2.7724e-02, -6.3174e-05]],\n              \n                       ...,\n              \n                       [[-3.4356e-02,  1.8208e-02, -5.5587e-02],\n                        [ 2.1489e-02,  1.4958e-02,  3.3865e-02],\n                        [-4.4627e-02,  1.8665e-02,  2.7637e-02]],\n              \n                       [[-9.2466e-04,  2.2760e-02, -7.8882e-04],\n                        [-3.7062e-02, -2.5828e-02,  6.6869e-04],\n                        [ 4.4100e-02,  3.9810e-02, -6.4636e-02]],\n              \n                       [[-3.2808e-02,  1.6266e-02,  1.5872e-02],\n                        [ 1.2143e-02,  3.6925e-02, -1.0871e-02],\n                        [ 1.1037e-02,  9.3137e-03, -1.4215e-03]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-5.1781e-02,  3.5283e-02,  1.0767e-01],\n                        [-1.5183e-02,  6.1384e-02,  1.5467e-01],\n                        [-2.2659e-02,  2.8353e-02,  9.4087e-02]],\n              \n                       [[-1.6674e-01, -3.0681e-01, -2.7696e-01],\n                        [-2.8626e-01, -3.4257e-01, -1.6349e-01],\n                        [-2.9519e-01, -2.2313e-01,  2.3451e-03]],\n              \n                       [[ 6.3665e-02, -4.8163e-02, -8.4488e-02],\n                        [ 1.1238e-01, -7.9882e-02, -7.2324e-02],\n                        [ 5.1250e-02, -2.2604e-02,  1.3578e-02]],\n              \n                       ...,\n              \n                       [[ 1.6311e-01,  1.7848e-01, -1.0215e-01],\n                        [ 2.5892e-01,  2.7301e-01, -2.4280e-01],\n                        [ 1.7050e-01,  3.5680e-01, -2.9459e-01]],\n              \n                       [[-2.1608e-03,  2.0595e-02,  1.3102e-01],\n                        [-1.0765e-02,  6.3693e-02,  1.1053e-01],\n                        [ 5.4670e-02,  9.7428e-02,  2.7013e-02]],\n              \n                       [[ 5.3937e-02,  9.8963e-02,  3.9402e-02],\n                        [ 1.4837e-02,  6.2237e-02,  6.6508e-02],\n                        [ 8.1561e-02,  6.1687e-02,  5.3015e-02]]],\n              \n              \n                      [[[-4.7737e-02, -9.5583e-02, -3.4827e-02],\n                        [-9.9112e-02, -1.9875e-02, -6.9309e-02],\n                        [-7.3062e-02, -4.8370e-02, -2.3447e-02]],\n              \n                       [[ 1.3627e-01,  1.6264e-01,  1.1768e-01],\n                        [ 1.5836e-01,  1.6844e-01,  1.3863e-01],\n                        [-1.0812e-01, -4.2537e-02,  6.9082e-04]],\n              \n                       [[-5.9681e-02,  9.5939e-02,  1.1568e-01],\n                        [-1.9910e-02,  1.9618e-02,  4.7870e-02],\n                        [-5.3381e-02, -1.3058e-01, -8.3085e-02]],\n              \n                       ...,\n              \n                       [[ 3.6773e-02, -2.3195e-01, -2.0046e-01],\n                        [-1.5721e-01, -3.2840e-01, -2.8322e-01],\n                        [-1.9740e-01, -2.6132e-01, -2.5054e-01]],\n              \n                       [[ 1.2656e-01,  9.8077e-02,  3.9224e-02],\n                        [-3.1886e-02, -6.4912e-02, -5.2371e-02],\n                        [-9.2841e-03,  3.0690e-02, -3.8346e-02]],\n              \n                       [[ 3.4394e-02, -2.8340e-02, -4.9342e-02],\n                        [-1.8722e-03, -3.0252e-02,  2.2752e-02],\n                        [ 6.6110e-02,  4.2267e-02, -2.6959e-04]]],\n              \n              \n                      [[[-5.3799e-02, -4.8728e-02,  3.2254e-02],\n                        [ 1.0050e-02,  2.1237e-02,  7.3689e-02],\n                        [-2.9971e-02,  4.2905e-02,  2.4268e-02]],\n              \n                       [[ 4.7773e-02,  1.0516e-02,  6.7324e-02],\n                        [-2.2302e-02, -8.2064e-02,  2.5509e-02],\n                        [-3.4093e-02, -3.1965e-02, -9.0890e-02]],\n              \n                       [[ 1.0843e-01,  5.4627e-02, -7.9101e-02],\n                        [ 2.4743e-02, -1.1903e-01,  2.5795e-03],\n                        [-6.8374e-02, -3.6218e-02,  4.0557e-02]],\n              \n                       ...,\n              \n                       [[-4.5248e-02, -2.3070e-01, -1.1560e-01],\n                        [-1.4729e-01, -2.8602e-02, -4.4234e-02],\n                        [-1.1287e-01, -3.4855e-03, -2.1710e-02]],\n              \n                       [[-8.7791e-02, -5.4266e-02, -2.8428e-02],\n                        [ 3.2614e-02,  4.1651e-02,  6.5261e-02],\n                        [ 1.1037e-01, -2.1600e-02, -1.8111e-02]],\n              \n                       [[ 5.5061e-03,  2.0774e-02,  1.7531e-03],\n                        [ 6.5709e-02, -4.7911e-04,  3.2301e-03],\n                        [-1.0933e-02, -3.4110e-02, -3.7913e-02]]]])),\n             ('layer.2.bias',\n              tensor([ 0.0289, -0.0240, -0.0572,  0.0391, -0.0360,  0.0409,  0.0563,  0.0460,\n                      -0.0343,  0.0346,  0.0179,  0.0015,  0.0399,  0.0039,  0.0534,  0.0471])),\n             ('layer.6.weight',\n              tensor([[-0.0113,  0.0310, -0.0409,  ..., -0.0252, -0.1212, -0.0846],\n                      [-0.0319,  0.0259, -0.0016,  ...,  0.0062, -0.0812, -0.0195],\n                      [-0.0172, -0.0364,  0.0628,  ...,  0.0218, -0.0090,  0.0448],\n                      ...,\n                      [-0.0155, -0.0212, -0.0255,  ..., -0.0655, -0.1040, -0.0930],\n                      [ 0.0117, -0.0589, -0.0469,  ...,  0.0612,  0.0320, -0.0624],\n                      [ 0.0503, -0.0260, -0.0157,  ...,  0.0110,  0.0058,  0.0729]])),\n             ('layer.6.bias',\n              tensor([-0.0033,  0.0335,  0.0078, -0.0065, -0.0088,  0.0054,  0.0014,  0.0127,\n                      -0.0242, -0.0118]))])"
     },
     "metadata": {}
    }
   ],
   "source": [
    "## パラメータ初期化関数\n",
    "def initializer(layer):\n",
    "    if type(layer) == nn.Linear:\n",
    "        nn.init.kaiming_normal_(layer.weight)\n",
    "        layer.bias.data.fill_(0.0)\n",
    "\n",
    "## パラメータ初期化する\n",
    "model.apply(initializer)\n",
    "\n",
    "n_epoch = 5\n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "# 1エポック内1データあたりの損失・正解率の平均\n",
    "avg_loss = 0.0\n",
    "avg_acc = 0.0\n",
    "avg_val_loss = 0.0\n",
    "avg_val_acc = 0.0\n",
    "\n",
    "## 学習する\n",
    "for epoch in range(n_epoch):\n",
    "    ## epoch内のloss, accの保存\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    total_val_loss = 0.0\n",
    "    total_val_acc = 0.0\n",
    "    ## 累計データ数の保存\n",
    "    total_train = 0\n",
    "    total_val = 0\n",
    "\n",
    "    ## 訓練\n",
    "    for X_train, y_train in loader_train:\n",
    "       loss, acc = train_step(X_train, y_train)\n",
    "       total_loss += loss\n",
    "       total_acc += acc\n",
    "       total_train += len(y_train)\n",
    "\n",
    "    ## 評価\n",
    "    for X_val, y_val in loader_valid:\n",
    "        val_loss, val_acc = valid_step(X_val, y_val)\n",
    "        total_val_loss += val_loss\n",
    "        total_val_acc += val_acc\n",
    "        total_val += len(y_val)\n",
    "\n",
    "    # 平均計算\n",
    "    avg_loss = total_loss/total_train\n",
    "    avg_val_loss = total_val_loss/total_val\n",
    "    avg_acc = total_acc/total_train\n",
    "    avg_val_acc = total_val_acc/total_val\n",
    "\n",
    "    print(f'epoch{epoch+1}/{n_epoch}: [loss:{avg_loss:.4f}, acc:{avg_acc:.4f}, val_loss:{avg_val_loss:.4f}, val_acc:{avg_val_acc:.4f}]')\n",
    "\n",
    "    # リストに追加\n",
    "    train_history.append(avg_loss)\n",
    "    val_history.append(avg_val_loss)\n",
    "\n",
    "print('Done!')\n",
    "display(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "y_pred_proba tensor([[ -5.5794,  -7.3717,   0.0401,  ...,  16.5424,  -1.1148,   3.0051],\n        [ -5.8804,   0.9850,  12.7610,  ..., -17.0270,  -3.5527, -12.7304],\n        [ -4.2491,   8.8230,  -0.7761,  ...,   1.0472,  -0.8975,  -6.4178],\n        ...,\n        [-11.6454,  -4.9167, -10.9417,  ...,  -0.0927,   1.1010,   1.8523],\n        [ -7.6838, -10.2607, -11.8314,  ...,  -8.4907,   4.0409,  -4.2381],\n        [ -8.2704, -15.6708,  -1.4593,  ..., -11.7622,  -5.0503, -14.0557]],\n       grad_fn=<AddmmBackward>)\ny_pred [7 2 1 ... 4 5 6]\ny_test [7 2 1 ... 4 5 6]\nacc:  0.9849\n"
    }
   ],
   "source": [
    "# 予測\n",
    "## Tensor変換\n",
    "t_X_test = torch.from_numpy(X_test).float()\n",
    "y_pred_proba = model(t_X_test)\n",
    "y_pred = torch.argmax(y_pred_proba, axis=1)\n",
    "print(\"y_pred_proba\", y_pred_proba)\n",
    "print(\"y_pred\", y_pred.flatten().int().detach().numpy())\n",
    "print(\"y_test\", y_test.flatten())\n",
    "print(\"acc: \", accuracy_score(y_test.flatten(),  y_pred.flatten().int().detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 391.190625 248.518125\" width=\"391.190625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.518125 \r\nL 391.190625 248.518125 \r\nL 391.190625 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 49.190625 224.64 \r\nL 383.990625 224.64 \r\nL 383.990625 7.2 \r\nL 49.190625 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m595c0a6668\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"64.408807\" xlink:href=\"#m595c0a6668\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0.0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n       <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n      </defs>\r\n      <g transform=\"translate(56.457244 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"102.454261\" xlink:href=\"#m595c0a6668\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 0.5 -->\r\n      <defs>\r\n       <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n      </defs>\r\n      <g transform=\"translate(94.502699 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"140.499716\" xlink:href=\"#m595c0a6668\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 1.0 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g transform=\"translate(132.548153 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"178.54517\" xlink:href=\"#m595c0a6668\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 1.5 -->\r\n      <g transform=\"translate(170.593608 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"216.590625\" xlink:href=\"#m595c0a6668\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 2.0 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      </defs>\r\n      <g transform=\"translate(208.639063 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"254.63608\" xlink:href=\"#m595c0a6668\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 2.5 -->\r\n      <g transform=\"translate(246.684517 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_7\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"292.681534\" xlink:href=\"#m595c0a6668\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 3.0 -->\r\n      <defs>\r\n       <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n      </defs>\r\n      <g transform=\"translate(284.729972 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_8\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"330.726989\" xlink:href=\"#m595c0a6668\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 3.5 -->\r\n      <g transform=\"translate(322.775426 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_9\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"368.772443\" xlink:href=\"#m595c0a6668\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 4.0 -->\r\n      <defs>\r\n       <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n      </defs>\r\n      <g transform=\"translate(360.820881 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_10\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m26eb5ff510\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"49.190625\" xlink:href=\"#m26eb5ff510\" y=\"200.391147\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 0.0005 -->\r\n      <g transform=\"translate(7.2 204.190366)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"286.279297\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"49.190625\" xlink:href=\"#m26eb5ff510\" y=\"155.133393\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 0.0010 -->\r\n      <g transform=\"translate(7.2 158.932612)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"286.279297\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"49.190625\" xlink:href=\"#m26eb5ff510\" y=\"109.87564\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 0.0015 -->\r\n      <g transform=\"translate(7.2 113.674859)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"286.279297\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"49.190625\" xlink:href=\"#m26eb5ff510\" y=\"64.617886\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 0.0020 -->\r\n      <g transform=\"translate(7.2 68.417105)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"286.279297\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"49.190625\" xlink:href=\"#m26eb5ff510\" y=\"19.360133\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- 0.0025 -->\r\n      <g transform=\"translate(7.2 23.159352)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"286.279297\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_15\">\r\n    <path clip-path=\"url(#p68fab7c7f2)\" d=\"M 64.408807 17.083636 \r\nL 140.499716 179.289668 \r\nL 216.590625 198.84135 \r\nL 292.681534 209.297684 \r\nL 368.772443 214.756364 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_16\">\r\n    <path clip-path=\"url(#p68fab7c7f2)\" d=\"M 64.408807 163.513362 \r\nL 140.499716 178.957465 \r\nL 216.590625 194.568002 \r\nL 292.681534 199.24589 \r\nL 368.772443 201.94667 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 49.190625 224.64 \r\nL 49.190625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 383.990625 224.64 \r\nL 383.990625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 49.190625 224.64 \r\nL 383.990625 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 49.190625 7.2 \r\nL 383.990625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"legend_1\">\r\n    <g id=\"patch_7\">\r\n     <path d=\"M 305.85 44.834375 \r\nL 376.990625 44.834375 \r\nQ 378.990625 44.834375 378.990625 42.834375 \r\nL 378.990625 14.2 \r\nQ 378.990625 12.2 376.990625 12.2 \r\nL 305.85 12.2 \r\nQ 303.85 12.2 303.85 14.2 \r\nL 303.85 42.834375 \r\nQ 303.85 44.834375 305.85 44.834375 \r\nz\r\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\r\n    </g>\r\n    <g id=\"line2d_17\">\r\n     <path d=\"M 307.85 20.298437 \r\nL 327.85 20.298437 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_18\"/>\r\n    <g id=\"text_15\">\r\n     <!-- loss -->\r\n     <defs>\r\n      <path d=\"M 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\n\" id=\"DejaVuSans-108\"/>\r\n      <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n      <path d=\"M 44.28125 53.078125 \r\nL 44.28125 44.578125 \r\nQ 40.484375 46.53125 36.375 47.5 \r\nQ 32.28125 48.484375 27.875 48.484375 \r\nQ 21.1875 48.484375 17.84375 46.4375 \r\nQ 14.5 44.390625 14.5 40.28125 \r\nQ 14.5 37.15625 16.890625 35.375 \r\nQ 19.28125 33.59375 26.515625 31.984375 \r\nL 29.59375 31.296875 \r\nQ 39.15625 29.25 43.1875 25.515625 \r\nQ 47.21875 21.78125 47.21875 15.09375 \r\nQ 47.21875 7.46875 41.1875 3.015625 \r\nQ 35.15625 -1.421875 24.609375 -1.421875 \r\nQ 20.21875 -1.421875 15.453125 -0.5625 \r\nQ 10.6875 0.296875 5.421875 2 \r\nL 5.421875 11.28125 \r\nQ 10.40625 8.6875 15.234375 7.390625 \r\nQ 20.0625 6.109375 24.8125 6.109375 \r\nQ 31.15625 6.109375 34.5625 8.28125 \r\nQ 37.984375 10.453125 37.984375 14.40625 \r\nQ 37.984375 18.0625 35.515625 20.015625 \r\nQ 33.0625 21.96875 24.703125 23.78125 \r\nL 21.578125 24.515625 \r\nQ 13.234375 26.265625 9.515625 29.90625 \r\nQ 5.8125 33.546875 5.8125 39.890625 \r\nQ 5.8125 47.609375 11.28125 51.796875 \r\nQ 16.75 56 26.8125 56 \r\nQ 31.78125 56 36.171875 55.265625 \r\nQ 40.578125 54.546875 44.28125 53.078125 \r\nz\r\n\" id=\"DejaVuSans-115\"/>\r\n     </defs>\r\n     <g transform=\"translate(335.85 23.798437)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"line2d_19\">\r\n     <path d=\"M 307.85 34.976562 \r\nL 327.85 34.976562 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_20\"/>\r\n    <g id=\"text_16\">\r\n     <!-- val_loss -->\r\n     <defs>\r\n      <path d=\"M 2.984375 54.6875 \r\nL 12.5 54.6875 \r\nL 29.59375 8.796875 \r\nL 46.6875 54.6875 \r\nL 56.203125 54.6875 \r\nL 35.6875 0 \r\nL 23.484375 0 \r\nz\r\n\" id=\"DejaVuSans-118\"/>\r\n      <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n      <path d=\"M 50.984375 -16.609375 \r\nL 50.984375 -23.578125 \r\nL -0.984375 -23.578125 \r\nL -0.984375 -16.609375 \r\nz\r\n\" id=\"DejaVuSans-95\"/>\r\n     </defs>\r\n     <g transform=\"translate(335.85 38.476562)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-118\"/>\r\n      <use x=\"59.179688\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"120.458984\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"148.242188\" xlink:href=\"#DejaVuSans-95\"/>\r\n      <use x=\"198.242188\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"226.025391\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"287.207031\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"339.306641\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p68fab7c7f2\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"49.190625\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "plt.plot(train_history, label=\"loss\")\n",
    "plt.plot(val_history, label=\"val_loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題8】\n",
    "#### （アドバンス課題）フレームワークの比較\n",
    "それぞれのフレームワークにはどのような違いがあるかをまとめてください。\n",
    "\n",
    "\n",
    "**《視点例》**\n",
    "\n",
    "\n",
    "- 計算速度\n",
    "- コードの行数・可読性\n",
    "- 用意されている機能"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}