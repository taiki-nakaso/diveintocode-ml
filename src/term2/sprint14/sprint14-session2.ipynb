{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint\n",
    "## ディープラーニングフレームワーク2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.このSprintについて\n",
    "\n",
    "### Sprintの目的\n",
    "- フレームワークのコードを読めるようにする\n",
    "- フレームワークを習得し続けられるようになる\n",
    "- 理論を知っている範囲をフレームワークで動かす\n",
    "\n",
    "### どのように学ぶか\n",
    "前半はTensorFlowのExampleを動かします。後半ではKerasのコードを書いていきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.公式Example\n",
    "\n",
    "深層学習フレームワークには公式に様々なモデルのExampleコードが公開されています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題1】\n",
    "#### 公式チュートリアルモデルを分担して実行\n",
    "TensorFLowの公式チュートリアルモデルを分担して実行してください。\n",
    "\n",
    "\n",
    "以下の中から1人ひとつ選び実行し、その結果を簡単に発表してください。\n",
    "\n",
    "\n",
    "[models/tutorials at master · tensorflow/models](https://www.tensorflow.org/tutorials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kerasによる分散トレーニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "WARNING:absl:Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n"
    }
   ],
   "source": [
    "# mnistデータセットの読み込み\n",
    "tfds.disable_progress_bar()\n",
    "datasets, info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
    "\n",
    "mnist_train, mnist_test = datasets['train'], datasets['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用可能デバイス探索\n",
    "def get_available_devices():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x for x in local_device_protos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[name: \"/device:CPU:0\"\n device_type: \"CPU\"\n memory_limit: 268435456\n locality {\n }\n incarnation: 8428612216925228810]"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "get_available_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:There is non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\nWARNING:tensorflow:There is non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n"
    }
   ],
   "source": [
    "# 複数GPUでの計算を可能にするAPI\n",
    "# このスコープ内でモデルを作成すると分散処理ができる\n",
    "strategy = tf.distribute.MirroredStrategy(devices=['/cpu:0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Number of devices: 1\n"
    }
   ],
   "source": [
    "# デバイス数表示\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ分割\n",
    "num_train_examples = info.splits['train'].num_examples\n",
    "num_test_examples = info.splits['test'].num_examples\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "BATCH_SIZE_PER_REPLICA = 64\n",
    "BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# スケーリング\n",
    "def scale(image, label):\n",
    "  image = tf.cast(image, tf.float32)\n",
    "  image /= 255\n",
    "\n",
    "  return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = mnist_train.map(scale).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "eval_dataset = mnist_test.map(scale).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From C:\\Users\\tnaka\\anaconda3\\envs\\dic\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\nInstructions for updating:\nIf using Keras pass *_constraint arguments to layers.\nWARNING:tensorflow:From C:\\Users\\tnaka\\anaconda3\\envs\\dic\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\nInstructions for updating:\nIf using Keras pass *_constraint arguments to layers.\n"
    }
   ],
   "source": [
    "# model作成\n",
    "# strategyのスコープ内で作成することで計算をうまく複数デバイスに分散してくれる\n",
    "with strategy.scope():\n",
    "  model = tf.keras.Sequential([\n",
    "      tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\n",
    "      tf.keras.layers.MaxPooling2D(),\n",
    "      tf.keras.layers.Flatten(),\n",
    "      tf.keras.layers.Dense(64, activation='relu'),\n",
    "      tf.keras.layers.Dense(10)\n",
    "  ])\n",
    "\n",
    "  model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint作成\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習率をエポックごとに変更する\n",
    "def decay(epoch):\n",
    "  if epoch < 3:\n",
    "    return 1e-3\n",
    "  elif epoch >= 3 and epoch < 7:\n",
    "    return 1e-4\n",
    "  else:\n",
    "    return 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習率を表示する\n",
    "class PrintLR(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    print('\\nLearning rate for epoch {} is {}'.format(epoch + 1,\n",
    "                                                      model.optimizer.lr.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コールバック設定\n",
    "callbacks = [\n",
    "    #tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,\n",
    "                                       save_weights_only=True),\n",
    "    tf.keras.callbacks.LearningRateScheduler(decay),\n",
    "    PrintLR()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習\n",
    "# model.fit(train_dataset, epochs=12, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> localだとうまく動かない/CPU1基のみなのでGoogle Colabを利用: ./keras_distribute.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題2】\n",
    "#### （アドバンス課題）様々な手法を実行\n",
    "TensorFLowやGoogle AI ResearchのGitHubリポジトリには、定番のモデルから最新のモデルまで多様なコードが公開されています。これらから興味あるものを選び実行してください。\n",
    "\n",
    "\n",
    "なお、これらのコードは初学者向けではないため、巨大なデータセットのダウンロードが必要な場合など、実行が簡単ではないこともあります。そういった場合は、コードリーディングを行ってください。\n",
    "\n",
    "\n",
    "[models/research at master · tensorflow/models]()\n",
    "\n",
    "\n",
    "[google-research/google-research: Google AI Research]()\n",
    "\n",
    "\n",
    "更新日が古いものはPythonやTensorFlowのバージョンが古く、扱いずらい場合があります。新しいものから見ることを推奨します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.異なるフレームワークへの書き換え\n",
    "\n",
    "「ディープラーニングフレームワーク1」で作成した4種類のデータセットを扱うTensorFLowのコードを異なるフレームワークに変更していきます。\n",
    "\n",
    "\n",
    "- Iris（Iris-versicolorとIris-virginicaのみの2値分類）\n",
    "- Iris（3種類全ての目的変数を使用して多値分類）\n",
    "- House Prices\n",
    "- MNIST\n",
    "\n",
    "### Kerasへの書き換え\n",
    "KerasはTensorFLowに含まれるtf.kerasモジュールを使用してください。\n",
    "\n",
    "\n",
    "KerasにはSequentialモデルかFunctional APIかなど書き方に種類がありますが、これは指定しません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題3】\n",
    "#### Iris（2値分類）をKerasで学習\n",
    "TensorFlowによるIrisデータセットに対する2値分類をKerasに書き換えてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "      Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm  \\\n0      1            5.1           3.5            1.4           0.2   \n1      2            4.9           3.0            1.4           0.2   \n2      3            4.7           3.2            1.3           0.2   \n3      4            4.6           3.1            1.5           0.2   \n4      5            5.0           3.6            1.4           0.2   \n..   ...            ...           ...            ...           ...   \n145  146            6.7           3.0            5.2           2.3   \n146  147            6.3           2.5            5.0           1.9   \n147  148            6.5           3.0            5.2           2.0   \n148  149            6.2           3.4            5.4           2.3   \n149  150            5.9           3.0            5.1           1.8   \n\n            Species  \n0       Iris-setosa  \n1       Iris-setosa  \n2       Iris-setosa  \n3       Iris-setosa  \n4       Iris-setosa  \n..              ...  \n145  Iris-virginica  \n146  Iris-virginica  \n147  Iris-virginica  \n148  Iris-virginica  \n149  Iris-virginica  \n\n[150 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>SepalLengthCm</th>\n      <th>SepalWidthCm</th>\n      <th>PetalLengthCm</th>\n      <th>PetalWidthCm</th>\n      <th>Species</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>5.1</td>\n      <td>3.5</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>4.9</td>\n      <td>3.0</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>4.7</td>\n      <td>3.2</td>\n      <td>1.3</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>4.6</td>\n      <td>3.1</td>\n      <td>1.5</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>5.0</td>\n      <td>3.6</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>146</td>\n      <td>6.7</td>\n      <td>3.0</td>\n      <td>5.2</td>\n      <td>2.3</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>146</th>\n      <td>147</td>\n      <td>6.3</td>\n      <td>2.5</td>\n      <td>5.0</td>\n      <td>1.9</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>148</td>\n      <td>6.5</td>\n      <td>3.0</td>\n      <td>5.2</td>\n      <td>2.0</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>148</th>\n      <td>149</td>\n      <td>6.2</td>\n      <td>3.4</td>\n      <td>5.4</td>\n      <td>2.3</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>149</th>\n      <td>150</td>\n      <td>5.9</td>\n      <td>3.0</td>\n      <td>5.1</td>\n      <td>1.8</td>\n      <td>Iris-virginica</td>\n    </tr>\n  </tbody>\n</table>\n<p>150 rows × 6 columns</p>\n</div>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "(100, 4)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "(100, 1)"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(64, 4) (64, 1)\n(16, 4) (16, 1)\n(20, 4) (20, 1)\n"
    }
   ],
   "source": [
    "# dataset読み込み\n",
    "df_iris = pd.read_csv('Iris.csv')\n",
    "display(df_iris)\n",
    "X = np.array(df_iris[(df_iris['Species'] == 'Iris-versicolor')|(df_iris['Species'] == 'Iris-virginica')].iloc[:, 1:5])\n",
    "y = np.array(df_iris[(df_iris['Species'] == 'Iris-versicolor')|(df_iris['Species'] == 'Iris-virginica')].iloc[:, 5])\n",
    "y[y=='Iris-versicolor'] = 0\n",
    "y[y=='Iris-virginica'] = 1\n",
    "y = y.astype(np.int)[:, np.newaxis]\n",
    "display(X.shape)\n",
    "display(y.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense (Dense)                (None, 10)                50        \n_________________________________________________________________\ndense_1 (Dense)              (None, 5)                 55        \n_________________________________________________________________\ndense_2 (Dense)              (None, 1)                 6         \n=================================================================\nTotal params: 111\nTrainable params: 111\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "K.clear_session()\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(10, activation = tf.nn.relu, input_shape=(4,)))\n",
    "model.add(tf.keras.layers.Dense(5, activation = tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(1, activation = tf.nn.sigmoid))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From C:\\Users\\tnaka\\anaconda3\\envs\\dic\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.where in 2.0, which has the same broadcast rule as np.where\nWARNING:tensorflow:From C:\\Users\\tnaka\\anaconda3\\envs\\dic\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.where in 2.0, which has the same broadcast rule as np.where\nTrain on 64 samples, validate on 16 samples\nEpoch 1/10\n64/64 [==============================] - 0s 6ms/sample - loss: 0.3010 - acc: 0.9219 - val_loss: 0.0348 - val_acc: 1.0000\nEpoch 2/10\n64/64 [==============================] - 0s 2ms/sample - loss: 0.0922 - acc: 0.9531 - val_loss: 0.0353 - val_acc: 1.0000\nEpoch 3/10\n64/64 [==============================] - 0s 2ms/sample - loss: 0.0801 - acc: 0.9688 - val_loss: 0.0846 - val_acc: 0.9375\nEpoch 4/10\n64/64 [==============================] - 0s 1ms/sample - loss: 0.0417 - acc: 0.9844 - val_loss: 0.0076 - val_acc: 1.0000\nEpoch 5/10\n64/64 [==============================] - 0s 1ms/sample - loss: 0.0247 - acc: 1.0000 - val_loss: 0.0068 - val_acc: 1.0000\nEpoch 6/10\n64/64 [==============================] - 0s 2ms/sample - loss: 0.0108 - acc: 1.0000 - val_loss: 0.0377 - val_acc: 1.0000\nEpoch 7/10\n64/64 [==============================] - 0s 2ms/sample - loss: 0.0036 - acc: 1.0000 - val_loss: 0.0066 - val_acc: 1.0000\nEpoch 8/10\n64/64 [==============================] - 0s 2ms/sample - loss: 0.0027 - acc: 1.0000 - val_loss: 0.0089 - val_acc: 1.0000\nEpoch 9/10\n64/64 [==============================] - 0s 2ms/sample - loss: 0.0014 - acc: 1.0000 - val_loss: 0.0060 - val_acc: 1.0000\nEpoch 10/10\n64/64 [==============================] - 0s 1ms/sample - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0053 - val_acc: 1.0000\n"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=tf.train.AdamOptimizer(learning_rate=0.03),\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=1,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "y_pred_proba [5.9217215e-05 1.0000000e+00 8.0764294e-06 1.0000000e+00 9.9999845e-01\n 1.0000000e+00 7.8290701e-05 9.9985981e-01 1.0000000e+00 1.0000000e+00\n 9.9999952e-01 9.9999988e-01 1.0000000e+00 2.1111965e-04 0.0000000e+00\n 0.0000000e+00 8.0413735e-01 0.0000000e+00 9.9963063e-01 2.0861626e-07]\ny_pred [0 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 1 0 1 0]\ny_test [0 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 0 0 0 0]\n"
    }
   ],
   "source": [
    "y_pred_proba = model.predict(X_test)[:, 0]\n",
    "y_pred = np.where(y_pred_proba >0.5, 1, 0)\n",
    "print(\"y_pred_proba\", y_pred_proba)\n",
    "print(\"y_pred\", y_pred)\n",
    "print(\"y_test\", y_test.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題4】\n",
    "#### Iris（多値分類）をKerasで学習\n",
    "TensorFlowによるIrisデータセットに対する3値分類をKerasに書き換えてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "(150, 4)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "(150, 3)"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(96, 4) (96, 3)\n(24, 4) (24, 3)\n(30, 4) (30, 3)\n"
    }
   ],
   "source": [
    "# 抽出\n",
    "X = np.array(df_iris.iloc[:, 1:5])\n",
    "y = np.array(pd.get_dummies(df_iris.iloc[:, 5]))\n",
    "y = y.astype(np.int)\n",
    "display(X.shape)\n",
    "display(y.shape)\n",
    "\n",
    "# 分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "# 標準化\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense (Dense)                (None, 100)               500       \n_________________________________________________________________\ndense_1 (Dense)              (None, 10)                1010      \n_________________________________________________________________\ndense_2 (Dense)              (None, 3)                 33        \n=================================================================\nTotal params: 1,543\nTrainable params: 1,543\nNon-trainable params: 0\n_________________________________________________________________\nTrain on 96 samples, validate on 24 samples\nEpoch 1/10\n96/96 [==============================] - 0s 3ms/sample - loss: 0.6409 - acc: 0.6042 - val_loss: 0.5143 - val_acc: 0.7083\nEpoch 2/10\n96/96 [==============================] - 0s 1ms/sample - loss: 0.5490 - acc: 0.6771 - val_loss: 0.4884 - val_acc: 0.7083\nEpoch 3/10\n96/96 [==============================] - 0s 1ms/sample - loss: 0.5329 - acc: 0.6354 - val_loss: 0.4833 - val_acc: 0.6250\nEpoch 4/10\n96/96 [==============================] - 0s 1ms/sample - loss: 0.5306 - acc: 0.6562 - val_loss: 0.4753 - val_acc: 0.7083\nEpoch 5/10\n96/96 [==============================] - 0s 2ms/sample - loss: 0.5285 - acc: 0.6667 - val_loss: 0.4790 - val_acc: 0.6250\nEpoch 6/10\n96/96 [==============================] - 0s 1ms/sample - loss: 0.5320 - acc: 0.5938 - val_loss: 0.4708 - val_acc: 0.7083\nEpoch 7/10\n96/96 [==============================] - 0s 1ms/sample - loss: 0.5343 - acc: 0.6250 - val_loss: 0.4755 - val_acc: 0.6250\nEpoch 8/10\n96/96 [==============================] - 0s 1ms/sample - loss: 0.5298 - acc: 0.6667 - val_loss: 0.4716 - val_acc: 0.7083\nEpoch 9/10\n96/96 [==============================] - 0s 1ms/sample - loss: 0.5235 - acc: 0.6979 - val_loss: 0.4710 - val_acc: 0.7083\nEpoch 10/10\n96/96 [==============================] - 0s 1ms/sample - loss: 0.5346 - acc: 0.6771 - val_loss: 0.4681 - val_acc: 0.7083\n"
    }
   ],
   "source": [
    "# モデル構築\n",
    "K.clear_session()\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(100, activation = tf.nn.relu, input_shape=(4,)))\n",
    "model.add(tf.keras.layers.Dense(10, activation = tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(3, activation = tf.nn.softmax))\n",
    "model.summary()\n",
    "\n",
    "# コンパイル\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=tf.train.AdamOptimizer(learning_rate=0.03),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 学習\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=1,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "y_pred_proba [[1.6351793e-02 4.4424191e-01 5.3940624e-01]\n [1.6351793e-02 4.4424191e-01 5.3940624e-01]\n [1.0000000e+00 1.0054625e-11 5.1776939e-15]\n [1.6351793e-02 4.4424191e-01 5.3940624e-01]\n [1.0000000e+00 5.2044641e-08 2.9599864e-10]\n [1.6351793e-02 4.4424191e-01 5.3940624e-01]\n [1.0000000e+00 2.2560716e-08 1.0480896e-10]\n [1.6351793e-02 4.4424191e-01 5.3940624e-01]\n [1.6351793e-02 4.4424191e-01 5.3940624e-01]\n [1.6351793e-02 4.4424191e-01 5.3940624e-01]\n [1.6351793e-02 4.4424191e-01 5.3940624e-01]\n [1.6351793e-02 4.4424191e-01 5.3940624e-01]\n [1.6351793e-02 4.4424191e-01 5.3940624e-01]\n [1.6351793e-02 4.4424191e-01 5.3940624e-01]\n [1.6351793e-02 4.4424191e-01 5.3940624e-01]\n [9.9999928e-01 7.0836012e-07 7.3318369e-09]\n [1.6351793e-02 4.4424191e-01 5.3940624e-01]\n [1.6351793e-02 4.4424191e-01 5.3940624e-01]\n [9.9996328e-01 3.5708748e-05 9.5527037e-07]\n [1.0000000e+00 1.1195316e-09 2.2083496e-12]\n [1.6351793e-02 4.4424191e-01 5.3940624e-01]\n [1.6351793e-02 4.4424191e-01 5.3940624e-01]\n [9.9999988e-01 1.5461842e-07 1.1763377e-09]\n [9.9977142e-01 2.2027746e-04 8.2983170e-06]\n [1.6351793e-02 4.4424191e-01 5.3940624e-01]\n [1.0000000e+00 7.9198686e-10 1.5334035e-12]\n [1.0000000e+00 1.5789622e-08 6.7764808e-11]\n [1.6351793e-02 4.4424191e-01 5.3940624e-01]\n [1.6351793e-02 4.4424191e-01 5.3940624e-01]\n [9.9999952e-01 4.5001516e-07 4.6416906e-09]]\ny_pred [2 2 0 2 0 2 0 2 2 2 2 2 2 2 2 0 2 2 0 0 2 2 0 0 2 0 0 2 2 0]\ny_test [2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0]\n"
    }
   ],
   "source": [
    "y_pred_proba = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "print(\"y_pred_proba\", y_pred_proba)\n",
    "print(\"y_pred\", y_pred)\n",
    "print(\"y_test\", np.argmax(y_test, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題5】\n",
    "#### House PricesをKerasで学習\n",
    "TensorFlowによるHouse Pricesデータセットに対する回帰をKerasに書き換えてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "        Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n0        1          60       RL         65.0     8450   Pave   NaN      Reg   \n1        2          20       RL         80.0     9600   Pave   NaN      Reg   \n2        3          60       RL         68.0    11250   Pave   NaN      IR1   \n3        4          70       RL         60.0     9550   Pave   NaN      IR1   \n4        5          60       RL         84.0    14260   Pave   NaN      IR1   \n...    ...         ...      ...          ...      ...    ...   ...      ...   \n1455  1456          60       RL         62.0     7917   Pave   NaN      Reg   \n1456  1457          20       RL         85.0    13175   Pave   NaN      Reg   \n1457  1458          70       RL         66.0     9042   Pave   NaN      Reg   \n1458  1459          20       RL         68.0     9717   Pave   NaN      Reg   \n1459  1460          20       RL         75.0     9937   Pave   NaN      Reg   \n\n     LandContour Utilities  ... PoolArea PoolQC  Fence MiscFeature MiscVal  \\\n0            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n1            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n2            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n3            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n4            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n...          ...       ...  ...      ...    ...    ...         ...     ...   \n1455         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n1456         Lvl    AllPub  ...        0    NaN  MnPrv         NaN       0   \n1457         Lvl    AllPub  ...        0    NaN  GdPrv        Shed    2500   \n1458         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n1459         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n\n     MoSold YrSold  SaleType  SaleCondition  SalePrice  \n0         2   2008        WD         Normal     208500  \n1         5   2007        WD         Normal     181500  \n2         9   2008        WD         Normal     223500  \n3         2   2006        WD        Abnorml     140000  \n4        12   2008        WD         Normal     250000  \n...     ...    ...       ...            ...        ...  \n1455      8   2007        WD         Normal     175000  \n1456      2   2010        WD         Normal     210000  \n1457      5   2010        WD         Normal     266500  \n1458      4   2010        WD         Normal     142125  \n1459      6   2008        WD         Normal     147500  \n\n[1460 rows x 81 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>MSSubClass</th>\n      <th>MSZoning</th>\n      <th>LotFrontage</th>\n      <th>LotArea</th>\n      <th>Street</th>\n      <th>Alley</th>\n      <th>LotShape</th>\n      <th>LandContour</th>\n      <th>Utilities</th>\n      <th>...</th>\n      <th>PoolArea</th>\n      <th>PoolQC</th>\n      <th>Fence</th>\n      <th>MiscFeature</th>\n      <th>MiscVal</th>\n      <th>MoSold</th>\n      <th>YrSold</th>\n      <th>SaleType</th>\n      <th>SaleCondition</th>\n      <th>SalePrice</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>65.0</td>\n      <td>8450</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>208500</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>20</td>\n      <td>RL</td>\n      <td>80.0</td>\n      <td>9600</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>5</td>\n      <td>2007</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>181500</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>68.0</td>\n      <td>11250</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>9</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>223500</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>70</td>\n      <td>RL</td>\n      <td>60.0</td>\n      <td>9550</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2006</td>\n      <td>WD</td>\n      <td>Abnorml</td>\n      <td>140000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>84.0</td>\n      <td>14260</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>12</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>250000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1455</th>\n      <td>1456</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>62.0</td>\n      <td>7917</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>8</td>\n      <td>2007</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>175000</td>\n    </tr>\n    <tr>\n      <th>1456</th>\n      <td>1457</td>\n      <td>20</td>\n      <td>RL</td>\n      <td>85.0</td>\n      <td>13175</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>MnPrv</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2010</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>210000</td>\n    </tr>\n    <tr>\n      <th>1457</th>\n      <td>1458</td>\n      <td>70</td>\n      <td>RL</td>\n      <td>66.0</td>\n      <td>9042</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>GdPrv</td>\n      <td>Shed</td>\n      <td>2500</td>\n      <td>5</td>\n      <td>2010</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>266500</td>\n    </tr>\n    <tr>\n      <th>1458</th>\n      <td>1459</td>\n      <td>20</td>\n      <td>RL</td>\n      <td>68.0</td>\n      <td>9717</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>4</td>\n      <td>2010</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>142125</td>\n    </tr>\n    <tr>\n      <th>1459</th>\n      <td>1460</td>\n      <td>20</td>\n      <td>RL</td>\n      <td>75.0</td>\n      <td>9937</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>6</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>147500</td>\n    </tr>\n  </tbody>\n</table>\n<p>1460 rows × 81 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# 読み込み\n",
    "df_price = pd.read_csv('train.csv')\n",
    "display(df_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "(1460, 3)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "(1460, 1)"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(934, 3) (934, 1)\n(234, 3) (234, 1)\n(292, 3) (292, 1)\n"
    }
   ],
   "source": [
    "# 抽出\n",
    "X = df_price[['LotArea', 'GrLivArea', 'YearBuilt']].values\n",
    "y = df_price['SalePrice'].values[:, np.newaxis]\n",
    "display(X.shape)\n",
    "display(y.shape)\n",
    "\n",
    "# 分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "# 標準化\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# 対数変換\n",
    "y_train = np.log(y_train)\n",
    "y_val = np.log(y_val)\n",
    "y_test = np.log(y_test)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense (Dense)                (None, 10)                40        \n_________________________________________________________________\ndense_1 (Dense)              (None, 3)                 33        \n_________________________________________________________________\ndense_2 (Dense)              (None, 1)                 4         \n=================================================================\nTotal params: 77\nTrainable params: 77\nNon-trainable params: 0\n_________________________________________________________________\nTrain on 934 samples, validate on 234 samples\nEpoch 1/100\n934/934 [==============================] - 0s 233us/sample - loss: 143.7683 - mean_absolute_error: 11.9846 - val_loss: 141.8503 - val_mean_absolute_error: 11.9059\nEpoch 2/100\n934/934 [==============================] - 0s 87us/sample - loss: 140.9924 - mean_absolute_error: 11.8700 - val_loss: 139.0272 - val_mean_absolute_error: 11.7880\nEpoch 3/100\n934/934 [==============================] - 0s 46us/sample - loss: 137.9786 - mean_absolute_error: 11.7432 - val_loss: 135.7180 - val_mean_absolute_error: 11.6474\nEpoch 4/100\n934/934 [==============================] - 0s 48us/sample - loss: 134.4463 - mean_absolute_error: 11.5915 - val_loss: 131.7908 - val_mean_absolute_error: 11.4769\nEpoch 5/100\n934/934 [==============================] - 0s 58us/sample - loss: 130.1424 - mean_absolute_error: 11.4022 - val_loss: 126.9551 - val_mean_absolute_error: 11.2617\nEpoch 6/100\n934/934 [==============================] - 0s 46us/sample - loss: 124.5631 - mean_absolute_error: 11.1507 - val_loss: 120.4840 - val_mean_absolute_error: 10.9660\nEpoch 7/100\n934/934 [==============================] - 0s 47us/sample - loss: 117.7561 - mean_absolute_error: 10.8317 - val_loss: 113.5672 - val_mean_absolute_error: 10.6376\nEpoch 8/100\n934/934 [==============================] - 0s 44us/sample - loss: 110.5467 - mean_absolute_error: 10.4800 - val_loss: 106.0638 - val_mean_absolute_error: 10.2648\nEpoch 9/100\n934/934 [==============================] - 0s 46us/sample - loss: 102.6922 - mean_absolute_error: 10.0785 - val_loss: 97.8423 - val_mean_absolute_error: 9.8345\nEpoch 10/100\n934/934 [==============================] - 0s 43us/sample - loss: 94.1938 - mean_absolute_error: 9.6243 - val_loss: 89.0695 - val_mean_absolute_error: 9.3489\nEpoch 11/100\n934/934 [==============================] - 0s 57us/sample - loss: 85.1021 - mean_absolute_error: 9.1178 - val_loss: 79.6432 - val_mean_absolute_error: 8.7916\nEpoch 12/100\n934/934 [==============================] - 0s 46us/sample - loss: 75.4575 - mean_absolute_error: 8.5430 - val_loss: 69.7376 - val_mean_absolute_error: 8.1606\nEpoch 13/100\n934/934 [==============================] - 0s 43us/sample - loss: 65.3590 - mean_absolute_error: 7.8963 - val_loss: 59.5708 - val_mean_absolute_error: 7.4670\nEpoch 14/100\n934/934 [==============================] - 0s 42us/sample - loss: 55.0152 - mean_absolute_error: 7.1829 - val_loss: 49.2901 - val_mean_absolute_error: 6.7207\nEpoch 15/100\n934/934 [==============================] - 0s 46us/sample - loss: 44.8248 - mean_absolute_error: 6.4067 - val_loss: 39.4732 - val_mean_absolute_error: 5.9583\nEpoch 16/100\n934/934 [==============================] - 0s 46us/sample - loss: 35.3880 - mean_absolute_error: 5.6276 - val_loss: 30.6597 - val_mean_absolute_error: 5.2031\nEpoch 17/100\n934/934 [==============================] - 0s 46us/sample - loss: 27.0028 - mean_absolute_error: 4.8565 - val_loss: 23.0551 - val_mean_absolute_error: 4.4797\nEpoch 18/100\n934/934 [==============================] - 0s 41us/sample - loss: 20.0598 - mean_absolute_error: 4.1126 - val_loss: 16.8239 - val_mean_absolute_error: 3.7830\nEpoch 19/100\n934/934 [==============================] - 0s 47us/sample - loss: 14.5914 - mean_absolute_error: 3.4230 - val_loss: 12.1429 - val_mean_absolute_error: 3.1519\nEpoch 20/100\n934/934 [==============================] - 0s 45us/sample - loss: 10.7157 - mean_absolute_error: 2.8510 - val_loss: 9.0223 - val_mean_absolute_error: 2.6542\nEpoch 21/100\n934/934 [==============================] - 0s 44us/sample - loss: 8.1711 - mean_absolute_error: 2.4052 - val_loss: 6.9174 - val_mean_absolute_error: 2.2682\nEpoch 22/100\n934/934 [==============================] - 0s 45us/sample - loss: 6.5484 - mean_absolute_error: 2.0926 - val_loss: 5.6165 - val_mean_absolute_error: 1.9915\nEpoch 23/100\n934/934 [==============================] - 0s 52us/sample - loss: 5.5766 - mean_absolute_error: 1.8892 - val_loss: 4.8504 - val_mean_absolute_error: 1.8288\nEpoch 24/100\n934/934 [==============================] - 0s 46us/sample - loss: 4.9794 - mean_absolute_error: 1.7585 - val_loss: 4.3470 - val_mean_absolute_error: 1.7138\nEpoch 25/100\n934/934 [==============================] - 0s 42us/sample - loss: 4.5757 - mean_absolute_error: 1.6672 - val_loss: 4.0123 - val_mean_absolute_error: 1.6339\nEpoch 26/100\n934/934 [==============================] - 0s 51us/sample - loss: 4.2423 - mean_absolute_error: 1.5903 - val_loss: 3.7186 - val_mean_absolute_error: 1.5623\nEpoch 27/100\n934/934 [==============================] - 0s 50us/sample - loss: 3.9690 - mean_absolute_error: 1.5287 - val_loss: 3.4783 - val_mean_absolute_error: 1.5048\nEpoch 28/100\n934/934 [==============================] - 0s 56us/sample - loss: 3.7187 - mean_absolute_error: 1.4700 - val_loss: 3.2530 - val_mean_absolute_error: 1.4482\nEpoch 29/100\n934/934 [==============================] - 0s 43us/sample - loss: 3.4871 - mean_absolute_error: 1.4188 - val_loss: 3.0507 - val_mean_absolute_error: 1.3992\nEpoch 30/100\n934/934 [==============================] - 0s 50us/sample - loss: 3.2594 - mean_absolute_error: 1.3719 - val_loss: 2.8845 - val_mean_absolute_error: 1.3565\nEpoch 31/100\n934/934 [==============================] - 0s 43us/sample - loss: 3.0652 - mean_absolute_error: 1.3258 - val_loss: 2.6988 - val_mean_absolute_error: 1.3083\nEpoch 32/100\n934/934 [==============================] - 0s 42us/sample - loss: 2.8677 - mean_absolute_error: 1.2763 - val_loss: 2.5140 - val_mean_absolute_error: 1.2576\nEpoch 33/100\n934/934 [==============================] - 0s 43us/sample - loss: 2.6845 - mean_absolute_error: 1.2318 - val_loss: 2.3677 - val_mean_absolute_error: 1.2180\nEpoch 34/100\n934/934 [==============================] - 0s 53us/sample - loss: 2.5184 - mean_absolute_error: 1.1878 - val_loss: 2.2039 - val_mean_absolute_error: 1.1716\nEpoch 35/100\n934/934 [==============================] - 0s 43us/sample - loss: 2.3533 - mean_absolute_error: 1.1450 - val_loss: 2.0533 - val_mean_absolute_error: 1.1265\nEpoch 36/100\n934/934 [==============================] - 0s 61us/sample - loss: 2.2040 - mean_absolute_error: 1.1035 - val_loss: 1.9097 - val_mean_absolute_error: 1.0841\nEpoch 37/100\n934/934 [==============================] - 0s 47us/sample - loss: 2.0561 - mean_absolute_error: 1.0605 - val_loss: 1.7704 - val_mean_absolute_error: 1.0408\nEpoch 38/100\n934/934 [==============================] - 0s 41us/sample - loss: 1.9270 - mean_absolute_error: 1.0213 - val_loss: 1.6535 - val_mean_absolute_error: 1.0043\nEpoch 39/100\n934/934 [==============================] - 0s 44us/sample - loss: 1.8007 - mean_absolute_error: 0.9849 - val_loss: 1.5337 - val_mean_absolute_error: 0.9656\nEpoch 40/100\n934/934 [==============================] - 0s 43us/sample - loss: 1.6881 - mean_absolute_error: 0.9502 - val_loss: 1.4351 - val_mean_absolute_error: 0.9321\nEpoch 41/100\n934/934 [==============================] - 0s 43us/sample - loss: 1.5781 - mean_absolute_error: 0.9183 - val_loss: 1.3381 - val_mean_absolute_error: 0.8996\nEpoch 42/100\n934/934 [==============================] - 0s 46us/sample - loss: 1.4816 - mean_absolute_error: 0.8855 - val_loss: 1.2569 - val_mean_absolute_error: 0.8697\nEpoch 43/100\n934/934 [==============================] - 0s 44us/sample - loss: 1.3910 - mean_absolute_error: 0.8576 - val_loss: 1.1723 - val_mean_absolute_error: 0.8385\nEpoch 44/100\n934/934 [==============================] - 0s 46us/sample - loss: 1.3081 - mean_absolute_error: 0.8290 - val_loss: 1.0982 - val_mean_absolute_error: 0.8100\nEpoch 45/100\n934/934 [==============================] - 0s 42us/sample - loss: 1.2252 - mean_absolute_error: 0.7979 - val_loss: 1.0192 - val_mean_absolute_error: 0.7787\nEpoch 46/100\n934/934 [==============================] - 0s 43us/sample - loss: 1.1479 - mean_absolute_error: 0.7688 - val_loss: 0.9484 - val_mean_absolute_error: 0.7512\nEpoch 47/100\n934/934 [==============================] - 0s 43us/sample - loss: 1.0755 - mean_absolute_error: 0.7442 - val_loss: 0.8914 - val_mean_absolute_error: 0.7272\nEpoch 48/100\n934/934 [==============================] - 0s 51us/sample - loss: 1.0051 - mean_absolute_error: 0.7178 - val_loss: 0.8272 - val_mean_absolute_error: 0.6988\nEpoch 49/100\n934/934 [==============================] - 0s 43us/sample - loss: 0.9409 - mean_absolute_error: 0.6905 - val_loss: 0.7694 - val_mean_absolute_error: 0.6731\nEpoch 50/100\n934/934 [==============================] - 0s 78us/sample - loss: 0.8813 - mean_absolute_error: 0.6638 - val_loss: 0.7185 - val_mean_absolute_error: 0.6492\nEpoch 51/100\n934/934 [==============================] - 0s 41us/sample - loss: 0.8243 - mean_absolute_error: 0.6415 - val_loss: 0.6750 - val_mean_absolute_error: 0.6277\nEpoch 52/100\n934/934 [==============================] - 0s 45us/sample - loss: 0.7705 - mean_absolute_error: 0.6158 - val_loss: 0.6272 - val_mean_absolute_error: 0.6036\nEpoch 53/100\n934/934 [==============================] - 0s 46us/sample - loss: 0.7185 - mean_absolute_error: 0.5917 - val_loss: 0.5817 - val_mean_absolute_error: 0.5802\nEpoch 54/100\n934/934 [==============================] - 0s 50us/sample - loss: 0.6707 - mean_absolute_error: 0.5641 - val_loss: 0.5347 - val_mean_absolute_error: 0.5548\nEpoch 55/100\n934/934 [==============================] - 0s 43us/sample - loss: 0.6331 - mean_absolute_error: 0.5512 - val_loss: 0.5205 - val_mean_absolute_error: 0.5467\nEpoch 56/100\n934/934 [==============================] - 0s 47us/sample - loss: 0.5747 - mean_absolute_error: 0.5274 - val_loss: 0.4760 - val_mean_absolute_error: 0.5216\nEpoch 57/100\n934/934 [==============================] - 0s 44us/sample - loss: 0.5348 - mean_absolute_error: 0.5052 - val_loss: 0.4364 - val_mean_absolute_error: 0.4988\nEpoch 58/100\n934/934 [==============================] - 0s 46us/sample - loss: 0.4995 - mean_absolute_error: 0.4847 - val_loss: 0.4015 - val_mean_absolute_error: 0.4781\nEpoch 59/100\n934/934 [==============================] - 0s 46us/sample - loss: 0.4671 - mean_absolute_error: 0.4647 - val_loss: 0.3712 - val_mean_absolute_error: 0.4594\nEpoch 60/100\n934/934 [==============================] - 0s 45us/sample - loss: 0.4362 - mean_absolute_error: 0.4491 - val_loss: 0.3488 - val_mean_absolute_error: 0.4451\nEpoch 61/100\n934/934 [==============================] - 0s 45us/sample - loss: 0.4105 - mean_absolute_error: 0.4341 - val_loss: 0.3254 - val_mean_absolute_error: 0.4297\nEpoch 62/100\n934/934 [==============================] - 0s 47us/sample - loss: 0.3839 - mean_absolute_error: 0.4172 - val_loss: 0.2987 - val_mean_absolute_error: 0.4111\nEpoch 63/100\n934/934 [==============================] - 0s 45us/sample - loss: 0.3586 - mean_absolute_error: 0.4026 - val_loss: 0.2817 - val_mean_absolute_error: 0.3994\nEpoch 64/100\n934/934 [==============================] - 0s 44us/sample - loss: 0.3351 - mean_absolute_error: 0.3869 - val_loss: 0.2607 - val_mean_absolute_error: 0.3831\nEpoch 65/100\n934/934 [==============================] - 0s 42us/sample - loss: 0.3135 - mean_absolute_error: 0.3727 - val_loss: 0.2449 - val_mean_absolute_error: 0.3727\nEpoch 66/100\n934/934 [==============================] - 0s 46us/sample - loss: 0.2945 - mean_absolute_error: 0.3628 - val_loss: 0.2301 - val_mean_absolute_error: 0.3609\nEpoch 67/100\n934/934 [==============================] - 0s 43us/sample - loss: 0.2792 - mean_absolute_error: 0.3526 - val_loss: 0.2189 - val_mean_absolute_error: 0.3516\nEpoch 68/100\n934/934 [==============================] - 0s 45us/sample - loss: 0.2665 - mean_absolute_error: 0.3403 - val_loss: 0.2069 - val_mean_absolute_error: 0.3423\nEpoch 69/100\n934/934 [==============================] - 0s 45us/sample - loss: 0.2553 - mean_absolute_error: 0.3377 - val_loss: 0.2031 - val_mean_absolute_error: 0.3400\nEpoch 70/100\n934/934 [==============================] - 0s 88us/sample - loss: 0.2455 - mean_absolute_error: 0.3278 - val_loss: 0.1910 - val_mean_absolute_error: 0.3278\nEpoch 71/100\n934/934 [==============================] - 0s 46us/sample - loss: 0.2333 - mean_absolute_error: 0.3228 - val_loss: 0.1852 - val_mean_absolute_error: 0.3236\nEpoch 72/100\n934/934 [==============================] - 0s 45us/sample - loss: 0.2254 - mean_absolute_error: 0.3177 - val_loss: 0.1811 - val_mean_absolute_error: 0.3192\nEpoch 73/100\n934/934 [==============================] - 0s 49us/sample - loss: 0.2179 - mean_absolute_error: 0.3124 - val_loss: 0.1715 - val_mean_absolute_error: 0.3109\nEpoch 74/100\n934/934 [==============================] - 0s 67us/sample - loss: 0.2089 - mean_absolute_error: 0.3077 - val_loss: 0.1690 - val_mean_absolute_error: 0.3088\nEpoch 75/100\n934/934 [==============================] - 0s 48us/sample - loss: 0.2015 - mean_absolute_error: 0.3036 - val_loss: 0.1617 - val_mean_absolute_error: 0.3012\nEpoch 76/100\n934/934 [==============================] - 0s 44us/sample - loss: 0.1947 - mean_absolute_error: 0.2996 - val_loss: 0.1589 - val_mean_absolute_error: 0.2988\nEpoch 77/100\n934/934 [==============================] - 0s 43us/sample - loss: 0.1898 - mean_absolute_error: 0.2935 - val_loss: 0.1516 - val_mean_absolute_error: 0.2914\nEpoch 78/100\n934/934 [==============================] - 0s 42us/sample - loss: 0.1832 - mean_absolute_error: 0.2908 - val_loss: 0.1487 - val_mean_absolute_error: 0.2890\nEpoch 79/100\n934/934 [==============================] - 0s 46us/sample - loss: 0.1756 - mean_absolute_error: 0.2856 - val_loss: 0.1428 - val_mean_absolute_error: 0.2831\nEpoch 80/100\n934/934 [==============================] - 0s 46us/sample - loss: 0.1701 - mean_absolute_error: 0.2812 - val_loss: 0.1391 - val_mean_absolute_error: 0.2796\nEpoch 81/100\n934/934 [==============================] - 0s 49us/sample - loss: 0.1650 - mean_absolute_error: 0.2771 - val_loss: 0.1323 - val_mean_absolute_error: 0.2719\nEpoch 82/100\n934/934 [==============================] - 0s 45us/sample - loss: 0.1615 - mean_absolute_error: 0.2735 - val_loss: 0.1294 - val_mean_absolute_error: 0.2699\nEpoch 83/100\n934/934 [==============================] - 0s 42us/sample - loss: 0.1590 - mean_absolute_error: 0.2779 - val_loss: 0.1374 - val_mean_absolute_error: 0.2758\nEpoch 84/100\n934/934 [==============================] - 0s 82us/sample - loss: 0.1523 - mean_absolute_error: 0.2708 - val_loss: 0.1266 - val_mean_absolute_error: 0.2654\nEpoch 85/100\n934/934 [==============================] - 0s 50us/sample - loss: 0.1467 - mean_absolute_error: 0.2657 - val_loss: 0.1210 - val_mean_absolute_error: 0.2606\nEpoch 86/100\n934/934 [==============================] - 0s 56us/sample - loss: 0.1431 - mean_absolute_error: 0.2617 - val_loss: 0.1180 - val_mean_absolute_error: 0.2565\nEpoch 87/100\n934/934 [==============================] - 0s 41us/sample - loss: 0.1398 - mean_absolute_error: 0.2599 - val_loss: 0.1148 - val_mean_absolute_error: 0.2535\nEpoch 88/100\n934/934 [==============================] - 0s 42us/sample - loss: 0.1357 - mean_absolute_error: 0.2553 - val_loss: 0.1120 - val_mean_absolute_error: 0.2505\nEpoch 89/100\n934/934 [==============================] - 0s 40us/sample - loss: 0.1334 - mean_absolute_error: 0.2522 - val_loss: 0.1089 - val_mean_absolute_error: 0.2472\nEpoch 90/100\n934/934 [==============================] - 0s 50us/sample - loss: 0.1290 - mean_absolute_error: 0.2499 - val_loss: 0.1083 - val_mean_absolute_error: 0.2458\nEpoch 91/100\n934/934 [==============================] - 0s 41us/sample - loss: 0.1259 - mean_absolute_error: 0.2464 - val_loss: 0.1037 - val_mean_absolute_error: 0.2412\nEpoch 92/100\n934/934 [==============================] - 0s 42us/sample - loss: 0.1236 - mean_absolute_error: 0.2442 - val_loss: 0.1018 - val_mean_absolute_error: 0.2395\nEpoch 93/100\n934/934 [==============================] - 0s 56us/sample - loss: 0.1207 - mean_absolute_error: 0.2415 - val_loss: 0.1013 - val_mean_absolute_error: 0.2378\nEpoch 94/100\n934/934 [==============================] - 0s 42us/sample - loss: 0.1178 - mean_absolute_error: 0.2392 - val_loss: 0.0977 - val_mean_absolute_error: 0.2340\nEpoch 95/100\n934/934 [==============================] - 0s 46us/sample - loss: 0.1146 - mean_absolute_error: 0.2354 - val_loss: 0.0948 - val_mean_absolute_error: 0.2308\nEpoch 96/100\n934/934 [==============================] - 0s 44us/sample - loss: 0.1125 - mean_absolute_error: 0.2322 - val_loss: 0.0933 - val_mean_absolute_error: 0.2284\nEpoch 97/100\n934/934 [==============================] - 0s 44us/sample - loss: 0.1101 - mean_absolute_error: 0.2318 - val_loss: 0.0917 - val_mean_absolute_error: 0.2262\nEpoch 98/100\n934/934 [==============================] - 0s 46us/sample - loss: 0.1083 - mean_absolute_error: 0.2292 - val_loss: 0.0926 - val_mean_absolute_error: 0.2259\nEpoch 99/100\n934/934 [==============================] - 0s 52us/sample - loss: 0.1092 - mean_absolute_error: 0.2322 - val_loss: 0.0959 - val_mean_absolute_error: 0.2283\nEpoch 100/100\n934/934 [==============================] - 0s 51us/sample - loss: 0.1052 - mean_absolute_error: 0.2272 - val_loss: 0.0907 - val_mean_absolute_error: 0.2226\n"
    }
   ],
   "source": [
    "# モデル構築\n",
    "K.clear_session()\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(10, activation = tf.nn.relu, input_shape=(X_train.shape[1],)))\n",
    "model.add(tf.keras.layers.Dense(3, activation = tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "model.summary()\n",
    "\n",
    "# コンパイル\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=tf.train.AdamOptimizer(learning_rate=0.001),\n",
    "              metrics=['mae'])\n",
    "\n",
    "# 学習\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=32,\n",
    "                    epochs=100,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "y_pred_log [12.173798 11.92067  11.794713 11.995914 12.070793]\ny_test [12.20918779 11.79810441 11.60823564 12.16525065 11.38509209]\nmae 0.20861352161122418\n"
    }
   ],
   "source": [
    "y_pred_log = model.predict(X_test)\n",
    "print(\"y_pred_log\", y_pred_log.flatten()[:5])\n",
    "print(\"y_test\", y_test.flatten()[:5])\n",
    "print(\"mae\", mean_absolute_error(y_test, y_pred_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 375.2875 248.518125\" width=\"375.2875pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.518125 \r\nL 375.2875 248.518125 \r\nL 375.2875 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 33.2875 224.64 \r\nL 368.0875 224.64 \r\nL 368.0875 7.2 \r\nL 33.2875 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"mdaa62433c5\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"48.505682\" xlink:href=\"#mdaa62433c5\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n      </defs>\r\n      <g transform=\"translate(45.324432 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"109.993285\" xlink:href=\"#mdaa62433c5\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 20 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      </defs>\r\n      <g transform=\"translate(103.630785 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"171.480888\" xlink:href=\"#mdaa62433c5\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 40 -->\r\n      <defs>\r\n       <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n      </defs>\r\n      <g transform=\"translate(165.118388 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"232.968492\" xlink:href=\"#mdaa62433c5\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 60 -->\r\n      <defs>\r\n       <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n      </defs>\r\n      <g transform=\"translate(226.605992 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-54\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"294.456095\" xlink:href=\"#mdaa62433c5\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 80 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n      </defs>\r\n      <g transform=\"translate(288.093595 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-56\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"355.943698\" xlink:href=\"#mdaa62433c5\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 100 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g transform=\"translate(346.399948 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m61abbc1cef\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m61abbc1cef\" y=\"214.881097\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(19.925 218.680315)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m61abbc1cef\" y=\"187.364953\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(13.5625 191.164172)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m61abbc1cef\" y=\"159.848809\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 40 -->\r\n      <g transform=\"translate(13.5625 163.648028)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m61abbc1cef\" y=\"132.332665\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 60 -->\r\n      <g transform=\"translate(13.5625 136.131884)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-54\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m61abbc1cef\" y=\"104.816521\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 80 -->\r\n      <g transform=\"translate(13.5625 108.61574)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-56\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m61abbc1cef\" y=\"77.300377\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 100 -->\r\n      <g transform=\"translate(7.2 81.099596)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m61abbc1cef\" y=\"49.784233\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 120 -->\r\n      <g transform=\"translate(7.2 53.583452)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_8\">\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m61abbc1cef\" y=\"22.26809\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- 140 -->\r\n      <g transform=\"translate(7.2 26.067308)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_15\">\r\n    <path clip-path=\"url(#p74e43b39bd)\" d=\"M 48.505682 17.083636 \r\nL 51.580062 20.902683 \r\nL 54.654442 25.049209 \r\nL 57.728822 29.908877 \r\nL 60.803202 35.830231 \r\nL 63.877583 43.506311 \r\nL 66.951963 52.871449 \r\nL 70.026343 62.790179 \r\nL 73.100723 73.59639 \r\nL 76.175103 85.288529 \r\nL 79.249483 97.797075 \r\nL 82.323864 111.06618 \r\nL 85.398244 124.959689 \r\nL 88.472624 139.190735 \r\nL 91.547004 153.210768 \r\nL 94.621384 166.194018 \r\nL 97.695764 177.73041 \r\nL 100.770145 187.282641 \r\nL 103.844525 194.806186 \r\nL 106.918905 200.138296 \r\nL 109.993285 203.639269 \r\nL 113.067665 205.871703 \r\nL 116.142045 207.208808 \r\nL 119.216426 208.030459 \r\nL 122.290806 208.585834 \r\nL 125.365186 209.044507 \r\nL 128.439566 209.420497 \r\nL 131.513946 209.764913 \r\nL 134.588326 210.083499 \r\nL 137.662707 210.396753 \r\nL 140.737087 210.663905 \r\nL 143.811467 210.935696 \r\nL 146.885847 211.187808 \r\nL 149.960227 211.416244 \r\nL 153.034607 211.643403 \r\nL 156.108988 211.848783 \r\nL 159.183368 212.052271 \r\nL 162.257748 212.229849 \r\nL 165.332128 212.403615 \r\nL 168.406508 212.558579 \r\nL 171.480888 212.709987 \r\nL 174.555269 212.842641 \r\nL 177.629649 212.967303 \r\nL 180.704029 213.081404 \r\nL 183.778409 213.195437 \r\nL 186.852789 213.30177 \r\nL 189.927169 213.401357 \r\nL 193.00155 213.498215 \r\nL 196.07593 213.586575 \r\nL 199.15031 213.668633 \r\nL 202.22469 213.74705 \r\nL 205.29907 213.821054 \r\nL 208.37345 213.892565 \r\nL 211.447831 213.95841 \r\nL 214.522211 214.01012 \r\nL 217.596591 214.090433 \r\nL 220.670971 214.145313 \r\nL 223.745351 214.19389 \r\nL 226.819731 214.238437 \r\nL 229.894112 214.280905 \r\nL 232.968492 214.316309 \r\nL 236.042872 214.35291 \r\nL 239.117252 214.387777 \r\nL 242.191632 214.420013 \r\nL 245.266012 214.449795 \r\nL 248.340393 214.475909 \r\nL 251.414773 214.49695 \r\nL 254.489153 214.514394 \r\nL 257.563533 214.529799 \r\nL 260.637913 214.543294 \r\nL 263.712293 214.560094 \r\nL 266.786674 214.57095 \r\nL 269.861054 214.58133 \r\nL 272.935434 214.593746 \r\nL 276.009814 214.60391 \r\nL 279.084194 214.613161 \r\nL 282.158574 214.620027 \r\nL 285.232955 214.629047 \r\nL 288.307335 214.639448 \r\nL 291.381715 214.647039 \r\nL 294.456095 214.65403 \r\nL 297.530475 214.658948 \r\nL 300.604855 214.662387 \r\nL 303.679236 214.671502 \r\nL 306.753616 214.679302 \r\nL 309.827996 214.684176 \r\nL 312.902376 214.688788 \r\nL 315.976756 214.694449 \r\nL 319.051136 214.697627 \r\nL 322.125517 214.703579 \r\nL 325.199897 214.707881 \r\nL 328.274277 214.711055 \r\nL 331.348657 214.715036 \r\nL 334.423037 214.719034 \r\nL 337.497417 214.723491 \r\nL 340.571798 214.72634 \r\nL 343.646178 214.72958 \r\nL 346.720558 214.732102 \r\nL 349.794938 214.730921 \r\nL 352.869318 214.73641 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_16\">\r\n    <path clip-path=\"url(#p74e43b39bd)\" d=\"M 48.505682 19.722492 \r\nL 51.580062 23.60645 \r\nL 54.654442 28.159304 \r\nL 57.728822 33.562303 \r\nL 60.803202 40.215398 \r\nL 63.877583 49.118392 \r\nL 66.951963 58.634524 \r\nL 70.026343 68.957707 \r\nL 73.100723 80.269007 \r\nL 76.175103 92.33858 \r\nL 79.249483 105.307463 \r\nL 82.323864 118.935668 \r\nL 85.398244 132.923179 \r\nL 88.472624 147.067462 \r\nL 91.547004 160.573636 \r\nL 94.621384 172.699306 \r\nL 97.695764 183.161707 \r\nL 100.770145 191.73459 \r\nL 103.844525 198.174748 \r\nL 106.918905 202.468114 \r\nL 109.993285 205.364132 \r\nL 113.067665 207.153836 \r\nL 116.142045 208.207896 \r\nL 119.216426 208.90053 \r\nL 122.290806 209.361007 \r\nL 125.365186 209.764998 \r\nL 128.439566 210.095563 \r\nL 131.513946 210.40557 \r\nL 134.588326 210.683986 \r\nL 137.662707 210.912553 \r\nL 140.737087 211.168066 \r\nL 143.811467 211.422346 \r\nL 146.885847 211.623595 \r\nL 149.960227 211.849013 \r\nL 153.034607 212.056129 \r\nL 156.108988 212.25373 \r\nL 159.183368 212.445326 \r\nL 162.257748 212.60616 \r\nL 165.332128 212.771084 \r\nL 168.406508 212.906715 \r\nL 171.480888 213.040117 \r\nL 174.555269 213.151891 \r\nL 177.629649 213.268265 \r\nL 180.704029 213.37014 \r\nL 183.778409 213.4789 \r\nL 186.852789 213.576248 \r\nL 189.927169 213.654669 \r\nL 193.00155 213.74304 \r\nL 196.07593 213.822601 \r\nL 199.15031 213.892585 \r\nL 202.22469 213.952368 \r\nL 205.29907 214.018218 \r\nL 208.37345 214.080776 \r\nL 211.447831 214.145425 \r\nL 214.522211 214.164922 \r\nL 217.596591 214.226193 \r\nL 220.670971 214.280763 \r\nL 223.745351 214.328666 \r\nL 226.819731 214.370352 \r\nL 229.894112 214.401213 \r\nL 232.968492 214.433379 \r\nL 236.042872 214.470134 \r\nL 239.117252 214.493564 \r\nL 242.191632 214.522441 \r\nL 245.266012 214.544185 \r\nL 248.340393 214.564525 \r\nL 251.414773 214.579879 \r\nL 254.489153 214.596502 \r\nL 257.563533 214.601628 \r\nL 260.637913 214.618328 \r\nL 263.712293 214.626335 \r\nL 266.786674 214.63196 \r\nL 269.861054 214.645117 \r\nL 272.935434 214.648626 \r\nL 276.009814 214.658653 \r\nL 279.084194 214.662495 \r\nL 282.158574 214.672526 \r\nL 285.232955 214.676563 \r\nL 288.307335 214.68466 \r\nL 291.381715 214.689705 \r\nL 294.456095 214.69908 \r\nL 297.530475 214.703065 \r\nL 300.604855 214.692114 \r\nL 303.679236 214.706911 \r\nL 306.753616 214.714558 \r\nL 309.827996 214.71877 \r\nL 312.902376 214.723191 \r\nL 315.976756 214.726961 \r\nL 319.051136 214.731215 \r\nL 322.125517 214.732088 \r\nL 325.199897 214.738443 \r\nL 328.274277 214.740987 \r\nL 331.348657 214.741659 \r\nL 334.423037 214.746743 \r\nL 337.497417 214.750631 \r\nL 340.571798 214.752713 \r\nL 343.646178 214.754974 \r\nL 346.720558 214.753741 \r\nL 349.794938 214.749092 \r\nL 352.869318 214.756364 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 33.2875 224.64 \r\nL 33.2875 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 368.0875 224.64 \r\nL 368.0875 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 33.2875 224.64 \r\nL 368.0875 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 33.2875 7.2 \r\nL 368.0875 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"legend_1\">\r\n    <g id=\"patch_7\">\r\n     <path d=\"M 289.946875 44.834375 \r\nL 361.0875 44.834375 \r\nQ 363.0875 44.834375 363.0875 42.834375 \r\nL 363.0875 14.2 \r\nQ 363.0875 12.2 361.0875 12.2 \r\nL 289.946875 12.2 \r\nQ 287.946875 12.2 287.946875 14.2 \r\nL 287.946875 42.834375 \r\nQ 287.946875 44.834375 289.946875 44.834375 \r\nz\r\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\r\n    </g>\r\n    <g id=\"line2d_17\">\r\n     <path d=\"M 291.946875 20.298437 \r\nL 311.946875 20.298437 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_18\"/>\r\n    <g id=\"text_15\">\r\n     <!-- loss -->\r\n     <defs>\r\n      <path d=\"M 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\n\" id=\"DejaVuSans-108\"/>\r\n      <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n      <path d=\"M 44.28125 53.078125 \r\nL 44.28125 44.578125 \r\nQ 40.484375 46.53125 36.375 47.5 \r\nQ 32.28125 48.484375 27.875 48.484375 \r\nQ 21.1875 48.484375 17.84375 46.4375 \r\nQ 14.5 44.390625 14.5 40.28125 \r\nQ 14.5 37.15625 16.890625 35.375 \r\nQ 19.28125 33.59375 26.515625 31.984375 \r\nL 29.59375 31.296875 \r\nQ 39.15625 29.25 43.1875 25.515625 \r\nQ 47.21875 21.78125 47.21875 15.09375 \r\nQ 47.21875 7.46875 41.1875 3.015625 \r\nQ 35.15625 -1.421875 24.609375 -1.421875 \r\nQ 20.21875 -1.421875 15.453125 -0.5625 \r\nQ 10.6875 0.296875 5.421875 2 \r\nL 5.421875 11.28125 \r\nQ 10.40625 8.6875 15.234375 7.390625 \r\nQ 20.0625 6.109375 24.8125 6.109375 \r\nQ 31.15625 6.109375 34.5625 8.28125 \r\nQ 37.984375 10.453125 37.984375 14.40625 \r\nQ 37.984375 18.0625 35.515625 20.015625 \r\nQ 33.0625 21.96875 24.703125 23.78125 \r\nL 21.578125 24.515625 \r\nQ 13.234375 26.265625 9.515625 29.90625 \r\nQ 5.8125 33.546875 5.8125 39.890625 \r\nQ 5.8125 47.609375 11.28125 51.796875 \r\nQ 16.75 56 26.8125 56 \r\nQ 31.78125 56 36.171875 55.265625 \r\nQ 40.578125 54.546875 44.28125 53.078125 \r\nz\r\n\" id=\"DejaVuSans-115\"/>\r\n     </defs>\r\n     <g transform=\"translate(319.946875 23.798437)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"line2d_19\">\r\n     <path d=\"M 291.946875 34.976562 \r\nL 311.946875 34.976562 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_20\"/>\r\n    <g id=\"text_16\">\r\n     <!-- val_loss -->\r\n     <defs>\r\n      <path d=\"M 2.984375 54.6875 \r\nL 12.5 54.6875 \r\nL 29.59375 8.796875 \r\nL 46.6875 54.6875 \r\nL 56.203125 54.6875 \r\nL 35.6875 0 \r\nL 23.484375 0 \r\nz\r\n\" id=\"DejaVuSans-118\"/>\r\n      <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n      <path d=\"M 50.984375 -16.609375 \r\nL 50.984375 -23.578125 \r\nL -0.984375 -23.578125 \r\nL -0.984375 -16.609375 \r\nz\r\n\" id=\"DejaVuSans-95\"/>\r\n     </defs>\r\n     <g transform=\"translate(319.946875 38.476562)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-118\"/>\r\n      <use x=\"59.179688\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"120.458984\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"148.242188\" xlink:href=\"#DejaVuSans-95\"/>\r\n      <use x=\"198.242188\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"226.025391\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"287.207031\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"339.306641\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p74e43b39bd\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"33.2875\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3deXzU9b3v8ddnluyABAKEJBBAQBFcA24Fq9biVvGordiq1Out99H2WtuHtur19Nb2HHu8bW972lvbHh+tS0+py3Fp6SLWqi1qUQmb7IusgQAJEPZsM5/7xwwaIWjIZPKbTN7Px2Mev/mt8/km8J5fvr/N3B0REckuoaALEBGRrqdwFxHJQgp3EZEspHAXEclCCncRkSwUCboAgIEDB3plZWXQZYiI9Cjz58+vd/eS9uZlRLhXVlZSXV0ddBkiIj2KmW081jx1y4iIZCGFu4hIFlK4i4hkoYzocxeR3qmlpYWamhoaGxuDLiWj5eXlUV5eTjQa7fA6CncRCUxNTQ19+vShsrISMwu6nIzk7uzcuZOamhpGjBjR4fXULSMigWlsbGTAgAEK9g9hZgwYMOC4/7pRuItIoBTsH60zP6MeHe77Glu4f9Yy9hxqCboUEZGM0qPDffX2/fzmzY189cmFxOK6L72IHL+ioqKgS0iLHh3uZw3vz7evOJFXV9Xxo5dWB12OiEjG+MhwN7NHzGyHmS1tZ95dZuZmNrDNtHvNbK2ZrTKzqV1d8Adsmc9n536Kb4zbw09fXcufl9Sm9eNEJHu5O1//+tcZP348EyZM4KmnngKgtraWKVOmcPrppzN+/Hhee+01YrEYn//8599b9kc/+lHA1R+tI6dCPgb8FPh124lmVgFcAmxqM20cMB04BRgK/NXMxrh7rKsK/oDikVg0jy/WfYd5Zd/nrv9azImDihgzuE9aPk5E0ufbf1jG8q17u3Sb44b25VufOqVDyz733HMsWrSIxYsXU19fz8SJE5kyZQq//e1vmTp1Kvfddx+xWIyDBw+yaNEitmzZwtKliX3ehoaGLq27K3zknru7zwF2tTPrR8A3gLad3dOAJ929yd3XA2uBSV1RaLvy+8P1M7FDDfxH3v+jTxS+PHMBB5tb0/aRIpKdXn/9dW644QbC4TCDBw/mggsuYN68eUycOJFHH32U+++/nyVLltCnTx9GjhzJunXruP3225k9ezZ9+/YNuvyjdOoiJjO7Ctji7ouPOEWnDHizzXhNclp727gNuA1g2LBhnSkjYch4mPZTcp69lefH/onz37mU+2ct43vXndb5bYpIt+voHna6uLd/UsaUKVOYM2cOf/rTn7jpppv4+te/zs0338zixYt58cUXeeihh3j66ad55JFHurniD3fcB1TNrAC4D/jf7c1uZ1q7PzF3f9jdq9y9qqSk3dsRd9yE6+CcLzN01a/5yYT1PF1dw/MLa1Lbpoj0KlOmTOGpp54iFotRV1fHnDlzmDRpEhs3bmTQoEF84Qtf4NZbb2XBggXU19cTj8e59tpr+Zd/+RcWLFgQdPlH6cye+yhgBHB4r70cWGBmk0jsqVe0WbYc2JpqkR1yyXdg81tcufkHzKp4iPueX8pp5ScwsiQ7T3MSka71T//0T8ydO5fTTjsNM+N73/seQ4YM4fHHH+f73/8+0WiUoqIifv3rX7NlyxZuueUW4vE4AP/2b/8WcPVHs2P9KfKBhcwqgT+6+/h25m0Aqty93sxOAX5Lop99KPAyMPqjDqhWVVV5lzyso241/MdkGoddwNnrbmXskL48eds5hEK6Ak4kE61YsYKTTz456DJ6hPZ+VmY2392r2lu+I6dCPgHMBcaaWY2Z3XqsZd19GfA0sByYDXw5bWfKtKdkDFz0TfLWvcjDp6/h7Q27eKp6c7d9vIhIpujI2TI3uHupu0fdvdzdf3XE/Ep3r28z/oC7j3L3se7+QjqK/lDnfBGGncukFf+Hy4fH+O6fV7Bjr24nKiK9S4++QrVdoTBc/TMs1sL3+vwXTa1xvv3H5UFXJSLSrbIv3AGKR8L5d1C0dhbfPWs/f3qnlldWbg+6KhGRbpOd4Q5w/legTynX1P2MUQPzefCFlbq5mIj0Gtkb7jmF8In7CW1dwA9OWsXq7fv5w+LuOStTRCRo2RvuABM+A0PP4PRVP+b0ITn86K+raYnFg65KRCTtsjvcQyG49EFsXy3/t/x1Nu48yDPzdeWqiHTOh937fcOGDYwff9SlQIHJ7nAHGHYOjJ7KyHX/yaTyfH7y8hoaW7rv1HsRkSB06sZhPc75d2CPXc53T3mHT7w2miff3sTnz+/4U8RFpBu8cA9sW9K12xwyAS578Jiz7777boYPH86XvvQlAO6//37MjDlz5rB7925aWlr413/9V6ZNm3ZcH9vY2MgXv/hFqquriUQi/PCHP+TCCy9k2bJl3HLLLTQ3NxOPx3n22WcZOnQon/nMZ6ipqSEWi/HNb36T66+/PqVmQ2/YcwcYfh6UVTFq7aNUVfTh13M3HvMOcCLSe0yfPv29h3IAPP3009xyyy08//zzLFiwgFdffZU777zzuPPioYceAmDJkiU88cQTzJgxg8bGRn7xi19wxx13sGjRIqqrqykvL2f27NkMHTqUxYsXs3TpUi699NIuaVvv2HM3g499FXvqRu46azXT3yjlzXW7OHfUgKArE5HDPmQPO13OOOMMduzYwdatW6mrq6N///6Ulpbyta99jTlz5hAKhdiyZQvbt29nyJAhHd7u66+/zu233w7ASSedxPDhw1m9ejXnnnsuDzzwADU1NVxzzTWMHj2aCRMmcNddd3H33Xdz5ZVXMnny5C5pW+/YcwcYezkMOJFJW35N37wwv31700evIyJZ77rrruOZZ57hqaeeYvr06cycOZO6ujrmz5/PokWLGDx4MI2Nx3cLk2Pt6X/2s59l1qxZ5OfnM3XqVF555RXGjBnD/PnzmTBhAvfeey/f+c53uqJZvSjcQ2E473ZC2xZz54nbmb20lp37m4KuSkQCNn36dJ588kmeeeYZrrvuOvbs2cOgQYOIRqO8+uqrbNy48bi3OWXKFGbOnAnA6tWr2bRpE2PHjmXdunWMHDmSr3zlK1x11VW88847bN26lYKCAm688UbuuuuuLrs3fO8Jd4BTp0NhCdfG/kxLzHl2gU6LFOntTjnlFPbt20dZWRmlpaV87nOfo7q6mqqqKmbOnMlJJ5103Nv80pe+RCwWY8KECVx//fU89thj5Obm8tRTTzF+/HhOP/10Vq5cyc0338ySJUuYNGkSp59+Og888AD//M//3CXt6tD93NOty+7n3hEv3A3VjzJjwEw2HYzyyp0XcMSjAkWkm+h+7h3X5fdzzzrjr4VYE7eXrWZ9/QHmrtsZdEUiIl2ud5wt01b5ROg3jDP2vkK//P/BU/M2c96ogUFXJSI9xJIlS7jppps+MC03N5e33noroIra1/vC3QzGX0N47k+57uQ7eHLpdhpbYuRFw0FXJtIruXuP6hqdMGECixYt6tbP7Ez3ee/rloFE10y8lemFizjQHOO1NfUfvY6IdLm8vDx27typiwo/hLuzc+dO8vLyjmu93rfnDolLkgeOYdSOF+mXfwcvLKnlknGDg65KpNcpLy+npqaGurq6oEvJaHl5eZSXlx/XOh8Z7mb2CHAlsMPdxyenfR/4FNAMvAvc4u4NyXn3ArcCMeAr7v7icVXUHcxg/LWE/vYg1435Gk+v2E5Ta4zciLpmRLpTNBplxAjd5ykdOtIt8xhw5M0OXgLGu/upwGrgXgAzGwdMB05JrvMzM8vMxBx/LeBML5zPvsZW/rFWZ82ISPb4yHB39znAriOm/cXdW5OjbwKH/16YBjzp7k3uvh5YC0zqwnq7zsDRMGQCo+peok9ehD8vqQ26IhGRLtMVB1T/G/BC8n0ZsLnNvJrktKOY2W1mVm1m1YH1t42bRqhmHtedGOIvy7frKU0ikjVSCnczuw9oBWYentTOYu0eBnf3h929yt2rSkpKUimj806+CoDr+yxmz6EW5r6rrhkRyQ6dDnczm0HiQOvn/P3zmGqAijaLlQOZ+1TqkrEwcCyjd/2NwpywumZEJGt0KtzN7FLgbuAqdz/YZtYsYLqZ5ZrZCGA08HbqZabRuKsIb3qDT52Yw19XbCcW1/m2ItLzfWS4m9kTwFxgrJnVmNmtwE+BPsBLZrbIzH4B4O7LgKeB5cBs4MvuntkPLD35KvA40/stoX5/M4s27w66IhGRlH3kee7ufkM7k3/1Ics/ADyQSlHdasgEOGE44/f8nWh4NH9Ztp2zhhcHXZWISEp65+0H2jKDcVcR2TCHiypzeWn59qArEhFJmcId4ORpEG/hpuIVrKs/wNod+4OuSEQkJQp3gLKzoE8pVQdfB+Avy7cFXJCISGoU7gChEIy9nLxNczhzaL66ZkSkx1O4Hzb2Mmg5wIzSzSza3MCOvcf3tHMRkUyicD+scjJEC5jCfNzhryt2BF2RiEinKdwPi+bByI9zQs0rVPTP4yX1u4tID6Zwb2vMVGzPZm6oPMA/3t1JY0tmX38lInIsCve2Rk8F4NKcxTS1xnlznW4kJiI9k8K9rb6lUHo6lTvnkBsJ8bdVevSXiPRMCvcjjbmUUM08LqmM8PfVCncR6ZkU7kcaMxVwPtNvJevrD7Bx54GgKxIROW4K9yOVng5FQzir6S0Adc2ISI+kcD9SKASjP0FhzRxGFufyt1U6311Eeh6Fe3tGXQSNe7ihYhdz1+mUSBHpeRTu7RnxccC4OLqUxpY4b63fFXRFIiLHReHensIBUHoaw/e8TU4kpK4ZEelxFO7HMuoiwlvm8fHKfJ0SKSI9TkeeofqIme0ws6VtphWb2UtmtiY57N9m3r1mttbMVpnZ1HQVnnajLoR4K9cVr2dd3QG2NhwKuiIRkQ7ryJ77Y8ClR0y7B3jZ3UcDLyfHMbNxwHTglOQ6PzOzcJdV250qzoZoAVWxRQC8sbY+4IJERDruI8Pd3ecARx5RnAY8nnz/OHB1m+lPunuTu68H1gKTuqjW7hXJhcqP0X/b6wwozOEf7+o+MyLSc3S2z32wu9cCJIeDktPLgM1tlqtJTjuKmd1mZtVmVl1Xl6F92iMvxHau5fJhLbyxth53D7oiEZEO6eoDqtbOtHYT0d0fdvcqd68qKSnp4jK6yKiLALiyaBU79jXpwdki0mN0Nty3m1kpQHJ4+FzBGqCizXLlwNbOlxewkrHQp5TxhxYA6ncXkZ6js+E+C5iRfD8D+H2b6dPNLNfMRgCjgbdTKzFAZjDyQgq3vMaw/rm8oX53EekhOnIq5BPAXGCsmdWY2a3Ag8AlZrYGuCQ5jrsvA54GlgOzgS+7e8++dn/EFDi0m2vL9vLmuztpjcWDrkhE5CNFPmoBd7/hGLMuPsbyDwAPpFJURhkxGYCL81fxo6ZTWbJlD2cM6/8RK4mIBEtXqH6UfuVQPJLRBxcC6JRIEekRFO4dUTmZ3Jo3GTe4gNfX6KCqiGQ+hXtHjJgCTXu4Zugu5m/arVsAi0jGU7h3ROXHAJgSXUlza5xFmxsCLkhE5MMp3DuizxAYOJYR++ZjBm+t0/3dRSSzKdw7asRkojVvcsrgAt7eoIOqIpLZFO4dVTkZWg5w9aDtzN+4m+ZWne8uIplL4d5RlYnz3SdHV9DYEmfJFvW7i0jmUrh3VOEAGHQKI/Yn7jPzpvrdRSSDKdyPx4jJ5GyZx8kluXpotohkNIX78Rh+HrQeYtqQOuZv2KX7zIhIxlK4H49h5wIwJWctB5pjLN26N+CCRETap3A/HkWDoHgUow69A8Bb63RKpIhkJoX78Rp+Lrlb32bUgHzeVr+7iGQohfvxGnYeNDZwZdk+3t6wi1hcz1UVkcyjcD9ew84B4ON5a9nX2Mrq7fsCLkhE5GgK9+NVPBKKBjOmaQkA1RvUNSMimUfhfrzMYNi5FNTOY1CfXKo37g66IhGRoyjcO2PYudjeGj5Z1kL1BoW7iGSelMLdzL5mZsvMbKmZPWFmeWZWbGYvmdma5DD7Hjg6PHG++ycK32VLwyFq9xwKuCARkQ/qdLibWRnwFaDK3ccDYWA6cA/wsruPBl5OjmeXweMhpw8TYssBtPcuIhkn1W6ZCJBvZhGgANgKTAMeT85/HLg6xc/IPKEwVEyieOd8CnLCOqgqIhmn0+Hu7luAHwCbgFpgj7v/BRjs7rXJZWqBQe2tb2a3mVm1mVXX1dV1tozgDD8Xq1vJ+WUhHVQVkYyTSrdMfxJ76SOAoUChmd3Y0fXd/WF3r3L3qpKSks6WEZyKswG4/IQaVtTuZX9Ta8AFiYi8L5VumU8A6929zt1bgOeA84DtZlYKkBzuSL3MDDT0TLAwZ4bWEHdYuEl77yKSOVIJ903AOWZWYGYGXAysAGYBM5LLzAB+n1qJGSq3CIaMp2zfO4RMB1VFJLNEOruiu79lZs8AC4BWYCHwMFAEPG1mt5L4Avh0VxSakSrOJrJwJicPLmS++t1FJIN0OtwB3P1bwLeOmNxEYi8++5VPgrcf5opBu/jpygJaY3EiYV0XJiLBUxKlomISAB/Le5eDzTFWbtNNxEQkMyjcU3HCMCgawqimxMVMCzc3BFyQiEiCwj0VZlAxiYLt8xlYlMtC9buLSIZQuKeqYhLWsJELhsZZoNMhRSRDKNxTlbyY6ZI+G9mw8yC7DjQHXJCIiMI9daWnQTiHU30VoIuZRCQzKNxTFcmFoWcweO9iwiFj4SYdVBWR4Cncu0LFJMK1i5gwJE/97iKSERTuXaF8EsSauXxgHYs3NxCLe9AViUgvp3DvCuUTATgn510ONMdYvV0XM4lIsBTuXaFvKfSrYFTTCgB1zYhI4BTuXaW8ioIdCxlQmKODqiISOIV7VymfiO3ZzMeHxrTnLiKBU7h3lWS/+8V9N7Gu7gANB3Uxk4gER+HeVZIXM53GGkA3ERORYCncu0okF4acypC9SwkZLFK/u4gESOHelconEq5dyMmD8lmkPXcRCZDCvSuVV0HrIaaW7GTR5gbcdTGTiAQjpXA3sxPM7BkzW2lmK8zsXDMrNrOXzGxNcti/q4rNeMknM52Xu549h1pYX38g4IJEpLdKdc/9x8Bsdz8JOA1YAdwDvOzuo4GXk+O9Q78KKBrM6JaVAOqaEZHAdDrczawvMAX4FYC7N7t7AzANeDy52OPA1akW2WOYQflE+tYvpDAnrHAXkcCksuc+EqgDHjWzhWb2SzMrBAa7ey1AcjiovZXN7DYzqzaz6rq6uhTKyDDlE7Fd6zi/VLf/FZHgpBLuEeBM4OfufgZwgOPognH3h929yt2rSkpKUigjwyQvZvrkCZtZUbuXxpZYwAWJSG+USrjXADXu/lZy/BkSYb/dzEoBksMdqZXYwww9HSzMGaF3aY07y7buCboiEemFOh3u7r4N2GxmY5OTLgaWA7OAGclpM4Dfp1RhT5NTCIPHUXFwGYC6ZkQkEJEU178dmGlmOcA64BYSXxhPm9mtwCbg0yl+Rs9TPpGcJc9Q3i9XtyEQkUCkFO7uvgioamfWxalst8crq4LqR/jk8H28uEnXiYlI91PypEPyoOoFBRvY0nCIHfsaAy5IRHobhXs6DDgRcvsxLrYKUL+7iHQ/hXs6hEJQfhYDGpYQDZsuZhKRbqdwT5fyiYTqlnPmkCgL9WQmEelmCvd0KasCj3NZ8TbeqdlDaywedEUi0oso3NOlPHES0cToOg42x1i1fV/ABYlIb6JwT5eCYigexYjG5YAOqopI91K4p1N5Ffk7FjKgIKpwF5FupXBPp/KJ2P7tXFzWwsLNOqgqIt1H4Z5OyX73i4s2sK7uAA0HmwMuSER6C4V7Og0eD9ECJnjiYiad7y4i3UXhnk7hKJSdxeCGxYQMFqjfXUS6icI93SrOJrx9CRMG6WImEek+Cvd0qzgbPMYVA2pZtLmBeNyDrkhEegGFe7olD6qeG13LvsZW1tXvD7ggEekNFO7pVlAMA8cysjHxZKb5G9U1IyLpp3DvDsPOpmD7fIrzw8zboHAXkfRTuHeHirOxxgauKDvAvA27gq5GRHoBhXt3qDgbgEuKNrBx50F27NWTmUQkvVIOdzMLm9lCM/tjcrzYzF4yszXJYf/Uy+zhBpwI+cWcElsJoK4ZEUm7rthzvwNY0Wb8HuBldx8NvJwc793MoOJsinctJD8aVteMiKRdSuFuZuXAFcAv20yeBjyefP84cHUqn5E1KiZhO9cwpcwU7iKSdqnuuf878A2g7WOGBrt7LUByOKi9Fc3sNjOrNrPqurq6FMvoAZL97pf138yK2r3sa2wJuCARyWadDnczuxLY4e7zO7O+uz/s7lXuXlVSUtLZMnqOsrMgnMtEX07cdb67iKRXKnvu5wNXmdkG4EngIjP7DbDdzEoBksMdKVeZDaJ5UDGJIbvnEQ4Z1TqoKiJp1Olwd/d73b3c3SuB6cAr7n4jMAuYkVxsBvD7lKvMFpWTCW9fwtlDjLfV7y4iaZSO89wfBC4xszXAJclxARgxGXCu7r+BxZsbaGqNBV2RiGSpSFdsxN3/Bvwt+X4ncHFXbDfrlJ0FkXwm2TKaWoexdMsezhpeHHRVIpKFdIVqd4rkwrCzKW+oBuDNdeqaEZH0ULh3t8rJROpXcPagOG+srQ+6GhHJUgr37lY5GYBPD9xI9YbdHGpWv7uIdD2Fe3crOxOihZwbXk5zLK6rVUUkLRTu3S0chWHnULprHtGwqWtGRNJC4R6EEZMJ7VzNhWXO6wp3EUkDhXsQKqcAcE3/dSzbupddB5oDLkhEso3CPQilp0HeCUyMLQDgH+9q711EupbCPQjhCIy+hOKtc+ibF+L1NQp3EelaCvegjJ6KHaznhqH1vLamHncPuiIRySIK96CceDFYiCvyFrOl4RCbdh0MuiIRySIK96AUFEP5JMbunQvAHHXNiEgXUrgHacxUcuuXcmb/Rv66fHvQ1YhIFlG4B2nMVAD+++A1/OPdevYc0qP3RKRrKNyDNGgc9C3nvPh8WmLOKyu19y4iXUPhHiQzGPNJ+tW+QXmfELOXbgu6IhHJEgr3oI25FGs5wG3DtvL31XUcbG4NuiIRyQIK96BVToZoAZ8MzaOxJc7fV9UFXZGIZIFOh7uZVZjZq2a2wsyWmdkdyenFZvaSma1JDvt3XblZKKcATrqCwZtfYFCB8YK6ZkSkC6Sy594K3OnuJwPnAF82s3HAPcDL7j4aeDk5Lh/m1Ouxxga+XL6eV1bu0IOzRSRlnQ53d6919wXJ9/uAFUAZMA14PLnY48DVqRaZ9UZeCAUDucznsL+plX+s3Rl0RSLSw3VJn7uZVQJnAG8Bg929FhJfAMCgrviMrBaOwITrKNn6KqV5zfxu0ZagKxKRHi7lcDezIuBZ4Kvuvvc41rvNzKrNrLquTgcRmfAZLNbE3cNW8cKSbezc3xR0RSLSg6UU7mYWJRHsM939ueTk7WZWmpxfCuxob113f9jdq9y9qqSkJJUyskPZmVA8ik+2/p3mWJxn5tcEXZGI9GCpnC1jwK+AFe7+wzazZgEzku9nAL/vfHm9iBmcej0FW+dyWUWM3769iXhctwEWkc5JZc/9fOAm4CIzW5R8XQ48CFxiZmuAS5Lj0hGnfhpwbi+Zz8adB3lNz1cVkU6KdHZFd38dsGPMvriz2+3VikfCiCmcvOlJSgsmMfPNjVwwRl1WInL8dIVqppl8J7Z/G98atpi/rthO7Z5DQVckIj2Qwj3TjLgAyqr4xK4nCBFj5pubgq5IRHoghXumMYPJdxLZu4n7hi3nkTfWs21PY9BViUgPo3DPRGMuhUHj+FzLs8RiMb43e2XQFYlID6Nwz0ShEEy+k5xdq3nwlM08t3ALCzftDroqEelBFO6ZatzVMOBEpu34GZVFMb79h+U6711EOkzhnqnCEbj654T21PD44KdZtLmB5xfqnjMi0jEK90xWMQkuuJvhW/7AHYMW8r9/v5RlW/cEXZWI9AAK90w3+U6oOIc7Gn/O2Lxd3PLoPLY06Nx3EflwCvdMF47ANQ8TshBPFP07hS313PLo2+w51BJ0ZSKSwRTuPUH/4XD9b8jdV8Of+zxAc/0Gbv7VW2zedTDoykQkQynce4qRF8DNvyO/ZQ+z+34Xr1vN5T95jT8s3hp0ZSKSgRTuPUnFJPj8n8gLxfhd9F7+V+Es7nriLW5/YiHLt3b4OSki0gso3HuaIRPgC68SGnMpNxz4DW/3u5fCFU9z7U9eYvrDc/nD4q3sOaj+eJHeztyDvzCmqqrKq6urgy6j51n/Gsy+B7YvpSWUxxzO4vnGM5nvYxlUNoJzRg5g3NC+nFzalxEDC4mG9V0ukk3MbL67V7U7T+Hew8XjsGkuLHsOX/Y77GDiAR91oUFUt45gTbyU9fFSakKlxPsNp39JGSNKCinvX0B5/3zK+udT2i+fvnkREg/XEpGeQuHeW8RaYdti2DwPNr+Fb10EDRsxj723yCHy2OQl1MaL2eb92UYx270/e8IDiBcNIdJvMHknDKGkXxGD++QyuG8eg/rmMahPLgOKcijI6fTzXUSkiynce7PWZti9AXa9C7s3wu4NeMNGWhu2wt4tRA7VYxz9b2CXF7Hb+9BAEbu9iF3el530ZV+oH815xXj+ACgsIafPAPL6DKCwb3+Ki/LoX5DDCQVRTijIoX9BlL55UUIh/UUgkg4fFu7aDct2kRwoGZN4JRkQPTwSa4H9O2DfNthXCwd2wP4d9N+/g8J99QzeX48f2Em4cSu5TbsIews0k3i1uRNC3I29FLDbi2igDxu8kMUUspdCmsJ9ac7pSyynL57bD8vvRzj/BKKFJxAp6EduYT+KCgooyovSJy9KUW6EotwIhblhCnMj5EZC6jISOU5pC3czuxT4MRAGfunuelB2JgpHoV9Z4tWGAbnJ13vcoWkvHKiHgzvhQB0casAP7aZ53y5C+3dywv6d9Du4i8qmBsJNm4i27CW3dR+hZk98Iexvv4xmD7OffPZ7PgfIp45cNnouh8jlEHm0hPNoDecTC+cRD+cTj+ThkXyIFmDRPMjJJxTJJ5yTRyingEhu/nvDaG4Bkd4g7sAAAAcjSURBVJx8cnPzyMtJfFnkRcPkRkLkRkPkhEPkRELkRsJEw6YvEskKaQl3MwsDDwGXADXAPDOb5e7L0/F50k3MIK9f4jVg1PuTgbzkq13xODTvg0MN0NgAjXuhcQ/euIfWQ/toOthAy8G9xA/tJa9xH7lN+yhuPoi1HCTUuodw6zaisUNE4ofIaW4iRLzTTWjyCC1EaCZCM1GaPcIBoon3RGgiSpwwrRYlbpHEK3R4GE28LIKHIhCO4hbBQzl4OAKhKGYhLBxKDqNYOEooHIFwDhaOJF6h6HvvQ6EIhMKEIhEsFCFkYULhMBYOEwqFsFAYOzwtFCGUnB4KhQmFQ4lpocT8kBkWDhO2w+tachgiZIm/fiyUXC4UwoCQGVjiV2uQWCY5/fB3XGKeHbWMGfoizGDp2nOfBKx193UAZvYkMA1QuPdGodD7XwoMf2/y4e6h6LHWa497oiup5SC0NkLLoffftzZByyFamw/R3HiA1qaDtDY3EmtqJNZ8kHhLE7HWZuItjXhrE8SaoKWJSKyZSKyZwlgzFmvCvJVQrBnzA4TirYS8lVC8hXCslVBrKxFvJUwrYW8lSmvX/qy6UdzfD2YH4oRwwDk83ZLjEMfw98atzTIkj9jYe+8/OM8+MK/t9KOX473PSGzy/flHHhfy9+o5ehtHvj/SsZYzwO3oWjnGto7+GRx7+WNV40Btycc490v/ccx6Oytd4V4GbG4zXgOc3XYBM7sNuA1g2LBhaSpDso5Z4jhCJOeYi0ToxoNJ7hCPQbwl8aWDg8cTf63EWyHegsdaiLU0E4u1EGttId7SkngfayV++NXaSjweS7xirXg8Tjzeisdi4HHi8Rgeb4V4nLjH8Xg8OS+GewyPx8HjuDsej4F78n38/ZrccRxLrg/JUPL4+20hfngi751s4cmI/8Dw8GJt49iTn8EH13vvfcLhrwh3PrC99+d9cPnENt7/YnnvK8f9/XWOWp52tvPh89o7seDo9T98O223cfSaR0Z8YonwCeXH+IzUpOv/QHtfVB9oq7s/DDwMibNl0lSHSHqZJe7cGY5ANL/9RejmLxwR0nf7gRqgos14OaA7XImIdJN0hfs8YLSZjTCzHGA6MCtNnyUiIkdIy1+K7t5qZv8TeJHEqZCPuPuydHyWiIgcLW3dgO7+Z+DP6dq+iIgcm24TKCKShRTuIiJZSOEuIpKFFO4iIlkoI275a2Z1wMYUNjEQqO+icnqK3thm6J3tVpt7j+Nt93B3L2lvRkaEe6rMrPpY9zTOVr2xzdA726029x5d2W51y4iIZCGFu4hIFsqWcH846AIC0BvbDL2z3Wpz79Fl7c6KPncREfmgbNlzFxGRNhTuIiJZqEeHu5ldamarzGytmd0TdD3pYGYVZvaqma0ws2VmdkdyerGZvWRma5LD/kHXmg5mFjazhWb2x+R4VrfbzE4ws2fMbGXyd35utrcZwMy+lvz3vdTMnjCzvGxst5k9YmY7zGxpm2nHbKeZ3ZvMt1VmNvV4PqvHhnubh3BfBowDbjCzccFWlRatwJ3ufjJwDvDlZDvvAV5299HAy8nxbHQHsKLNeLa3+8fAbHc/CTiNRNuzus1mVgZ8Bahy9/EkbhM+nexs92PApUdMa7edyf/n04FTkuv8LJl7HdJjw502D+F292bg8EO4s4q717r7guT7fST+s5eRaOvjycUeB64OpsL0MbNy4Argl20mZ227zawvMAX4FYC7N7t7A1nc5jYiQL6ZRYACEk9uy7p2u/scYNcRk4/VzmnAk+7e5O7rgbUkcq9DenK4t/cQ7rKAaukWZlYJnAG8BQx291pIfAEAg4KrLG3+HfgGEG8zLZvbPRKoAx5NdkX90swKye424+5bgB8Am4BaYI+7/4Usb3cbx2pnShnXk8P9Ix/CnU3MrAh4Fviqu+8Nup50M7MrgR3uPj/oWrpRBDgT+Lm7nwEcIDu6Ij5Uso95GjACGAoUmtmNwVaVEVLKuJ4c7r3mIdxmFiUR7DPd/bnk5O1mVpqcXwrsCKq+NDkfuMrMNpDocrvIzH5Ddre7Bqhx97eS48+QCPtsbjPAJ4D17l7n7i3Ac8B5ZH+7DztWO1PKuJ4c7r3iIdxmZiT6YFe4+w/bzJoFzEi+nwH8vrtrSyd3v9fdy929ksTv9hV3v5Esbre7bwM2m9nY5KSLgeVkcZuTNgHnmFlB8t/7xSSOLWV7uw87VjtnAdPNLNfMRgCjgbc7vFV377Ev4HJgNfAucF/Q9aSpjR8j8afYO8Ci5OtyYACJI+trksPioGtN48/g48Afk++zut3A6UB18vf9O6B/trc52e5vAyuBpcB/ArnZ2G7gCRLHFVpI7Jnf+mHtBO5L5tsq4LLj+SzdfkBEJAv15G4ZERE5BoW7iEgWUriLiGQhhbuISBZSuIuIZCGFu4hIFlK4i4hkof8PdntXjtXqiJoAAAAASUVORK5CYII=\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "plt.plot(range(len(history.history['loss'])), history.history['loss'], label='loss')\n",
    "plt.plot(range(len(history.history['val_loss'])), history.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題6】\n",
    "#### MNISTをKerasで学習\n",
    "TensorFlowによるMNISTデータセットによる画像の多値分類をKerasに書き換えてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(48000, 28, 28, 1)\n(12000, 28, 28, 1)\n(10000, 28, 28, 1)\n"
    }
   ],
   "source": [
    "# 読み込み\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.astype(np.float)[:, :, :, np.newaxis]\n",
    "X_test = X_test.astype(np.float)[:, :, :, np.newaxis]\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(48000, 10)\n(12000, 10)\n(10000, 10)\n"
    }
   ],
   "source": [
    "y_train = np.identity(10)[y_train]\n",
    "y_val = np.identity(10)[y_val]\n",
    "y_test = np.identity(10)[y_test]\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d (Conv2D)              (None, 26, 26, 32)        320       \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 24, 24, 16)        4624      \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 12, 12, 16)        0         \n_________________________________________________________________\nflatten (Flatten)            (None, 2304)              0         \n_________________________________________________________________\ndense (Dense)                (None, 10)                23050     \n=================================================================\nTotal params: 27,994\nTrainable params: 27,994\nNon-trainable params: 0\n_________________________________________________________________\nTrain on 48000 samples, validate on 12000 samples\nEpoch 1/5\n48000/48000 [==============================] - 19s 402us/sample - loss: 0.1558 - acc: 0.9527 - val_loss: 0.0851 - val_acc: 0.9744\nEpoch 2/5\n48000/48000 [==============================] - 20s 410us/sample - loss: 0.0605 - acc: 0.9813 - val_loss: 0.0604 - val_acc: 0.9803\nEpoch 3/5\n48000/48000 [==============================] - 22s 458us/sample - loss: 0.0443 - acc: 0.9859 - val_loss: 0.0568 - val_acc: 0.9826\nEpoch 4/5\n48000/48000 [==============================] - 20s 407us/sample - loss: 0.0340 - acc: 0.9890 - val_loss: 0.0604 - val_acc: 0.9826\nEpoch 5/5\n48000/48000 [==============================] - 20s 411us/sample - loss: 0.0288 - acc: 0.9908 - val_loss: 0.0708 - val_acc: 0.9810\n"
    }
   ],
   "source": [
    "# モデル構築\n",
    "import tensorflow.keras.layers as layers\n",
    "K.clear_session()\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Conv2D(filters=32, kernel_size=(3, 3), activation = tf.nn.relu, input_shape=X_train.shape[1:]))\n",
    "model.add(layers.Conv2D(filters=16, kernel_size=(3, 3), activation = tf.nn.relu))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(10, activation = tf.nn.softmax))\n",
    "model.summary()\n",
    "\n",
    "# コンパイル\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=tf.train.AdamOptimizer(learning_rate=0.01),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 学習\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=5,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "y_pred_proba [[2.47149767e-13 8.49596001e-12 1.35125364e-12 6.40594244e-10\n  1.25526759e-14 1.65671491e-12 2.78811086e-22 1.00000000e+00\n  2.94381049e-14 1.94739884e-11]\n [5.34899252e-12 2.46878429e-09 1.00000000e+00 3.52235623e-12\n  3.38760053e-14 1.43696971e-13 2.83084365e-08 2.67084031e-19\n  2.67431854e-10 1.35675867e-13]\n [5.02171815e-08 9.99903798e-01 6.34809538e-08 1.85491983e-10\n  8.28697885e-05 1.68360330e-07 1.02892656e-07 3.96535688e-06\n  8.99678071e-06 8.04858735e-10]\n [9.99998450e-01 1.84800041e-14 3.89966814e-08 9.92149830e-13\n  1.18522261e-10 2.94137905e-08 1.48188019e-06 1.08714101e-12\n  5.23613597e-09 2.54026915e-08]\n [7.95300437e-10 5.88887576e-12 2.13421503e-11 8.35541827e-14\n  1.00000000e+00 7.99295144e-11 2.67526242e-14 2.57576649e-09\n  1.26824773e-08 2.03190282e-08]]\ny_pred [7 2 1 ... 4 5 6]\ny_test [7 2 1 ... 4 5 6]\ntest_acc 0.9797\n"
    }
   ],
   "source": [
    "y_pred_proba = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "print(\"y_pred_proba\", y_pred_proba[:5])\n",
    "print(\"y_pred\", y_pred)\n",
    "print(\"y_test\", np.argmax(y_test, axis=1))\n",
    "print(\"test_acc\", accuracy_score(np.argmax(y_test, axis=1), y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題7】\n",
    "#### （アドバンス課題）PyTorchへの書き換え\n",
    "4種類の問題をPyTorchに書き換えてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras: (N, H, W, C)\n",
    "\n",
    "PyTorch: (N, C, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "      Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm  \\\n0      1            5.1           3.5            1.4           0.2   \n1      2            4.9           3.0            1.4           0.2   \n2      3            4.7           3.2            1.3           0.2   \n3      4            4.6           3.1            1.5           0.2   \n4      5            5.0           3.6            1.4           0.2   \n..   ...            ...           ...            ...           ...   \n145  146            6.7           3.0            5.2           2.3   \n146  147            6.3           2.5            5.0           1.9   \n147  148            6.5           3.0            5.2           2.0   \n148  149            6.2           3.4            5.4           2.3   \n149  150            5.9           3.0            5.1           1.8   \n\n            Species  \n0       Iris-setosa  \n1       Iris-setosa  \n2       Iris-setosa  \n3       Iris-setosa  \n4       Iris-setosa  \n..              ...  \n145  Iris-virginica  \n146  Iris-virginica  \n147  Iris-virginica  \n148  Iris-virginica  \n149  Iris-virginica  \n\n[150 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>SepalLengthCm</th>\n      <th>SepalWidthCm</th>\n      <th>PetalLengthCm</th>\n      <th>PetalWidthCm</th>\n      <th>Species</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>5.1</td>\n      <td>3.5</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>4.9</td>\n      <td>3.0</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>4.7</td>\n      <td>3.2</td>\n      <td>1.3</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>4.6</td>\n      <td>3.1</td>\n      <td>1.5</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>5.0</td>\n      <td>3.6</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>146</td>\n      <td>6.7</td>\n      <td>3.0</td>\n      <td>5.2</td>\n      <td>2.3</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>146</th>\n      <td>147</td>\n      <td>6.3</td>\n      <td>2.5</td>\n      <td>5.0</td>\n      <td>1.9</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>148</td>\n      <td>6.5</td>\n      <td>3.0</td>\n      <td>5.2</td>\n      <td>2.0</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>148</th>\n      <td>149</td>\n      <td>6.2</td>\n      <td>3.4</td>\n      <td>5.4</td>\n      <td>2.3</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>149</th>\n      <td>150</td>\n      <td>5.9</td>\n      <td>3.0</td>\n      <td>5.1</td>\n      <td>1.8</td>\n      <td>Iris-virginica</td>\n    </tr>\n  </tbody>\n</table>\n<p>150 rows × 6 columns</p>\n</div>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "(100, 4)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "(100, 1)"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(64, 4) (64, 1)\n(16, 4) (16, 1)\n(20, 4) (20, 1)\n"
    }
   ],
   "source": [
    "# 【問題3】Iris（2値分類）\n",
    "## 読み込み\n",
    "df_iris = pd.read_csv('Iris.csv')\n",
    "display(df_iris)\n",
    "X = np.array(df_iris[(df_iris['Species'] == 'Iris-versicolor')|(df_iris['Species'] == 'Iris-virginica')].iloc[:, 1:5])\n",
    "y = np.array(df_iris[(df_iris['Species'] == 'Iris-versicolor')|(df_iris['Species'] == 'Iris-virginica')].iloc[:, 5])\n",
    "y[y=='Iris-versicolor'] = 0\n",
    "y[y=='Iris-virginica'] = 1\n",
    "y = y.astype(np.int)[:, np.newaxis]\n",
    "display(X.shape)\n",
    "display(y.shape)\n",
    "\n",
    "## 分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "## 標準化\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "NNfor3(\n  (layer_1): Linear(in_features=4, out_features=10, bias=True)\n  (layer_2): Linear(in_features=10, out_features=5, bias=True)\n  (layer_output): Linear(in_features=5, out_features=1, bias=True)\n)\n"
    }
   ],
   "source": [
    "# PyTorchでモデル構築\n",
    "n_features = 4\n",
    "n_nodes_1 = 10\n",
    "activation_1 = nn.ReLU()\n",
    "n_nodes_2 = 5\n",
    "activation_2 = nn.ReLU()\n",
    "n_output = 1\n",
    "activation_output = nn.Sigmoid()\n",
    "\n",
    "## ネットワーク\n",
    "class NNfor3(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(NNfor3, self).__init__()\n",
    "        self.layer_1 = nn.Linear(\n",
    "            n_features,\n",
    "            n_nodes_1)\n",
    "        self.layer_2 = nn.Linear(\n",
    "            n_nodes_1,\n",
    "            n_nodes_2)\n",
    "        self.layer_output = nn.Linear(\n",
    "            n_nodes_2,\n",
    "            n_output)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = activation_1(self.layer_1(X))\n",
    "        X = activation_2(self.layer_2(X))\n",
    "        X = activation_output(self.layer_output(X))\n",
    "\n",
    "        return X\n",
    "\n",
    "## インスタンス作成\n",
    "model = NNfor3()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ミニバッチ\n",
    "batch_size = 1\n",
    "\n",
    "## numpy配列をテンソルに変換\n",
    "t_X_train = torch.from_numpy(X_train).float()\n",
    "t_y_train = torch.from_numpy(y_train).float()\n",
    "t_X_valid = torch.from_numpy(X_val).float()\n",
    "t_y_valid = torch.from_numpy(y_val).float()\n",
    "\n",
    "## 特徴量とラベルをデータセットにまとめる\n",
    "dataset_train = TensorDataset(t_X_train, t_y_train)\n",
    "dataset_valid = TensorDataset(t_X_valid, t_y_valid)\n",
    "\n",
    "# データローダを作成\n",
    "loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "loader_valid = DataLoader(dataset_valid, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## オプティマイザ\n",
    "lr = 0.003\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=lr\n",
    ")\n",
    "\n",
    "## 損失関数\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 学習(train)\n",
    "def train_step(X_train, y_train):\n",
    "    ## 訓練モード    \n",
    "    model.train()\n",
    "\n",
    "    ## 訓練データに対して予測\n",
    "    y_train_pred = model(X_train)\n",
    "\n",
    "    ## 勾配初期化\n",
    "    optimizer.zero_grad()\n",
    "    ## 損失の取得\n",
    "    loss = criterion(y_train_pred, y_train)\n",
    "    ## 勾配を計算\n",
    "    loss.backward()\n",
    "\n",
    "    ## パラメータ更新\n",
    "    optimizer.step()\n",
    "\n",
    "    ## 正解率の確認\n",
    "    with torch.no_grad():\n",
    "        y_train_pred_binary = torch.where(y_train_pred>0.5, torch.ones(1), torch.zeros(1))\n",
    "        acc = (y_train_pred_binary == y_train).sum()\n",
    "\n",
    "    return (loss.item(), acc.item())\n",
    "\n",
    "## 評価(validation)\n",
    "def valid_step(X_val, y_val):\n",
    "    ## 評価モード\n",
    "    model.eval()\n",
    "\n",
    "    ## 予測\n",
    "    y_val_pred = model(X_val)\n",
    "\n",
    "    ## 損失計算\n",
    "    loss = criterion(y_val_pred, y_val)\n",
    "\n",
    "    ## 正解率計算\n",
    "    with torch.no_grad():\n",
    "        y_val_pred_binary = torch.where(y_val_pred>0.5, torch.ones(1), torch.zeros(1))\n",
    "        acc = (y_val_pred_binary == y_val).sum()\n",
    "\n",
    "    return (loss.item(), acc.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch1/10: [loss:0.7555, acc:0.5000, val_loss:0.4092, val_acc:0.7500]\nepoch2/10: [loss:0.4939, acc:0.7656, val_loss:0.3113, val_acc:0.8750]\nepoch3/10: [loss:0.3751, acc:0.9375, val_loss:0.2294, val_acc:0.9375]\nepoch4/10: [loss:0.2828, acc:0.9375, val_loss:0.1736, val_acc:1.0000]\nepoch5/10: [loss:0.2134, acc:0.9688, val_loss:0.1206, val_acc:1.0000]\nepoch6/10: [loss:0.1553, acc:0.9844, val_loss:0.0812, val_acc:1.0000]\nepoch7/10: [loss:0.1171, acc:0.9688, val_loss:0.0564, val_acc:1.0000]\nepoch8/10: [loss:0.0928, acc:0.9844, val_loss:0.0423, val_acc:1.0000]\nepoch9/10: [loss:0.0730, acc:0.9844, val_loss:0.0369, val_acc:1.0000]\nepoch10/10: [loss:0.0605, acc:0.9844, val_loss:0.0341, val_acc:1.0000]\nDone!\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "OrderedDict([('layer_1.weight',\n              tensor([[-0.0112,  1.1246,  0.3225, -0.3176],\n                      [ 0.1721,  1.0316, -0.4480,  0.0591],\n                      [-0.1982, -0.7988,  0.6863,  0.5604],\n                      [ 0.3793,  0.1388, -0.2065, -0.2460],\n                      [-0.1038,  1.1991, -0.8123, -1.1737],\n                      [-1.1994,  0.6630, -0.3975, -0.6415],\n                      [ 0.3623, -0.9293, -0.4348, -0.5136],\n                      [-0.4777,  0.2025, -0.6828, -0.6373],\n                      [-0.9724,  1.2208, -1.6156, -0.2407],\n                      [ 0.5018, -0.3788, -0.7131, -0.3110]])),\n             ('layer_1.bias',\n              tensor([-0.0482, -0.2414,  0.4781, -0.0692,  0.1952,  0.2007, -0.0733,  0.1002,\n                      -0.0614,  0.1847])),\n             ('layer_2.weight',\n              tensor([[-0.1320, -0.0153,  0.0225,  0.7805,  0.5355,  0.4060,  0.0760, -0.3798,\n                        0.3954,  1.0299],\n                      [ 0.0611,  0.3469,  0.0942,  0.3135,  0.0967, -0.2952,  0.0773,  0.1916,\n                        0.6825,  0.3233],\n                      [ 0.3848,  0.2425,  0.7643, -0.6274, -0.7544,  0.4045,  0.0679, -0.2819,\n                        0.0380, -0.6268],\n                      [-0.2671, -0.0770,  0.0664,  0.4608,  0.7420, -0.2332,  0.2519,  1.0082,\n                       -0.0520,  0.1991],\n                      [ 0.4072, -0.0331,  0.9327,  0.6323, -0.4039,  0.1124, -0.0563, -0.0541,\n                       -0.3394, -0.1089]])),\n             ('layer_2.bias',\n              tensor([-0.1444, -0.2097,  0.5821, -0.0989,  0.6589])),\n             ('layer_output.weight',\n              tensor([[-1.4425, -0.2312,  1.2494, -1.4622,  0.6374]])),\n             ('layer_output.bias', tensor([0.4293]))])"
     },
     "metadata": {}
    }
   ],
   "source": [
    "## パラメータ初期化関数\n",
    "def initializer(layer):\n",
    "    if type(layer) == nn.Linear:\n",
    "        nn.init.kaiming_normal_(layer.weight)\n",
    "        layer.bias.data.fill_(0.0)\n",
    "\n",
    "## パラメータ初期化する\n",
    "model.apply(initializer)\n",
    "\n",
    "n_epoch = 10\n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "# 1エポック内1データあたりの損失・正解率の平均\n",
    "avg_loss = 0.0\n",
    "avg_acc = 0.0\n",
    "avg_val_loss = 0.0\n",
    "avg_val_acc = 0.0\n",
    "\n",
    "## 学習する\n",
    "for epoch in range(n_epoch):\n",
    "    ## epoch内のloss, accの保存\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    total_val_loss = 0.0\n",
    "    total_val_acc = 0.0\n",
    "    ## 累計データ数の保存\n",
    "    total_train = 0\n",
    "    total_val = 0\n",
    "\n",
    "    ## 訓練\n",
    "    for X_train, y_train in loader_train:\n",
    "       loss, acc = train_step(X_train, y_train)\n",
    "       total_loss += loss\n",
    "       total_acc += acc\n",
    "       total_train += len(y_train)\n",
    "\n",
    "    ## 評価\n",
    "    for X_val, y_val in loader_valid:\n",
    "        val_loss, val_acc = valid_step(X_val, y_val)\n",
    "        total_val_loss += val_loss\n",
    "        total_val_acc += val_acc\n",
    "        total_val += len(y_val)\n",
    "\n",
    "    # 平均計算\n",
    "    avg_loss = total_loss/total_train\n",
    "    avg_val_loss = total_val_loss/total_val\n",
    "    avg_acc = total_acc/total_train\n",
    "    avg_val_acc = total_val_acc/total_val\n",
    "\n",
    "    print(f'epoch{epoch+1}/{n_epoch}: [loss:{avg_loss:.4f}, acc:{avg_acc:.4f}, val_loss:{avg_val_loss:.4f}, val_acc:{avg_val_acc:.4f}]')\n",
    "\n",
    "    # リストに追加\n",
    "    train_history.append(avg_loss)\n",
    "    val_history.append(avg_val_loss)\n",
    "\n",
    "print('Done!')\n",
    "display(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "y_pred_proba tensor([[3.0537e-02],\n        [9.8264e-01],\n        [4.9712e-02],\n        [9.9136e-01],\n        [9.6485e-01],\n        [9.8543e-01],\n        [5.6608e-03],\n        [9.1215e-01],\n        [9.8518e-01],\n        [9.6836e-01],\n        [9.7021e-01],\n        [9.8287e-01],\n        [9.8651e-01],\n        [1.5425e-02],\n        [1.2261e-06],\n        [8.1066e-05],\n        [6.4359e-01],\n        [3.6796e-04],\n        [9.4655e-01],\n        [2.4444e-03]], grad_fn=<SigmoidBackward>)\ny_pred [0 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 1 0 1 0]\ny_test [0 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 0 0 0 0]\n"
    }
   ],
   "source": [
    "# 予測\n",
    "## Tensor変換\n",
    "t_X_test = torch.from_numpy(X_test).float()\n",
    "t_y_test = torch.from_numpy(y_test).float()\n",
    "y_pred_proba = model(t_X_test)\n",
    "y_pred = torch.where(y_pred_proba>0.5, torch.ones(1), torch.zeros(1))\n",
    "print(\"y_pred_proba\", y_pred_proba)\n",
    "print(\"y_pred\", y_pred.flatten().int().detach().numpy())\n",
    "print(\"y_test\", y_test.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 372.103125 248.518125\" width=\"372.103125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.518125 \r\nL 372.103125 248.518125 \r\nL 372.103125 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 30.103125 224.64 \r\nL 364.903125 224.64 \r\nL 364.903125 7.2 \r\nL 30.103125 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"md53630dd2c\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"45.321307\" xlink:href=\"#md53630dd2c\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n      </defs>\r\n      <g transform=\"translate(42.140057 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"112.95767\" xlink:href=\"#md53630dd2c\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 2 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      </defs>\r\n      <g transform=\"translate(109.77642 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"180.594034\" xlink:href=\"#md53630dd2c\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 4 -->\r\n      <defs>\r\n       <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n      </defs>\r\n      <g transform=\"translate(177.412784 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"248.230398\" xlink:href=\"#md53630dd2c\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 6 -->\r\n      <defs>\r\n       <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n      </defs>\r\n      <g transform=\"translate(245.049148 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"315.866761\" xlink:href=\"#md53630dd2c\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 8 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n      </defs>\r\n      <g transform=\"translate(312.685511 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-56\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_6\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m4f2b82924e\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m4f2b82924e\" y=\"224.090404\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 0.0 -->\r\n      <defs>\r\n       <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 227.889623)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m4f2b82924e\" y=\"196.691025\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0.1 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 200.490244)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-49\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m4f2b82924e\" y=\"169.291646\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0.2 -->\r\n      <g transform=\"translate(7.2 173.090864)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m4f2b82924e\" y=\"141.892266\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 0.3 -->\r\n      <defs>\r\n       <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 145.691485)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-51\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m4f2b82924e\" y=\"114.492887\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 0.4 -->\r\n      <g transform=\"translate(7.2 118.292106)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m4f2b82924e\" y=\"87.093508\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 0.5 -->\r\n      <defs>\r\n       <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 90.892727)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m4f2b82924e\" y=\"59.694129\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 0.6 -->\r\n      <g transform=\"translate(7.2 63.493348)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_8\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m4f2b82924e\" y=\"32.29475\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 0.7 -->\r\n      <defs>\r\n       <path d=\"M 8.203125 72.90625 \r\nL 55.078125 72.90625 \r\nL 55.078125 68.703125 \r\nL 28.609375 0 \r\nL 18.3125 0 \r\nL 43.21875 64.59375 \r\nL 8.203125 64.59375 \r\nz\r\n\" id=\"DejaVuSans-55\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 36.093968)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_14\">\r\n    <path clip-path=\"url(#p31b8bb4463)\" d=\"M 45.321307 17.083636 \r\nL 79.139489 88.755085 \r\nL 112.95767 121.310119 \r\nL 146.775852 146.607735 \r\nL 180.594034 165.632409 \r\nL 214.412216 181.532716 \r\nL 248.230398 192.015848 \r\nL 282.04858 198.665508 \r\nL 315.866761 204.081808 \r\nL 349.684943 207.503557 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_15\">\r\n    <path clip-path=\"url(#p31b8bb4463)\" d=\"M 45.321307 111.979659 \r\nL 79.139489 138.80496 \r\nL 112.95767 161.229495 \r\nL 146.775852 176.514145 \r\nL 180.594034 191.036202 \r\nL 214.412216 201.831901 \r\nL 248.230398 208.627662 \r\nL 282.04858 212.505989 \r\nL 315.866761 213.978896 \r\nL 349.684943 214.756364 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 30.103125 224.64 \r\nL 30.103125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 364.903125 224.64 \r\nL 364.903125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 30.103125 224.64 \r\nL 364.903125 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 30.103125 7.2 \r\nL 364.903125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"legend_1\">\r\n    <g id=\"patch_7\">\r\n     <path d=\"M 286.7625 44.834375 \r\nL 357.903125 44.834375 \r\nQ 359.903125 44.834375 359.903125 42.834375 \r\nL 359.903125 14.2 \r\nQ 359.903125 12.2 357.903125 12.2 \r\nL 286.7625 12.2 \r\nQ 284.7625 12.2 284.7625 14.2 \r\nL 284.7625 42.834375 \r\nQ 284.7625 44.834375 286.7625 44.834375 \r\nz\r\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\r\n    </g>\r\n    <g id=\"line2d_16\">\r\n     <path d=\"M 288.7625 20.298437 \r\nL 308.7625 20.298437 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_17\"/>\r\n    <g id=\"text_14\">\r\n     <!-- loss -->\r\n     <defs>\r\n      <path d=\"M 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\n\" id=\"DejaVuSans-108\"/>\r\n      <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n      <path d=\"M 44.28125 53.078125 \r\nL 44.28125 44.578125 \r\nQ 40.484375 46.53125 36.375 47.5 \r\nQ 32.28125 48.484375 27.875 48.484375 \r\nQ 21.1875 48.484375 17.84375 46.4375 \r\nQ 14.5 44.390625 14.5 40.28125 \r\nQ 14.5 37.15625 16.890625 35.375 \r\nQ 19.28125 33.59375 26.515625 31.984375 \r\nL 29.59375 31.296875 \r\nQ 39.15625 29.25 43.1875 25.515625 \r\nQ 47.21875 21.78125 47.21875 15.09375 \r\nQ 47.21875 7.46875 41.1875 3.015625 \r\nQ 35.15625 -1.421875 24.609375 -1.421875 \r\nQ 20.21875 -1.421875 15.453125 -0.5625 \r\nQ 10.6875 0.296875 5.421875 2 \r\nL 5.421875 11.28125 \r\nQ 10.40625 8.6875 15.234375 7.390625 \r\nQ 20.0625 6.109375 24.8125 6.109375 \r\nQ 31.15625 6.109375 34.5625 8.28125 \r\nQ 37.984375 10.453125 37.984375 14.40625 \r\nQ 37.984375 18.0625 35.515625 20.015625 \r\nQ 33.0625 21.96875 24.703125 23.78125 \r\nL 21.578125 24.515625 \r\nQ 13.234375 26.265625 9.515625 29.90625 \r\nQ 5.8125 33.546875 5.8125 39.890625 \r\nQ 5.8125 47.609375 11.28125 51.796875 \r\nQ 16.75 56 26.8125 56 \r\nQ 31.78125 56 36.171875 55.265625 \r\nQ 40.578125 54.546875 44.28125 53.078125 \r\nz\r\n\" id=\"DejaVuSans-115\"/>\r\n     </defs>\r\n     <g transform=\"translate(316.7625 23.798437)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"line2d_18\">\r\n     <path d=\"M 288.7625 34.976562 \r\nL 308.7625 34.976562 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_19\"/>\r\n    <g id=\"text_15\">\r\n     <!-- val_loss -->\r\n     <defs>\r\n      <path d=\"M 2.984375 54.6875 \r\nL 12.5 54.6875 \r\nL 29.59375 8.796875 \r\nL 46.6875 54.6875 \r\nL 56.203125 54.6875 \r\nL 35.6875 0 \r\nL 23.484375 0 \r\nz\r\n\" id=\"DejaVuSans-118\"/>\r\n      <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n      <path d=\"M 50.984375 -16.609375 \r\nL 50.984375 -23.578125 \r\nL -0.984375 -23.578125 \r\nL -0.984375 -16.609375 \r\nz\r\n\" id=\"DejaVuSans-95\"/>\r\n     </defs>\r\n     <g transform=\"translate(316.7625 38.476562)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-118\"/>\r\n      <use x=\"59.179688\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"120.458984\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"148.242188\" xlink:href=\"#DejaVuSans-95\"/>\r\n      <use x=\"198.242188\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"226.025391\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"287.207031\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"339.306641\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p31b8bb4463\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"30.103125\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "plt.plot(train_history, label=\"loss\")\n",
    "plt.plot(val_history, label=\"val_loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "(150, 4)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "(150,)"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(96, 4) (96,)\n(24, 4) (24,)\n(30, 4) (30,)\n"
    }
   ],
   "source": [
    "# 【問題4】Iris（多値分類）をKerasで学習\n",
    "# 抽出\n",
    "X = np.array(df_iris.iloc[:, 1:5])\n",
    "y = np.array(df_iris.iloc[:, 5])\n",
    "y[y=='Iris-versicolor'] = 0\n",
    "y[y=='Iris-virginica'] = 1\n",
    "y[y=='Iris-setosa'] = 2\n",
    "y = y.astype(np.int)\n",
    "display(X.shape)\n",
    "display(y.shape)\n",
    "\n",
    "# 分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "# 標準化\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "NNfor4(\n  (layer): Sequential(\n    (0): Linear(in_features=4, out_features=100, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=100, out_features=10, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=10, out_features=3, bias=True)\n  )\n)\n"
    }
   ],
   "source": [
    "# PyTorchでモデル構築\n",
    "n_features = 4\n",
    "n_nodes_1 = 100\n",
    "activation_1 = nn.ReLU()\n",
    "n_nodes_2 = 10\n",
    "activation_2 = nn.ReLU()\n",
    "n_output = 3\n",
    "\n",
    "## ネットワーク\n",
    "class NNfor4(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(NNfor4, self).__init__()\n",
    "        self.layer = nn.Sequential(nn.Linear(n_features, n_nodes_1), activation_1,\n",
    "                                   nn.Linear(n_nodes_1, n_nodes_2), activation_2,\n",
    "                                   nn.Linear(n_nodes_2, n_output))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.layer(X)\n",
    "\n",
    "        return X\n",
    "\n",
    "## インスタンス作成\n",
    "model = NNfor4()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ミニバッチ\n",
    "batch_size = 1\n",
    "\n",
    "## numpy配列をテンソルに変換\n",
    "t_X_train = torch.from_numpy(X_train).float()\n",
    "t_y_train = torch.from_numpy(y_train).long()\n",
    "t_X_valid = torch.from_numpy(X_val).float()\n",
    "t_y_valid = torch.from_numpy(y_val).long()\n",
    "\n",
    "## 特徴量とラベルをデータセットにまとめる\n",
    "dataset_train = TensorDataset(t_X_train, t_y_train)\n",
    "dataset_valid = TensorDataset(t_X_valid, t_y_valid)\n",
    "\n",
    "# データローダを作成\n",
    "loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "loader_valid = DataLoader(dataset_valid, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## オプティマイザ\n",
    "lr = 0.003\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=lr\n",
    ")\n",
    "\n",
    "## 損失関数\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 学習(train)\n",
    "def train_step(X_train, y_train):\n",
    "    ## 訓練モード    \n",
    "    model.train()\n",
    "\n",
    "    ## 訓練データに対して予測\n",
    "    y_train_pred = model(X_train)\n",
    "    print(y_train_pred.shape, y_train.shape)\n",
    "\n",
    "    ## 勾配初期化\n",
    "    optimizer.zero_grad()\n",
    "    ## 損失の取得\n",
    "    loss = criterion(y_train_pred, y_train)\n",
    "    ## 勾配を計算\n",
    "    loss.backward()\n",
    "\n",
    "    ## パラメータ更新\n",
    "    optimizer.step()\n",
    "\n",
    "    ## 正解率の確認\n",
    "    with torch.no_grad():\n",
    "        y_train_pred_num = torch.argmax(y_train_pred, axis=1)\n",
    "        acc = (y_train_pred_num == y_train).sum()\n",
    "\n",
    "    return (loss.item(), acc.item())\n",
    "\n",
    "## 評価(validation)\n",
    "def valid_step(X_val, y_val):\n",
    "    ## 評価モード\n",
    "    model.eval()\n",
    "\n",
    "    ## 予測\n",
    "    y_val_pred = model(X_val)\n",
    "\n",
    "    ## 損失計算\n",
    "    loss = criterion(y_val_pred, y_val)\n",
    "\n",
    "    ## 正解率計算\n",
    "    with torch.no_grad():\n",
    "        y_val_pred_num = torch.argmax(y_val_pred, axis=1)\n",
    "        acc = (y_val_pred_num == y_val).sum()\n",
    "\n",
    "    return (loss.item(), acc.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\nepoch1/10: [loss:0.5725, acc:0.7396, val_loss:0.3897, val_acc:0.7917]\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\nepoch2/10: [loss:0.2813, acc:0.8750, val_loss:0.3512, val_acc:0.7917]\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\nepoch3/10: [loss:0.2231, acc:0.8958, val_loss:0.4301, val_acc:0.7917]\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\nepoch4/10: [loss:0.1725, acc:0.9271, val_loss:0.3573, val_acc:0.7917]\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\nepoch5/10: [loss:0.1406, acc:0.9375, val_loss:0.3911, val_acc:0.7917]\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\nepoch6/10: [loss:0.1289, acc:0.9688, val_loss:0.3585, val_acc:0.7917]\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\nepoch7/10: [loss:0.1020, acc:0.9688, val_loss:0.2911, val_acc:0.9167]\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\nepoch8/10: [loss:0.0957, acc:0.9583, val_loss:0.5129, val_acc:0.7917]\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\nepoch9/10: [loss:0.0879, acc:0.9792, val_loss:0.3128, val_acc:0.8750]\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\ntorch.Size([1, 3]) torch.Size([1])\nepoch10/10: [loss:0.0852, acc:0.9688, val_loss:0.2864, val_acc:0.9167]\nDone!\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "OrderedDict([('layer.0.weight',\n              tensor([[-1.6172e-01, -2.9952e-02, -4.0384e-01, -1.6883e-01],\n                      [-4.4544e-01, -2.9843e-02,  8.6297e-01,  2.5780e-01],\n                      [-6.0677e-01,  5.6239e-01,  5.8795e-01, -6.1593e-01],\n                      [-1.4384e-01, -7.0777e-01,  9.5688e-02, -4.3323e-01],\n                      [ 4.1650e-01, -3.3849e-01,  6.5422e-01,  4.5548e-01],\n                      [ 2.0147e-01,  8.2869e-02, -1.4099e-01, -1.3356e+00],\n                      [-6.4342e-01, -2.9998e-01,  2.8024e-01, -3.5627e-01],\n                      [-5.7008e-01, -8.9157e-01, -9.8975e-01, -3.6891e-01],\n                      [-1.4727e+00, -8.8067e-01, -1.3542e-03,  6.8844e-01],\n                      [-6.3768e-01,  1.1503e-01,  7.0110e-01,  1.8334e+00],\n                      [ 7.9395e-01,  2.3525e+00,  6.3460e-01,  2.2536e-01],\n                      [ 1.0079e+00, -1.1863e+00, -1.7975e-01,  4.0535e-01],\n                      [-9.7313e-01, -1.7195e-01, -7.7480e-01,  1.9884e-01],\n                      [-4.2036e-02, -1.9303e-01, -6.5491e-01, -4.0894e-01],\n                      [ 3.8110e-01, -1.1925e-01, -7.8421e-01,  4.6824e-01],\n                      [ 9.7837e-02, -6.9436e-01,  3.1604e-01,  3.1210e-01],\n                      [ 4.9136e-01, -6.1607e-01,  5.2732e-01, -1.1340e+00],\n                      [-2.6780e-01,  7.1788e-01, -1.2554e+00,  1.7120e-01],\n                      [ 1.5111e+00, -2.2394e-01, -2.2187e-01,  1.6795e-01],\n                      [-3.4713e-01, -6.1478e-01,  7.8081e-01, -1.1388e+00],\n                      [-1.3376e+00, -3.9527e-01, -1.2848e-01, -8.8808e-01],\n                      [ 1.3495e+00, -7.5299e-01,  1.6311e+00,  7.5031e-01],\n                      [-6.0916e-01,  5.4765e-01,  7.6959e-01,  3.0774e-01],\n                      [-1.2989e+00,  3.1272e-01,  2.8318e-01,  5.7713e-01],\n                      [-6.1823e-01,  7.6608e-01, -4.4597e-01, -1.4435e+00],\n                      [ 3.5919e-01, -8.3640e-01, -1.3587e+00,  2.3565e+00],\n                      [ 3.7097e-01, -6.3945e-02, -5.7759e-01, -4.8421e-01],\n                      [-2.2410e-01,  1.1381e+00, -7.6662e-02, -5.0529e-01],\n                      [-9.7415e-01,  9.9049e-01,  7.0853e-01, -4.4311e-02],\n                      [-1.1614e+00,  1.7515e-01, -1.9664e-01,  6.9784e-01],\n                      [ 9.3586e-01,  1.0219e+00, -5.6360e-02, -5.9826e-01],\n                      [-3.4481e-01,  1.6697e-02,  5.6675e-01,  1.0137e+00],\n                      [ 1.8325e-01,  4.9059e-01,  1.5636e-01, -5.4725e-02],\n                      [-9.5238e-01, -5.1331e-01, -2.1395e-01,  2.6896e-01],\n                      [-1.4137e-01, -2.3548e-01,  9.5734e-01, -3.2839e-01],\n                      [ 2.0943e-02, -1.4068e+00, -7.0860e-01, -1.2793e+00],\n                      [-2.9531e-01,  1.5386e-01,  2.5309e-01, -2.1757e-01],\n                      [-5.6990e-01, -3.1586e-01,  4.4640e-01,  1.8099e-01],\n                      [ 1.6849e+00, -9.3340e-01, -5.8943e-01, -1.5422e+00],\n                      [ 2.1820e+00,  1.1901e+00,  4.3152e-01, -9.3967e-02],\n                      [-1.1222e-01,  1.8395e-02,  3.9647e-01,  6.5188e-01],\n                      [-2.2671e-01, -5.1013e-01, -6.2771e-01, -3.4513e-01],\n                      [ 2.8459e-01, -6.2688e-01,  2.8003e-01,  2.7444e-01],\n                      [ 6.4433e-01, -7.5528e-01, -9.2279e-01,  7.8516e-02],\n                      [ 4.9550e-01,  3.4173e-01, -2.4005e-01,  3.3415e-01],\n                      [-8.7166e-01,  7.6894e-02, -6.0683e-01, -1.3364e+00],\n                      [ 6.0530e-01,  1.3636e+00,  1.8285e-01, -9.4730e-01],\n                      [-8.5818e-01, -9.6639e-01, -5.2077e-01,  9.4948e-01],\n                      [-7.4977e-01,  1.2510e-01,  3.7872e-01, -4.5028e-01],\n                      [ 5.6963e-02,  9.8165e-01,  4.2331e-01,  4.6535e-01],\n                      [ 1.8438e-01,  1.6029e+00,  6.7293e-01, -7.3395e-02],\n                      [ 2.4809e+00,  1.0345e+00,  2.7779e-01,  1.8865e+00],\n                      [ 2.2222e-01, -8.4633e-01,  2.3023e-01,  1.7720e-02],\n                      [-9.8857e-01, -5.9445e-01,  1.0010e+00, -1.1945e+00],\n                      [-1.0729e+00,  4.5335e-01,  4.2393e-01,  2.9726e-01],\n                      [-7.3255e-01, -5.1959e-01,  2.6117e-01,  3.5651e-01],\n                      [ 5.6479e-01, -9.6194e-01, -6.9124e-01, -1.9657e-01],\n                      [-1.5231e-01,  7.6073e-01, -3.9117e-01,  4.7143e-01],\n                      [ 6.0197e-01,  4.1610e-01, -6.1191e-01, -8.0017e-01],\n                      [-7.9627e-02,  1.3374e-01, -7.2612e-01,  3.3408e-01],\n                      [ 8.3625e-01,  8.4744e-01, -1.2975e-01,  3.2545e-01],\n                      [ 3.2363e-01, -1.3767e+00, -5.7073e-01,  1.5360e-01],\n                      [ 8.6470e-01, -8.6837e-01,  1.3767e+00, -1.1791e+00],\n                      [-1.0498e+00, -3.7737e-01,  1.2895e+00, -4.5862e-01],\n                      [-3.6747e-01, -3.4319e-01,  1.2380e+00, -4.0514e-01],\n                      [-1.2583e-01, -3.5799e-01,  1.0927e+00, -9.8762e-01],\n                      [ 6.9108e-01,  1.9189e+00, -9.0940e-01,  5.7527e-01],\n                      [-9.7475e-01, -3.5595e-01,  6.1413e-02, -1.2714e-01],\n                      [ 1.4957e+00, -2.5787e-01, -3.3080e-01, -1.0119e+00],\n                      [-1.0099e+00,  9.4828e-02,  6.0519e-01, -7.2437e-01],\n                      [-8.3311e-01, -3.2032e-01,  8.5331e-01,  1.7132e+00],\n                      [ 2.4253e-01, -2.0284e-01,  5.2373e-01,  2.5259e-03],\n                      [-5.8699e-01,  7.2254e-01,  1.8375e-01, -1.1517e+00],\n                      [ 4.1207e-01, -7.5244e-01, -8.6296e-01,  1.4161e+00],\n                      [-3.4218e-01, -7.2453e-02, -3.8649e-01, -9.4711e-01],\n                      [-1.2605e+00,  9.3837e-01, -7.5974e-01, -5.5248e-01],\n                      [ 1.0483e+00,  1.6278e-02, -6.7888e-01,  3.1402e-04],\n                      [ 5.0633e-01, -5.9397e-01,  4.3514e-01,  4.4976e-01],\n                      [ 3.6054e-01, -2.0828e-01,  1.0948e+00, -8.3176e-01],\n                      [ 1.2927e+00, -5.8746e-04, -2.0406e-01, -9.4934e-01],\n                      [-4.1274e-01, -7.3082e-01,  5.8972e-01, -1.5569e-01],\n                      [-7.5926e-01,  2.7262e-01, -1.0074e+00, -4.5836e-01],\n                      [-6.5893e-02, -2.1054e-01,  6.4847e-01,  6.1922e-01],\n                      [-1.0384e+00,  9.9697e-02,  1.1360e-01,  2.4750e+00],\n                      [-2.7456e-01, -7.6640e-01,  2.8606e-01, -4.1935e-01],\n                      [-1.5291e+00,  4.8467e-01,  2.5893e-01, -6.5229e-01],\n                      [ 1.2143e+00,  3.1064e-01, -4.8415e-01,  6.3011e-01],\n                      [ 5.3749e-01, -8.9029e-01,  4.3473e-01,  3.3433e-01],\n                      [-7.4938e-02, -2.5185e-01, -1.0039e-01,  1.2953e+00],\n                      [-3.3366e-01,  6.6430e-01, -5.5368e-01,  8.0015e-01],\n                      [-1.5723e-01, -5.3163e-02,  8.9541e-01, -1.0489e-01],\n                      [ 4.4917e-01,  1.5734e+00,  4.9390e-01,  4.5763e-01],\n                      [ 1.6191e+00, -3.2715e-01, -2.2362e-01, -1.9320e-01],\n                      [-1.5648e-01,  3.9844e-02,  1.4968e+00,  5.8307e-01],\n                      [-3.4060e-01, -6.5277e-01,  3.2990e-01, -6.9591e-04],\n                      [-4.3532e-01,  5.5179e-01, -1.4395e+00,  1.6388e-01],\n                      [-3.3246e-01,  5.1361e-01, -1.4288e-02, -5.4233e-01],\n                      [ 7.2955e-01,  3.7799e-01,  1.1051e-01,  8.1853e-01],\n                      [ 8.3294e-01, -2.3354e-01,  7.3885e-01,  5.4038e-01],\n                      [-4.0215e-01, -1.0982e-01,  4.9422e-01,  8.7100e-01]])),\n             ('layer.0.bias',\n              tensor([ 0.2315, -0.1139,  0.3893, -0.1309, -0.2092,  0.0227, -0.1022,  0.0222,\n                       0.0038,  0.0961, -0.2432, -0.0135,  0.2398,  0.1629,  0.1898, -0.2133,\n                       0.3202,  0.0890,  0.0842,  0.1969, -0.1132,  0.1784, -0.0186,  0.0951,\n                       0.0883, -0.0975, -0.0666,  0.0052,  0.2647, -0.1129, -0.0623, -0.1725,\n                       0.1001,  0.3244, -0.2766,  0.0139,  0.1541,  0.1118,  0.2674, -0.0786,\n                      -0.2057, -0.0472,  0.2770,  0.1841, -0.0919,  0.1159,  0.2200, -0.0704,\n                       0.0030, -0.1804, -0.2742, -0.1682,  0.2637,  0.1200,  0.2680, -0.1603,\n                       0.2467, -0.0591, -0.1776, -0.0694,  0.1179,  0.0843,  0.2137, -0.2242,\n                      -0.1729,  0.0165,  0.1872, -0.1606,  0.2189,  0.1654, -0.0861,  0.1907,\n                       0.0511, -0.1328,  0.3485,  0.1459,  0.2848,  0.0763,  0.0295,  0.2698,\n                       0.0645,  0.1237, -0.3527,  0.0050,  0.1675,  0.0279, -0.2269, -0.2273,\n                      -0.2106,  0.1290, -0.2372, -0.2852, -0.0181, -0.2883,  0.2302,  0.0132,\n                       0.3873, -0.2866, -0.2061, -0.1674])),\n             ('layer.2.weight',\n              tensor([[ 1.7766e-01, -2.2467e-01, -2.0340e-01,  3.3959e-02,  3.0559e-02,\n                       -1.1664e-01, -3.5461e-02, -1.2497e-01, -5.2767e-02, -3.5608e-01,\n                       -1.4508e-01, -1.8965e-01, -1.7155e-01, -3.1957e-02, -1.6462e-01,\n                       -1.6476e-01,  1.0062e-01,  2.5660e-01, -1.3489e-01,  1.3265e-01,\n                       -1.0698e-01,  1.0704e-01,  1.4745e-01,  5.5206e-03,  9.1541e-04,\n                       -8.7382e-02, -2.8801e-02, -1.2339e-01, -1.6123e-01,  4.3631e-02,\n                       -1.3489e-02,  6.5901e-02, -1.0533e-01, -7.2428e-02, -3.4915e-01,\n                       -4.6366e-02, -3.9299e-02, -1.1699e-01,  7.9658e-02, -2.0258e-01,\n                       -1.0103e-01, -1.9551e-01, -1.1931e-01,  2.4590e-01, -6.5887e-03,\n                       -7.6162e-02, -2.9163e-02,  3.0695e-02, -4.0652e-01, -1.3717e-01,\n                       -1.4940e-02, -3.3263e-02,  6.8778e-02,  5.0127e-02, -2.0197e-02,\n                        1.4120e-01, -2.4226e-01, -1.6260e-01,  4.8806e-02, -2.0051e-01,\n                       -4.4760e-02, -7.4159e-02, -1.2178e-01, -2.8304e-02, -2.4357e-02,\n                       -2.5122e-01, -9.1615e-02,  6.4785e-02,  8.0597e-02, -3.4592e-01,\n                        3.7507e-02,  1.5969e-01, -4.5650e-02,  1.2127e-01,  4.2852e-02,\n                       -1.6687e-01,  3.4419e-02,  6.0774e-02,  2.3276e-01,  9.1140e-02,\n                        3.5827e-02,  1.9639e-01, -1.2975e-01,  2.4619e-01,  1.7265e-01,\n                        3.5212e-01,  7.6067e-02,  1.4151e-02, -1.1292e-01,  8.1532e-02,\n                       -3.6475e-01, -6.6953e-02,  7.6803e-02,  1.4709e-01,  2.0635e-02,\n                        1.8969e-01,  5.5618e-02, -1.0903e-01, -8.5057e-02,  1.1215e-01],\n                      [-1.8533e-01,  1.5947e-01, -2.0958e-01,  5.2060e-02,  7.9999e-02,\n                       -3.5835e-02, -1.9609e-01, -2.0830e-01,  6.9319e-02,  3.8332e-01,\n                        6.1537e-02,  3.3036e-01,  1.1500e-01, -2.1666e-02, -2.3314e-01,\n                        2.6698e-01, -8.5035e-02, -2.7519e-01, -1.1164e-01, -7.5302e-02,\n                        1.1204e-01,  1.2399e-02,  1.7128e-02, -6.3242e-02, -1.7265e-01,\n                        1.8230e-01,  1.2296e-01, -2.5291e-01, -5.3034e-02, -7.6626e-02,\n                       -2.1764e-01,  1.0219e-01,  1.9457e-01,  1.1902e-01,  1.1715e-01,\n                       -1.1789e-01, -1.5891e-01,  2.4368e-01, -7.4161e-02,  4.1562e-02,\n                        1.4378e-01, -4.3824e-04,  1.6216e-01,  1.2964e-01, -1.1313e-01,\n                        1.4683e-01, -5.1463e-01, -3.4177e-03, -2.9995e-01,  4.1206e-01,\n                        1.5514e-01,  3.3416e-01,  1.4111e-01,  2.2512e-01,  1.5188e-02,\n                        6.0080e-02,  5.1563e-02,  2.2148e-01, -1.8895e-02,  1.1436e-01,\n                       -2.7771e-01, -2.2418e-01,  3.6700e-01,  1.3742e-01,  4.1918e-01,\n                       -3.7860e-02, -9.1208e-02, -1.4975e-01, -4.5562e-01, -1.5111e-01,\n                        2.1037e-01,  2.3660e-01,  8.3481e-02, -3.4045e-02, -5.8581e-02,\n                        1.7488e-01, -4.0698e-01,  2.6884e-01, -1.4463e-02, -3.5626e-01,\n                        3.8139e-01,  5.4315e-02,  2.2694e-01,  4.2461e-01,  2.2709e-01,\n                        3.5104e-01,  1.2977e-02, -9.7316e-02,  3.9467e-01,  1.4217e-01,\n                        2.2407e-01, -3.1591e-02,  1.8795e-02,  1.5589e-01,  2.3633e-01,\n                       -4.2343e-01, -4.0151e-02, -2.3626e-01,  3.5436e-01,  1.4152e-01],\n                      [ 1.4278e-01,  3.0860e-01, -2.0276e-01,  6.3891e-02,  8.2942e-02,\n                       -1.3892e-01,  2.8703e-01,  4.2769e-02,  6.3304e-02,  3.6448e-01,\n                        5.3976e-02,  4.2895e-02, -3.6245e-01, -2.0097e-01, -1.3344e-01,\n                        1.6064e-01, -1.8002e-01,  5.5956e-03, -1.5260e-01,  2.5150e-01,\n                       -2.7126e-01, -1.1729e-01,  1.2793e-01, -1.2653e-01, -3.9690e-01,\n                        4.4620e-02, -9.0869e-02,  3.7238e-01, -1.9081e-01, -1.6580e-02,\n                       -1.2347e-01,  3.3471e-01,  4.3564e-02, -1.1935e-01,  2.4483e-01,\n                        2.4113e-03, -3.4302e-01,  3.9916e-02, -1.3466e-01,  2.6076e-01,\n                        4.4685e-01,  2.6326e-02,  1.1778e-03, -2.1097e-02,  1.9040e-02,\n                       -3.6823e-01,  4.3167e-03,  1.5447e-01, -7.3870e-02, -5.6986e-02,\n                        1.7996e-01, -6.2393e-02, -2.2747e-01,  5.8932e-02, -2.9571e-01,\n                        8.6811e-02, -1.1816e-01,  1.1655e-01, -1.1342e-01,  3.3319e-03,\n                       -3.0195e-01,  3.7558e-02, -1.6782e-02, -3.1015e-03,  4.0628e-01,\n                        1.7127e-01, -1.6346e-01,  8.9455e-02, -1.9043e-01, -1.5846e-01,\n                        4.3527e-01,  3.0767e-01, -1.3223e-01,  1.2671e-01, -3.5035e-01,\n                       -4.6551e-01, -1.8897e-01,  2.6353e-01, -9.2627e-02, -2.9769e-01,\n                       -1.6885e-03, -2.1441e-01,  2.4887e-01,  3.3961e-01, -5.8670e-02,\n                        2.1837e-02,  1.6669e-01,  3.7880e-01,  5.3078e-01,  2.1763e-01,\n                        3.7876e-03,  1.3131e-01, -2.1374e-01,  4.5948e-01, -1.3986e-01,\n                        6.3599e-02, -2.2262e-01,  7.7338e-02,  3.7560e-01,  2.4723e-01],\n                      [-1.0503e-01, -3.1112e-02,  2.7608e-01,  8.6972e-02, -1.5561e-01,\n                        4.3644e-02,  8.7102e-02,  1.2103e-01,  2.7035e-01, -2.6010e-01,\n                       -5.3838e-02, -1.5710e-01,  8.5790e-02,  2.6242e-02,  1.5521e-01,\n                       -4.3356e-02, -3.6572e-01,  3.2975e-01, -5.7713e-02,  3.5160e-02,\n                        9.2262e-02, -4.5912e-02, -1.3836e-01, -1.3799e-01,  2.1410e-01,\n                       -2.1200e-01,  7.3232e-02,  8.7003e-02,  1.1703e-01, -5.3059e-02,\n                       -5.3784e-02,  1.1519e-01,  7.3166e-02,  3.4589e-02, -3.3397e-01,\n                        3.9484e-02,  8.8901e-02, -2.7986e-02,  2.6741e-01,  5.7183e-02,\n                        3.1436e-02,  2.7118e-02, -3.8850e-01, -1.9298e-01,  1.1414e-01,\n                        3.1768e-03,  2.3530e-01,  1.3806e-01,  1.3065e-01,  1.1816e-01,\n                       -1.5503e-01,  2.5603e-02, -1.0427e-01,  1.2805e-01,  8.2148e-02,\n                       -8.8521e-02, -9.2537e-02,  6.4557e-03, -1.9606e-01,  3.1569e-01,\n                        1.3345e-02,  6.7613e-02, -1.6280e-01,  4.5851e-02, -2.0743e-01,\n                       -6.0664e-02,  1.2776e-01, -1.1949e-01,  2.8083e-04,  7.8145e-02,\n                       -3.1539e-01,  2.3603e-01,  1.6973e-01, -1.2210e-01,  2.0239e-01,\n                        2.6220e-01,  2.3675e-01,  6.7010e-02, -1.7368e-01, -1.0684e-01,\n                       -6.1893e-02,  2.0215e-01,  6.3546e-02, -4.3812e-01, -3.0266e-01,\n                        2.0989e-01,  2.1637e-02, -2.5151e-01, -2.1225e-01,  1.3799e-01,\n                       -1.5165e-01,  1.3150e-01,  8.2220e-03, -2.8383e-01, -4.9950e-02,\n                        1.0766e-01,  2.8067e-01, -4.8331e-02,  5.8193e-03, -1.5156e-01],\n                      [ 4.5549e-02, -3.2937e-01,  1.3926e-01, -1.2631e-01, -2.4602e-01,\n                        2.5034e-01,  1.7502e-01, -1.6154e-01,  1.2609e-02,  1.4379e-01,\n                       -2.1920e-02,  4.7759e-02, -2.4111e-02,  2.1328e-01,  1.5548e-01,\n                        8.8537e-04,  7.4023e-02, -8.4056e-02, -3.4154e-02,  6.6948e-02,\n                        7.4504e-02, -1.5853e-01, -3.5599e-01,  6.9947e-02, -1.2079e-01,\n                        2.2271e-01,  8.6329e-02, -1.9473e-01, -2.9102e-01, -1.0333e-01,\n                        3.2022e-01, -3.0189e-01, -3.5928e-01, -1.3722e-01, -1.8603e-01,\n                       -1.2348e-01,  3.3281e-02, -3.4102e-02,  3.8795e-02, -4.9017e-02,\n                       -3.7599e-01, -1.3301e-02, -8.6158e-02, -1.2150e-01, -2.1676e-02,\n                        2.1863e-02,  7.4893e-02,  3.6219e-01,  5.8581e-02, -8.5426e-02,\n                        1.1926e-01, -1.4358e-01,  3.4640e-02,  1.9630e-01, -1.4916e-01,\n                       -1.0293e-01,  1.0394e-01, -2.2568e-01, -1.0172e-02,  9.6917e-02,\n                       -2.7398e-02, -9.3723e-02,  1.3202e-01,  1.3827e-01, -1.4890e-01,\n                        2.5436e-01,  1.9783e-01, -2.9407e-01,  9.9784e-02,  4.4406e-02,\n                       -7.9650e-02, -1.7092e-02, -1.2206e-01,  4.3224e-02, -5.4932e-02,\n                        1.0750e-01, -1.6947e-02,  2.8767e-02,  1.7416e-01,  3.3609e-02,\n                        1.4360e-01, -5.2645e-03,  7.3149e-03, -1.9913e-01,  3.2108e-01,\n                       -2.1388e-01,  2.8312e-01,  1.0912e-01, -3.8136e-01,  6.9004e-02,\n                        5.5016e-02, -1.3788e-01, -3.5151e-02, -2.1483e-01,  1.6661e-01,\n                       -6.7688e-02,  4.2335e-02,  7.8600e-02, -7.7628e-02,  1.7003e-01],\n                      [ 1.5111e-01, -2.9172e-02,  3.5418e-02, -1.6922e-01,  3.4897e-02,\n                        1.0284e-02,  3.0777e-03,  8.4862e-03, -5.6651e-02, -3.7017e-02,\n                        3.0599e-01, -7.3664e-02,  2.0809e-01,  5.0566e-02, -1.9702e-01,\n                       -3.5670e-01,  2.0254e-02,  1.2586e-01, -1.0769e-01, -8.6006e-03,\n                       -3.2561e-02,  1.9107e-02,  1.9194e-02,  4.4853e-02,  7.1435e-02,\n                       -2.5717e-01,  8.5607e-03,  2.4253e-01,  2.2303e-02, -7.2606e-02,\n                        1.6885e-01, -8.3900e-02,  3.2682e-03, -9.9519e-02, -6.0555e-02,\n                        1.6216e-01,  5.3320e-03, -2.5547e-01,  3.6941e-02,  1.5023e-02,\n                        2.3826e-01,  1.3893e-01, -2.8785e-01,  1.7102e-01, -3.0905e-01,\n                        3.0923e-01,  7.7996e-03,  6.9929e-02, -1.3997e-01,  1.6482e-01,\n                        1.5537e-01, -2.4783e-01,  1.7895e-02, -3.9834e-02,  2.5855e-02,\n                       -1.5133e-01, -1.7889e-01, -2.0479e-02, -9.6038e-02,  1.4627e-01,\n                       -8.3317e-02,  6.5707e-02,  6.7930e-02,  7.3314e-02, -4.4974e-01,\n                       -1.7597e-02,  2.2668e-01, -3.3720e-02, -3.6421e-02, -2.2357e-02,\n                       -2.5198e-01, -7.7218e-02,  1.7671e-01, -1.9784e-02,  1.2589e-01,\n                        1.2041e-01,  3.4781e-02, -2.8693e-01,  2.8062e-01,  7.8153e-02,\n                        2.5505e-02,  2.7705e-01, -7.4147e-02,  9.2981e-02, -1.7399e-01,\n                        2.8814e-01, -7.4974e-02, -1.9163e-01, -3.2187e-02,  2.3916e-01,\n                        2.8392e-02, -2.8639e-01, -1.2015e-01,  1.6087e-02, -2.8548e-02,\n                       -1.0913e-01,  1.3903e-01,  6.4719e-02, -9.5567e-02,  1.8381e-01],\n                      [ 3.1033e-02,  1.7830e-01,  2.5752e-01,  1.5062e-01, -3.4280e-01,\n                        4.2008e-02,  1.7239e-01, -2.4337e-02,  2.0798e-01,  8.1109e-02,\n                       -7.5950e-02, -2.5646e-01, -1.8679e-01,  1.1454e-01,  2.1690e-01,\n                       -1.9194e-02,  8.4854e-03, -7.5685e-02, -1.7527e-01, -4.6495e-02,\n                        1.0072e-01,  2.0671e-02, -1.1286e-01, -1.1659e-02, -1.8186e-01,\n                       -1.4148e-01,  4.4997e-02, -5.1656e-02,  1.7820e-02, -8.1022e-02,\n                       -9.2511e-03, -2.0208e-01,  6.4713e-02, -6.8079e-02, -7.2191e-02,\n                        1.4452e-01,  1.4581e-01,  5.0121e-02,  3.7986e-02, -2.1843e-01,\n                       -1.2723e-01, -4.5812e-01, -1.2715e-01,  1.6358e-01,  1.6977e-02,\n                       -6.5124e-02,  2.1845e-01, -1.6269e-01, -1.4208e-01, -7.9777e-02,\n                       -8.2048e-02, -1.4343e-01, -1.7499e-01,  1.4119e-01,  2.2970e-02,\n                       -1.9767e-01, -2.3665e-01,  6.3584e-02, -1.0071e-01,  4.4691e-02,\n                       -2.4219e-01, -5.1440e-02, -1.2980e-01, -9.6080e-02,  1.7718e-01,\n                        2.7449e-01, -1.4481e-02, -5.5667e-02, -8.2628e-02,  9.0358e-03,\n                        4.4831e-02, -6.8165e-02, -1.3093e-01, -3.6029e-02, -7.4971e-02,\n                        2.6161e-02, -1.5944e-01,  4.4281e-02,  2.8262e-01,  4.0873e-02,\n                        2.8024e-01,  1.2726e-01, -2.5343e-01,  6.3290e-02,  4.2691e-01,\n                        4.7687e-02, -3.1857e-01, -1.5173e-01,  1.4620e-02,  1.4307e-01,\n                       -5.5083e-02,  3.3334e-02, -1.2067e-02, -5.8100e-02,  1.4175e-02,\n                       -2.3431e-01, -5.3943e-03, -1.6466e-01, -9.7850e-02, -5.3263e-02],\n                      [ 5.0510e-03,  1.3780e-01,  7.7013e-03, -1.9900e-01,  1.1526e-01,\n                       -2.2321e-01,  1.2783e-01, -4.2528e-02,  2.4541e-02,  6.8951e-02,\n                       -8.2531e-02,  1.1933e-01, -1.4049e-01, -3.1701e-01, -6.0143e-02,\n                        2.0565e-01, -5.6374e-02, -1.2159e-01, -1.1481e-01,  2.0112e-01,\n                       -1.7572e-01,  1.7722e-01,  3.4915e-01,  1.5759e-01, -7.8732e-02,\n                        1.3246e-01,  1.2148e-01,  1.1713e-01, -1.6928e-01,  2.5623e-01,\n                        8.0292e-02,  1.3510e-01, -7.0861e-02,  3.6881e-01,  8.8336e-03,\n                        1.5333e-01,  3.9359e-01,  1.4765e-01, -9.0313e-02,  3.1892e-01,\n                        1.2124e-01, -1.2057e-01,  5.0005e-01,  1.6385e-02,  2.1095e-01,\n                        1.3986e-01,  5.9206e-02,  3.6988e-02,  3.5797e-02, -1.1947e-01,\n                        1.3898e-01,  2.2046e-02,  3.3298e-01, -2.5739e-01,  2.8385e-01,\n                        8.0965e-02, -1.0629e-01, -7.0083e-02,  1.4224e-01, -1.1385e-01,\n                       -7.9261e-02,  1.4411e-01,  3.7881e-01,  1.3646e-01, -4.8576e-02,\n                        2.4919e-01, -6.2823e-03, -2.1802e-02, -1.1620e-01,  4.7435e-03,\n                        2.8718e-01,  2.3164e-01, -2.2364e-01,  1.2649e-01,  1.0307e-01,\n                       -2.8665e-01, -8.2033e-02, -8.4376e-02, -2.3219e-03, -1.3738e-01,\n                        2.4022e-01,  1.3639e-01,  1.7764e-01,  2.2439e-01,  5.0590e-02,\n                       -3.5573e-02,  5.9362e-02,  4.1922e-02,  1.5764e-01,  5.3331e-02,\n                        2.0781e-01, -1.8378e-02,  1.3118e-01,  2.5940e-02,  1.2575e-01,\n                       -1.2129e-01, -1.1677e-01,  1.5630e-01, -7.4345e-02,  2.6070e-01],\n                      [ 3.3568e-01, -4.1193e-01,  6.7678e-02,  2.6240e-01, -4.4249e-02,\n                        3.7272e-01,  1.7165e-01, -9.8275e-02,  1.4822e-01, -9.6477e-02,\n                       -2.3517e-01, -1.1161e-01,  2.5371e-01,  9.0721e-02, -6.7436e-02,\n                       -5.7062e-02, -2.0155e-01,  1.1515e-01, -1.5757e-01, -1.1691e-01,\n                       -5.9948e-03, -2.0401e-01,  5.1847e-02,  3.6368e-01,  1.5088e-01,\n                       -1.2245e-01, -2.3492e-01,  1.1386e-01,  1.2807e-01,  8.2066e-02,\n                        5.5501e-02, -2.4966e-01,  1.0725e-01,  1.7729e-01,  3.3126e-02,\n                        2.9522e-01, -2.0526e-01, -1.9354e-01, -5.2446e-02,  4.3908e-02,\n                        7.5704e-02,  4.5811e-01, -3.1236e-01,  2.4373e-02,  1.9210e-01,\n                        1.8190e-01, -3.5059e-01, -1.0065e-01,  8.6922e-02, -5.0844e-02,\n                       -1.7626e-01,  1.4946e-01,  1.5854e-02,  9.0508e-02,  1.9634e-01,\n                        2.2340e-01,  1.1015e-01, -6.9216e-02,  7.6781e-04,  7.2535e-02,\n                       -9.6303e-02, -2.2499e-01, -1.5230e-01,  1.5421e-02, -2.9943e-01,\n                        7.6234e-02,  3.8123e-02,  5.8996e-02, -7.8620e-02, -1.6453e-01,\n                       -2.0467e-02, -1.9656e-01,  2.0534e-01,  1.0508e-01,  4.4490e-01,\n                        5.7262e-02, -6.0566e-03, -1.0305e-01, -2.7636e-01, -1.6134e-02,\n                        2.7858e-01,  2.9741e-01, -1.0656e-01, -1.0447e-01, -1.2146e-01,\n                        8.2412e-02,  1.7439e-01, -2.5953e-01, -2.2928e-01,  1.0705e-01,\n                       -2.1122e-01,  1.4532e-01, -2.8558e-01, -9.3136e-02, -1.4567e-01,\n                        2.2352e-01,  3.0887e-01,  3.5405e-02,  2.4466e-02, -8.6671e-02],\n                      [ 2.7182e-01, -8.4312e-02,  4.9763e-01, -2.4491e-02, -3.7331e-01,\n                       -9.4545e-02, -1.1321e-01,  5.3433e-02, -4.2526e-02,  2.3678e-01,\n                       -1.3333e-02,  7.0440e-02,  3.5863e-01,  2.4373e-01,  3.1374e-01,\n                       -1.7115e-01,  2.9026e-01,  2.6311e-01,  6.7193e-02,  5.3727e-01,\n                       -1.1409e-01,  2.6690e-02,  1.7616e-01,  8.3436e-02,  1.2961e-01,\n                       -9.8725e-02, -1.4191e-01, -7.8375e-02,  3.7401e-01, -3.7170e-02,\n                       -7.2932e-02, -4.2012e-01,  1.6882e-01,  2.8922e-01, -2.3207e-01,\n                        8.1252e-02,  2.9824e-01,  2.8073e-02,  2.4123e-01,  9.5004e-02,\n                       -2.7293e-01, -2.1158e-01,  1.0432e-01,  3.8381e-01,  1.4865e-02,\n                        1.2100e-01,  8.2986e-02, -3.0987e-02,  6.0708e-02,  2.3353e-02,\n                       -1.8171e-01, -6.4023e-02,  6.1864e-02,  7.8294e-02,  1.9794e-01,\n                        2.0037e-03,  2.5499e-01, -6.2220e-03, -7.0709e-02, -1.7361e-01,\n                        4.2292e-01,  1.3500e-01,  2.2525e-01, -1.6448e-01, -2.9859e-01,\n                       -4.2182e-02,  6.3935e-02, -1.9614e-02,  1.7099e-01,  1.4383e-01,\n                       -1.2560e-01,  3.1411e-01,  5.5875e-02, -2.2565e-02,  3.8038e-01,\n                        9.7426e-02,  8.0355e-02,  1.2736e-01, -8.8997e-02,  2.3058e-01,\n                       -4.4863e-02,  1.8214e-01, -1.6175e-01,  5.1860e-02,  3.9097e-02,\n                        1.4480e-01, -3.1867e-02, -2.1472e-01, -3.5558e-01,  1.4801e-01,\n                       -3.2803e-01,  6.2648e-03, -3.0232e-02, -2.2240e-01,  1.6228e-01,\n                        1.1317e-01,  3.4281e-01, -8.8815e-02,  1.3721e-02, -2.9453e-01]])),\n             ('layer.2.bias',\n              tensor([-0.0222, -0.0956, -0.2535, -0.0579,  0.1537, -0.0625,  0.1580,  0.0186,\n                      -0.0426,  0.2957])),\n             ('layer.4.weight',\n              tensor([[ 0.2748, -0.1055, -0.7688, -0.4583,  0.2742, -0.2357,  0.5090,  0.4272,\n                       -0.0809,  0.5585],\n                      [-0.1408,  0.1498, -0.2154, -0.2699,  0.0629,  0.0793,  0.1684,  0.5188,\n                       -0.1104, -0.6264],\n                      [-0.2100, -0.8918,  0.0080,  0.0732, -0.0411,  0.2633, -0.2573, -1.0606,\n                        0.4177,  0.2667]])),\n             ('layer.4.bias', tensor([ 0.2499, -0.2728, -0.0340]))])"
     },
     "metadata": {}
    }
   ],
   "source": [
    "## パラメータ初期化関数\n",
    "def initializer(layer):\n",
    "    if type(layer) == nn.Linear:\n",
    "        nn.init.kaiming_normal_(layer.weight)\n",
    "        layer.bias.data.fill_(0.0)\n",
    "\n",
    "## パラメータ初期化する\n",
    "model.apply(initializer)\n",
    "\n",
    "n_epoch = 10\n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "# 1エポック内1データあたりの損失・正解率の平均\n",
    "avg_loss = 0.0\n",
    "avg_acc = 0.0\n",
    "avg_val_loss = 0.0\n",
    "avg_val_acc = 0.0\n",
    "\n",
    "## 学習する\n",
    "for epoch in range(n_epoch):\n",
    "    ## epoch内のloss, accの保存\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    total_val_loss = 0.0\n",
    "    total_val_acc = 0.0\n",
    "    ## 累計データ数の保存\n",
    "    total_train = 0\n",
    "    total_val = 0\n",
    "\n",
    "    ## 訓練\n",
    "    for X_train, y_train in loader_train:\n",
    "       loss, acc = train_step(X_train, y_train)\n",
    "       total_loss += loss\n",
    "       total_acc += acc\n",
    "       total_train += len(y_train)\n",
    "\n",
    "    ## 評価\n",
    "    for X_val, y_val in loader_valid:\n",
    "        val_loss, val_acc = valid_step(X_val, y_val)\n",
    "        total_val_loss += val_loss\n",
    "        total_val_acc += val_acc\n",
    "        total_val += len(y_val)\n",
    "\n",
    "    # 平均計算\n",
    "    avg_loss = total_loss/total_train\n",
    "    avg_val_loss = total_val_loss/total_val\n",
    "    avg_acc = total_acc/total_train\n",
    "    avg_val_acc = total_val_acc/total_val\n",
    "\n",
    "    print(f'epoch{epoch+1}/{n_epoch}: [loss:{avg_loss:.4f}, acc:{avg_acc:.4f}, val_loss:{avg_val_loss:.4f}, val_acc:{avg_val_acc:.4f}]')\n",
    "\n",
    "    # リストに追加\n",
    "    train_history.append(avg_loss)\n",
    "    val_history.append(avg_val_loss)\n",
    "\n",
    "print('Done!')\n",
    "display(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "y_pred_proba tensor([[-4.3099e+00,  3.3991e+00, -1.7093e+01],\n        [ 5.5031e+00, -1.0094e+00, -5.6576e+00],\n        [-1.4447e+00, -9.3380e+00,  8.5164e+00],\n        [-2.6020e-01,  2.5684e+00, -1.2309e+01],\n        [-7.6004e-01, -8.2141e+00,  7.8958e+00],\n        [-5.4209e+00,  2.8095e+00, -1.6570e+01],\n        [-1.0396e+00, -8.1515e+00,  7.9747e+00],\n        [ 2.3756e+00, -2.7869e-01, -2.7388e+00],\n        [ 3.4171e+00, -1.9792e-01, -4.5037e+00],\n        [ 2.5493e+00, -8.0244e-01, -1.9612e+00],\n        [-3.0930e-01,  1.9866e+00, -1.0883e+01],\n        [ 1.6333e+00, -4.5852e-01, -2.0345e+00],\n        [ 2.6356e+00, -1.0244e-01, -3.9338e+00],\n        [ 2.1095e+00,  2.8288e-01, -5.0584e+00],\n        [ 1.0010e+00,  4.1020e-01, -4.7767e+00],\n        [ 6.9603e-02, -8.3832e+00,  7.9066e+00],\n        [ 3.7599e-01,  7.0462e-01, -5.3756e+00],\n        [ 3.1974e+00,  4.2757e-01, -6.3812e+00],\n        [ 1.6721e-02, -8.0253e+00,  7.6775e+00],\n        [-1.2350e+00, -8.1819e+00,  7.6003e+00],\n        [-2.5979e+00,  2.7531e+00, -1.3064e+01],\n        [ 1.0587e+00,  4.3130e-01, -5.2706e+00],\n        [-5.9658e-01, -8.5330e+00,  8.1232e+00],\n        [-1.3008e-02, -9.1741e+00,  8.9000e+00],\n        [-1.4873e+00,  2.0740e+00, -9.7985e+00],\n        [-1.3284e+00, -9.8073e+00,  9.7161e+00],\n        [-1.0458e+00, -8.1049e+00,  7.6411e+00],\n        [ 2.2738e+00, -6.7559e-01, -2.0207e+00],\n        [ 4.7730e+00, -1.4728e+00, -4.0757e+00],\n        [-7.5171e-01, -7.6087e+00,  7.3559e+00]], grad_fn=<AddmmBackward>)\ny_pred [1 0 2 1 2 1 2 0 0 0 1 0 0 0 0 2 1 0 2 2 1 0 2 2 1 2 2 0 0 2]\ny_test [1 0 2 1 2 1 2 0 0 0 1 0 0 0 0 2 0 0 2 2 1 0 2 2 1 2 2 0 0 2]\n"
    }
   ],
   "source": [
    "# 予測\n",
    "## Tensor変換\n",
    "t_X_test = torch.from_numpy(X_test).float()\n",
    "y_pred_proba = model(t_X_test)\n",
    "y_pred = torch.argmax(y_pred_proba, axis=1)\n",
    "print(\"y_pred_proba\", y_pred_proba)\n",
    "print(\"y_pred\", y_pred.flatten().int().detach().numpy())\n",
    "print(\"y_test\", y_test.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 372.103125 248.518125\" width=\"372.103125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.518125 \r\nL 372.103125 248.518125 \r\nL 372.103125 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 30.103125 224.64 \r\nL 364.903125 224.64 \r\nL 364.903125 7.2 \r\nL 30.103125 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m7db41cdced\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"45.321307\" xlink:href=\"#m7db41cdced\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n      </defs>\r\n      <g transform=\"translate(42.140057 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"112.95767\" xlink:href=\"#m7db41cdced\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 2 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      </defs>\r\n      <g transform=\"translate(109.77642 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"180.594034\" xlink:href=\"#m7db41cdced\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 4 -->\r\n      <defs>\r\n       <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n      </defs>\r\n      <g transform=\"translate(177.412784 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"248.230398\" xlink:href=\"#m7db41cdced\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 6 -->\r\n      <defs>\r\n       <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n      </defs>\r\n      <g transform=\"translate(245.049148 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"315.866761\" xlink:href=\"#m7db41cdced\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 8 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n      </defs>\r\n      <g transform=\"translate(312.685511 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-56\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_6\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m7194bb34d3\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m7194bb34d3\" y=\"208.74443\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 0.1 -->\r\n      <defs>\r\n       <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 212.543649)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-49\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m7194bb34d3\" y=\"168.183849\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0.2 -->\r\n      <g transform=\"translate(7.2 171.983068)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m7194bb34d3\" y=\"127.623268\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0.3 -->\r\n      <defs>\r\n       <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 131.422486)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-51\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m7194bb34d3\" y=\"87.062687\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 0.4 -->\r\n      <g transform=\"translate(7.2 90.861905)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m7194bb34d3\" y=\"46.502106\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 0.5 -->\r\n      <defs>\r\n       <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 50.301324)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_11\">\r\n    <path clip-path=\"url(#pe3acb61d3a)\" d=\"M 45.321307 17.083636 \r\nL 79.139489 135.212624 \r\nL 112.95767 158.818962 \r\nL 146.775852 179.331096 \r\nL 180.594034 192.279407 \r\nL 214.412216 197.00826 \r\nL 248.230398 207.941692 \r\nL 282.04858 210.492701 \r\nL 315.866761 213.644308 \r\nL 349.684943 214.756364 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_12\">\r\n    <path clip-path=\"url(#pe3acb61d3a)\" d=\"M 45.321307 91.260111 \r\nL 79.139489 106.86012 \r\nL 112.95767 74.87175 \r\nL 146.775852 104.391571 \r\nL 180.594034 90.683603 \r\nL 214.412216 103.900399 \r\nL 248.230398 131.233278 \r\nL 282.04858 41.265777 \r\nL 315.866761 122.442274 \r\nL 349.684943 133.142242 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 30.103125 224.64 \r\nL 30.103125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 364.903125 224.64 \r\nL 364.903125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 30.103125 224.64 \r\nL 364.903125 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 30.103125 7.2 \r\nL 364.903125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"legend_1\">\r\n    <g id=\"patch_7\">\r\n     <path d=\"M 286.7625 44.834375 \r\nL 357.903125 44.834375 \r\nQ 359.903125 44.834375 359.903125 42.834375 \r\nL 359.903125 14.2 \r\nQ 359.903125 12.2 357.903125 12.2 \r\nL 286.7625 12.2 \r\nQ 284.7625 12.2 284.7625 14.2 \r\nL 284.7625 42.834375 \r\nQ 284.7625 44.834375 286.7625 44.834375 \r\nz\r\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\r\n    </g>\r\n    <g id=\"line2d_13\">\r\n     <path d=\"M 288.7625 20.298437 \r\nL 308.7625 20.298437 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_14\"/>\r\n    <g id=\"text_11\">\r\n     <!-- loss -->\r\n     <defs>\r\n      <path d=\"M 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\n\" id=\"DejaVuSans-108\"/>\r\n      <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n      <path d=\"M 44.28125 53.078125 \r\nL 44.28125 44.578125 \r\nQ 40.484375 46.53125 36.375 47.5 \r\nQ 32.28125 48.484375 27.875 48.484375 \r\nQ 21.1875 48.484375 17.84375 46.4375 \r\nQ 14.5 44.390625 14.5 40.28125 \r\nQ 14.5 37.15625 16.890625 35.375 \r\nQ 19.28125 33.59375 26.515625 31.984375 \r\nL 29.59375 31.296875 \r\nQ 39.15625 29.25 43.1875 25.515625 \r\nQ 47.21875 21.78125 47.21875 15.09375 \r\nQ 47.21875 7.46875 41.1875 3.015625 \r\nQ 35.15625 -1.421875 24.609375 -1.421875 \r\nQ 20.21875 -1.421875 15.453125 -0.5625 \r\nQ 10.6875 0.296875 5.421875 2 \r\nL 5.421875 11.28125 \r\nQ 10.40625 8.6875 15.234375 7.390625 \r\nQ 20.0625 6.109375 24.8125 6.109375 \r\nQ 31.15625 6.109375 34.5625 8.28125 \r\nQ 37.984375 10.453125 37.984375 14.40625 \r\nQ 37.984375 18.0625 35.515625 20.015625 \r\nQ 33.0625 21.96875 24.703125 23.78125 \r\nL 21.578125 24.515625 \r\nQ 13.234375 26.265625 9.515625 29.90625 \r\nQ 5.8125 33.546875 5.8125 39.890625 \r\nQ 5.8125 47.609375 11.28125 51.796875 \r\nQ 16.75 56 26.8125 56 \r\nQ 31.78125 56 36.171875 55.265625 \r\nQ 40.578125 54.546875 44.28125 53.078125 \r\nz\r\n\" id=\"DejaVuSans-115\"/>\r\n     </defs>\r\n     <g transform=\"translate(316.7625 23.798437)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"line2d_15\">\r\n     <path d=\"M 288.7625 34.976562 \r\nL 308.7625 34.976562 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_16\"/>\r\n    <g id=\"text_12\">\r\n     <!-- val_loss -->\r\n     <defs>\r\n      <path d=\"M 2.984375 54.6875 \r\nL 12.5 54.6875 \r\nL 29.59375 8.796875 \r\nL 46.6875 54.6875 \r\nL 56.203125 54.6875 \r\nL 35.6875 0 \r\nL 23.484375 0 \r\nz\r\n\" id=\"DejaVuSans-118\"/>\r\n      <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n      <path d=\"M 50.984375 -16.609375 \r\nL 50.984375 -23.578125 \r\nL -0.984375 -23.578125 \r\nL -0.984375 -16.609375 \r\nz\r\n\" id=\"DejaVuSans-95\"/>\r\n     </defs>\r\n     <g transform=\"translate(316.7625 38.476562)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-118\"/>\r\n      <use x=\"59.179688\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"120.458984\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"148.242188\" xlink:href=\"#DejaVuSans-95\"/>\r\n      <use x=\"198.242188\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"226.025391\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"287.207031\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"339.306641\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"pe3acb61d3a\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"30.103125\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "plt.plot(train_history, label=\"loss\")\n",
    "plt.plot(val_history, label=\"val_loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "        Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n0        1          60       RL         65.0     8450   Pave   NaN      Reg   \n1        2          20       RL         80.0     9600   Pave   NaN      Reg   \n2        3          60       RL         68.0    11250   Pave   NaN      IR1   \n3        4          70       RL         60.0     9550   Pave   NaN      IR1   \n4        5          60       RL         84.0    14260   Pave   NaN      IR1   \n...    ...         ...      ...          ...      ...    ...   ...      ...   \n1455  1456          60       RL         62.0     7917   Pave   NaN      Reg   \n1456  1457          20       RL         85.0    13175   Pave   NaN      Reg   \n1457  1458          70       RL         66.0     9042   Pave   NaN      Reg   \n1458  1459          20       RL         68.0     9717   Pave   NaN      Reg   \n1459  1460          20       RL         75.0     9937   Pave   NaN      Reg   \n\n     LandContour Utilities  ... PoolArea PoolQC  Fence MiscFeature MiscVal  \\\n0            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n1            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n2            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n3            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n4            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n...          ...       ...  ...      ...    ...    ...         ...     ...   \n1455         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n1456         Lvl    AllPub  ...        0    NaN  MnPrv         NaN       0   \n1457         Lvl    AllPub  ...        0    NaN  GdPrv        Shed    2500   \n1458         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n1459         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n\n     MoSold YrSold  SaleType  SaleCondition  SalePrice  \n0         2   2008        WD         Normal     208500  \n1         5   2007        WD         Normal     181500  \n2         9   2008        WD         Normal     223500  \n3         2   2006        WD        Abnorml     140000  \n4        12   2008        WD         Normal     250000  \n...     ...    ...       ...            ...        ...  \n1455      8   2007        WD         Normal     175000  \n1456      2   2010        WD         Normal     210000  \n1457      5   2010        WD         Normal     266500  \n1458      4   2010        WD         Normal     142125  \n1459      6   2008        WD         Normal     147500  \n\n[1460 rows x 81 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>MSSubClass</th>\n      <th>MSZoning</th>\n      <th>LotFrontage</th>\n      <th>LotArea</th>\n      <th>Street</th>\n      <th>Alley</th>\n      <th>LotShape</th>\n      <th>LandContour</th>\n      <th>Utilities</th>\n      <th>...</th>\n      <th>PoolArea</th>\n      <th>PoolQC</th>\n      <th>Fence</th>\n      <th>MiscFeature</th>\n      <th>MiscVal</th>\n      <th>MoSold</th>\n      <th>YrSold</th>\n      <th>SaleType</th>\n      <th>SaleCondition</th>\n      <th>SalePrice</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>65.0</td>\n      <td>8450</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>208500</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>20</td>\n      <td>RL</td>\n      <td>80.0</td>\n      <td>9600</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>5</td>\n      <td>2007</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>181500</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>68.0</td>\n      <td>11250</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>9</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>223500</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>70</td>\n      <td>RL</td>\n      <td>60.0</td>\n      <td>9550</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2006</td>\n      <td>WD</td>\n      <td>Abnorml</td>\n      <td>140000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>84.0</td>\n      <td>14260</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>12</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>250000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1455</th>\n      <td>1456</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>62.0</td>\n      <td>7917</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>8</td>\n      <td>2007</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>175000</td>\n    </tr>\n    <tr>\n      <th>1456</th>\n      <td>1457</td>\n      <td>20</td>\n      <td>RL</td>\n      <td>85.0</td>\n      <td>13175</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>MnPrv</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2010</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>210000</td>\n    </tr>\n    <tr>\n      <th>1457</th>\n      <td>1458</td>\n      <td>70</td>\n      <td>RL</td>\n      <td>66.0</td>\n      <td>9042</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>GdPrv</td>\n      <td>Shed</td>\n      <td>2500</td>\n      <td>5</td>\n      <td>2010</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>266500</td>\n    </tr>\n    <tr>\n      <th>1458</th>\n      <td>1459</td>\n      <td>20</td>\n      <td>RL</td>\n      <td>68.0</td>\n      <td>9717</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>4</td>\n      <td>2010</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>142125</td>\n    </tr>\n    <tr>\n      <th>1459</th>\n      <td>1460</td>\n      <td>20</td>\n      <td>RL</td>\n      <td>75.0</td>\n      <td>9937</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>6</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>147500</td>\n    </tr>\n  </tbody>\n</table>\n<p>1460 rows × 81 columns</p>\n</div>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "(1460, 3)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "(1460, 1)"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(934, 3) (934, 1)\n(234, 3) (234, 1)\n(292, 3) (292, 1)\n"
    }
   ],
   "source": [
    "# 【問題5】House PricesをKerasで学習\n",
    "# 読み込み\n",
    "df_price = pd.read_csv('train.csv')\n",
    "display(df_price)\n",
    "# 抽出\n",
    "X = df_price[['LotArea', 'GrLivArea', 'YearBuilt']].values\n",
    "y = df_price['SalePrice'].values[:, np.newaxis]\n",
    "display(X.shape)\n",
    "display(y.shape)\n",
    "\n",
    "# 分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "# 標準化\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# 対数変換\n",
    "y_train = np.log(y_train)\n",
    "y_val = np.log(y_val)\n",
    "y_test = np.log(y_test)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "NNfor5(\n  (layer): Sequential(\n    (0): Linear(in_features=3, out_features=10, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=10, out_features=3, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=3, out_features=1, bias=True)\n  )\n)\n"
    }
   ],
   "source": [
    "# PyTorchでモデル構築\n",
    "n_features = 3\n",
    "n_nodes_1 = 10\n",
    "activation_1 = nn.ReLU()\n",
    "n_nodes_2 = 3\n",
    "activation_2 = nn.ReLU()\n",
    "n_output = 1\n",
    "\n",
    "## ネットワーク\n",
    "class NNfor5(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(NNfor5, self).__init__()\n",
    "        self.layer = nn.Sequential(nn.Linear(n_features, n_nodes_1), activation_1,\n",
    "                                   nn.Linear(n_nodes_1, n_nodes_2), activation_2,\n",
    "                                   nn.Linear(n_nodes_2, n_output))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.layer(X)\n",
    "\n",
    "        return X\n",
    "\n",
    "## インスタンス作成\n",
    "model = NNfor5()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ミニバッチ\n",
    "batch_size = 32\n",
    "\n",
    "## numpy配列をテンソルに変換\n",
    "t_X_train = torch.from_numpy(X_train).float()\n",
    "t_y_train = torch.from_numpy(y_train).float()\n",
    "t_X_valid = torch.from_numpy(X_val).float()\n",
    "t_y_valid = torch.from_numpy(y_val).float()\n",
    "\n",
    "## 特徴量とラベルをデータセットにまとめる\n",
    "dataset_train = TensorDataset(t_X_train, t_y_train)\n",
    "dataset_valid = TensorDataset(t_X_valid, t_y_valid)\n",
    "\n",
    "# データローダを作成\n",
    "loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "loader_valid = DataLoader(dataset_valid, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "## オプティマイザ\n",
    "lr = 0.001\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=lr\n",
    ")\n",
    "\n",
    "## 損失関数\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 学習(train)\n",
    "def train_step(X_train, y_train):\n",
    "    ## 訓練モード    \n",
    "    model.train()\n",
    "\n",
    "    ## 訓練データに対して予測\n",
    "    y_train_pred = model(X_train)\n",
    "\n",
    "    ## 勾配初期化\n",
    "    optimizer.zero_grad()\n",
    "    ## 損失の取得\n",
    "    loss = criterion(y_train_pred, y_train)\n",
    "    ## 勾配を計算\n",
    "    loss.backward()\n",
    "\n",
    "    ## パラメータ更新\n",
    "    optimizer.step()\n",
    "\n",
    "    ## 平均絶対誤差計算\n",
    "    with torch.no_grad():\n",
    "        mae = mean_absolute_error(y_train.detach().numpy(), y_train_pred.detach().numpy())\n",
    "\n",
    "    return (loss.item(), mae)\n",
    "\n",
    "## 評価(validation)\n",
    "def valid_step(X_val, y_val):\n",
    "    ## 評価モード\n",
    "    model.eval()\n",
    "\n",
    "    ## 予測\n",
    "    y_val_pred = model(X_val)\n",
    "\n",
    "    ## 損失計算\n",
    "    loss = criterion(y_val_pred, y_val)\n",
    "\n",
    "    ## 平均絶対誤差計算\n",
    "    with torch.no_grad():\n",
    "        mae = mean_absolute_error(y_val.detach().numpy(), y_val_pred.detach().numpy())\n",
    "\n",
    "    return (loss.item(), mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch1/100: [loss:4.1010, mae:11.2721, val_loss:4.2031, val_mae:11.0550]\nepoch2/100: [loss:3.8817, mae:10.9538, val_loss:3.9329, val_mae:10.6765]\nepoch3/100: [loss:3.6014, mae:10.5288, val_loss:3.5816, val_mae:10.1662]\nepoch4/100: [loss:3.2442, mae:9.9732, val_loss:3.1198, val_mae:9.4550]\nepoch5/100: [loss:2.7529, mae:9.1398, val_loss:2.5866, val_mae:8.5358]\nepoch6/100: [loss:2.2422, mae:8.1516, val_loss:2.1206, val_mae:7.6041]\nepoch7/100: [loss:1.8058, mae:7.1738, val_loss:1.7184, val_mae:6.6678]\nepoch8/100: [loss:1.4476, mae:6.2435, val_loss:1.3785, val_mae:5.7635]\nepoch9/100: [loss:1.1659, mae:5.4286, val_loss:1.1091, val_mae:4.9762]\nepoch10/100: [loss:0.9570, mae:4.7683, val_loss:0.9000, val_mae:4.3803]\nepoch11/100: [loss:0.7787, mae:4.2090, val_loss:0.7467, val_mae:3.9539]\nepoch12/100: [loss:0.6447, mae:3.7636, val_loss:0.6320, val_mae:3.6357]\nepoch13/100: [loss:0.5562, mae:3.4760, val_loss:0.5467, val_mae:3.3757]\nepoch14/100: [loss:0.4918, mae:3.2614, val_loss:0.4811, val_mae:3.1517]\nepoch15/100: [loss:0.4346, mae:3.0407, val_loss:0.4312, val_mae:2.9698]\nepoch16/100: [loss:0.3986, mae:2.8943, val_loss:0.3905, val_mae:2.8147]\nepoch17/100: [loss:0.3614, mae:2.7273, val_loss:0.3595, val_mae:2.6839]\nepoch18/100: [loss:0.3350, mae:2.6037, val_loss:0.3341, val_mae:2.5705]\nepoch19/100: [loss:0.3273, mae:2.5658, val_loss:0.3137, val_mae:2.4784]\nepoch20/100: [loss:0.2947, mae:2.4123, val_loss:0.2940, val_mae:2.3814]\nepoch21/100: [loss:0.2879, mae:2.3740, val_loss:0.2773, val_mae:2.2989]\nepoch22/100: [loss:0.2637, mae:2.2527, val_loss:0.2624, val_mae:2.2288]\nepoch23/100: [loss:0.2614, mae:2.2367, val_loss:0.2489, val_mae:2.1642]\nepoch24/100: [loss:0.2441, mae:2.1646, val_loss:0.2376, val_mae:2.1047]\nepoch25/100: [loss:0.2347, mae:2.1067, val_loss:0.2249, val_mae:2.0451]\nepoch26/100: [loss:0.2254, mae:2.0656, val_loss:0.2135, val_mae:1.9888]\nepoch27/100: [loss:0.2047, mae:1.9424, val_loss:0.2021, val_mae:1.9314]\nepoch28/100: [loss:0.1974, mae:1.9166, val_loss:0.1924, val_mae:1.8839]\nepoch29/100: [loss:0.1977, mae:1.9066, val_loss:0.1833, val_mae:1.8399]\nepoch30/100: [loss:0.1779, mae:1.8124, val_loss:0.1744, val_mae:1.7917]\nepoch31/100: [loss:0.1714, mae:1.7769, val_loss:0.1651, val_mae:1.7456]\nepoch32/100: [loss:0.1611, mae:1.7222, val_loss:0.1569, val_mae:1.7023]\nepoch33/100: [loss:0.1565, mae:1.7024, val_loss:0.1492, val_mae:1.6593]\nepoch34/100: [loss:0.1474, mae:1.6418, val_loss:0.1410, val_mae:1.6157]\nepoch35/100: [loss:0.1404, mae:1.6034, val_loss:0.1338, val_mae:1.5708]\nepoch36/100: [loss:0.1359, mae:1.5822, val_loss:0.1261, val_mae:1.5269]\nepoch37/100: [loss:0.1262, mae:1.5251, val_loss:0.1197, val_mae:1.4879]\nepoch38/100: [loss:0.1195, mae:1.4775, val_loss:0.1131, val_mae:1.4475]\nepoch39/100: [loss:0.1093, mae:1.4059, val_loss:0.1060, val_mae:1.4056]\nepoch40/100: [loss:0.1055, mae:1.3830, val_loss:0.1002, val_mae:1.3663]\nepoch41/100: [loss:0.0988, mae:1.3426, val_loss:0.0939, val_mae:1.3226]\nepoch42/100: [loss:0.0911, mae:1.2823, val_loss:0.0882, val_mae:1.2821]\nepoch43/100: [loss:0.0855, mae:1.2421, val_loss:0.0837, val_mae:1.2469]\nepoch44/100: [loss:0.0807, mae:1.2049, val_loss:0.0784, val_mae:1.2050]\nepoch45/100: [loss:0.0748, mae:1.1561, val_loss:0.0737, val_mae:1.1678]\nepoch46/100: [loss:0.0706, mae:1.1189, val_loss:0.0687, val_mae:1.1272]\nepoch47/100: [loss:0.0705, mae:1.1148, val_loss:0.0646, val_mae:1.0905]\nepoch48/100: [loss:0.0623, mae:1.0528, val_loss:0.0607, val_mae:1.0549]\nepoch49/100: [loss:0.0586, mae:1.0230, val_loss:0.0564, val_mae:1.0158]\nepoch50/100: [loss:0.0532, mae:0.9648, val_loss:0.0525, val_mae:0.9790]\nepoch51/100: [loss:0.0499, mae:0.9256, val_loss:0.0488, val_mae:0.9423]\nepoch52/100: [loss:0.0477, mae:0.9085, val_loss:0.0454, val_mae:0.9079]\nepoch53/100: [loss:0.0426, mae:0.8555, val_loss:0.0419, val_mae:0.8707]\nepoch54/100: [loss:0.0409, mae:0.8371, val_loss:0.0392, val_mae:0.8392]\nepoch55/100: [loss:0.0370, mae:0.7907, val_loss:0.0364, val_mae:0.8067]\nepoch56/100: [loss:0.0341, mae:0.7550, val_loss:0.0340, val_mae:0.7762]\nepoch57/100: [loss:0.0316, mae:0.7259, val_loss:0.0317, val_mae:0.7459]\nepoch58/100: [loss:0.0297, mae:0.6989, val_loss:0.0295, val_mae:0.7162]\nepoch59/100: [loss:0.0273, mae:0.6723, val_loss:0.0274, val_mae:0.6886]\nepoch60/100: [loss:0.0252, mae:0.6453, val_loss:0.0254, val_mae:0.6604]\nepoch61/100: [loss:0.0232, mae:0.6094, val_loss:0.0239, val_mae:0.6385]\nepoch62/100: [loss:0.0217, mae:0.5970, val_loss:0.0223, val_mae:0.6154]\nepoch63/100: [loss:0.0200, mae:0.5646, val_loss:0.0207, val_mae:0.5899]\nepoch64/100: [loss:0.0186, mae:0.5419, val_loss:0.0194, val_mae:0.5688]\nepoch65/100: [loss:0.0170, mae:0.5175, val_loss:0.0182, val_mae:0.5494]\nepoch66/100: [loss:0.0162, mae:0.5093, val_loss:0.0171, val_mae:0.5319]\nepoch67/100: [loss:0.0149, mae:0.4859, val_loss:0.0161, val_mae:0.5126]\nepoch68/100: [loss:0.0150, mae:0.4760, val_loss:0.0152, val_mae:0.4984]\nepoch69/100: [loss:0.0135, mae:0.4641, val_loss:0.0142, val_mae:0.4804]\nepoch70/100: [loss:0.0128, mae:0.4476, val_loss:0.0136, val_mae:0.4687]\nepoch71/100: [loss:0.0118, mae:0.4311, val_loss:0.0128, val_mae:0.4556]\nepoch72/100: [loss:0.0108, mae:0.4113, val_loss:0.0122, val_mae:0.4423]\nepoch73/100: [loss:0.0107, mae:0.4065, val_loss:0.0117, val_mae:0.4345]\nepoch74/100: [loss:0.0101, mae:0.3994, val_loss:0.0114, val_mae:0.4300]\nepoch75/100: [loss:0.0099, mae:0.3911, val_loss:0.0108, val_mae:0.4150]\nepoch76/100: [loss:0.0091, mae:0.3764, val_loss:0.0103, val_mae:0.4052]\nepoch77/100: [loss:0.0088, mae:0.3684, val_loss:0.0100, val_mae:0.3987]\nepoch78/100: [loss:0.0082, mae:0.3540, val_loss:0.0096, val_mae:0.3886]\nepoch79/100: [loss:0.0078, mae:0.3484, val_loss:0.0093, val_mae:0.3835]\nepoch80/100: [loss:0.0074, mae:0.3354, val_loss:0.0089, val_mae:0.3746]\nepoch81/100: [loss:0.0074, mae:0.3388, val_loss:0.0086, val_mae:0.3692]\nepoch82/100: [loss:0.0068, mae:0.3231, val_loss:0.0082, val_mae:0.3592]\nepoch83/100: [loss:0.0065, mae:0.3140, val_loss:0.0080, val_mae:0.3542]\nepoch84/100: [loss:0.0063, mae:0.3100, val_loss:0.0078, val_mae:0.3470]\nepoch85/100: [loss:0.0060, mae:0.3045, val_loss:0.0075, val_mae:0.3413]\nepoch86/100: [loss:0.0059, mae:0.3003, val_loss:0.0073, val_mae:0.3345]\nepoch87/100: [loss:0.0057, mae:0.2962, val_loss:0.0071, val_mae:0.3298]\nepoch88/100: [loss:0.0058, mae:0.2976, val_loss:0.0069, val_mae:0.3243]\nepoch89/100: [loss:0.0055, mae:0.2910, val_loss:0.0066, val_mae:0.3174]\nepoch90/100: [loss:0.0051, mae:0.2855, val_loss:0.0064, val_mae:0.3135]\nepoch91/100: [loss:0.0050, mae:0.2802, val_loss:0.0063, val_mae:0.3092]\nepoch92/100: [loss:0.0048, mae:0.2712, val_loss:0.0061, val_mae:0.3028]\nepoch93/100: [loss:0.0047, mae:0.2727, val_loss:0.0059, val_mae:0.3000]\nepoch94/100: [loss:0.0046, mae:0.2713, val_loss:0.0058, val_mae:0.2980]\nepoch95/100: [loss:0.0045, mae:0.2696, val_loss:0.0056, val_mae:0.2922]\nepoch96/100: [loss:0.0044, mae:0.2628, val_loss:0.0055, val_mae:0.2888]\nepoch97/100: [loss:0.0042, mae:0.2587, val_loss:0.0053, val_mae:0.2852]\nepoch98/100: [loss:0.0041, mae:0.2577, val_loss:0.0052, val_mae:0.2826]\nepoch99/100: [loss:0.0040, mae:0.2547, val_loss:0.0051, val_mae:0.2788]\nepoch100/100: [loss:0.0039, mae:0.2509, val_loss:0.0049, val_mae:0.2749]\nDone!\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "OrderedDict([('layer.0.weight',\n              tensor([[ 0.2202, -0.1152, -0.7598],\n                      [ 0.8974,  0.2736, -0.9688],\n                      [-0.2748,  0.8688, -1.3371],\n                      [ 0.8830,  0.6596, -0.0986],\n                      [ 0.3867, -0.9331, -0.9216],\n                      [-0.0939, -1.6006,  0.6810],\n                      [ 0.2627, -0.6191, -1.2107],\n                      [-0.4682,  0.0571, -1.4039],\n                      [ 0.2172, -0.0258,  1.1626],\n                      [-1.0451, -0.2601,  1.1364]])),\n             ('layer.0.bias',\n              tensor([ 1.0873, -0.5225,  1.0728,  1.2970, -0.3657,  0.7772,  0.8506, -0.6843,\n                       0.8390,  0.7751])),\n             ('layer.2.weight',\n              tensor([[ 1.3676, -0.8989,  0.0278,  0.1677, -0.3735,  0.1284,  0.6931, -0.7670,\n                        0.6672,  0.4866],\n                      [ 0.5381, -0.8158,  0.6016,  0.9449, -0.1460, -0.0056,  0.0900,  0.1746,\n                        0.7575,  0.6795],\n                      [ 0.4351, -0.0505, -0.6233, -0.5962, -0.3303, -0.0893,  0.4855,  0.4977,\n                       -0.0771,  0.3111]])),\n             ('layer.2.bias', tensor([ 0.6855,  0.5756, -0.1345])),\n             ('layer.4.weight', tensor([[ 1.6571,  1.1500, -0.1858]])),\n             ('layer.4.bias', tensor([0.4577]))])"
     },
     "metadata": {}
    }
   ],
   "source": [
    "## パラメータ初期化関数\n",
    "def initializer(layer):\n",
    "    if type(layer) == nn.Linear:\n",
    "        nn.init.kaiming_normal_(layer.weight)\n",
    "        layer.bias.data.fill_(0.0)\n",
    "\n",
    "## パラメータ初期化する\n",
    "model.apply(initializer)\n",
    "\n",
    "n_epoch = 100\n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "# 1エポック内1データあたりの損失・正解率の平均\n",
    "avg_loss = 0.0\n",
    "avg_mae = 0.0\n",
    "avg_val_loss = 0.0\n",
    "avg_val_mae = 0.0\n",
    "\n",
    "## 学習する\n",
    "for epoch in range(n_epoch):\n",
    "    ## epoch内のloss, accの保存\n",
    "    total_loss = 0.0\n",
    "    total_mae = 0.0\n",
    "    total_val_loss = 0.0\n",
    "    total_val_mae = 0.0\n",
    "    ## 累計データ数の保存\n",
    "    total_train = 0\n",
    "    total_val = 0\n",
    "\n",
    "    ## 訓練\n",
    "    for X_train, y_train in loader_train:\n",
    "       loss, mae = train_step(X_train, y_train)\n",
    "       total_loss += loss\n",
    "       total_mae += mae\n",
    "       total_train += len(y_train)\n",
    "\n",
    "    ## 評価\n",
    "    for X_val, y_val in loader_valid:\n",
    "        val_loss, val_mae = valid_step(X_val, y_val)\n",
    "        total_val_loss += val_loss\n",
    "        total_val_mae += val_mae\n",
    "        total_val += len(y_val)\n",
    "\n",
    "    # 平均計算\n",
    "    avg_loss = total_loss/total_train\n",
    "    avg_val_loss = total_val_loss/total_val\n",
    "    avg_mae = total_mae/len(loader_train)\n",
    "    avg_val_mae = total_val_mae/len(loader_valid)\n",
    "\n",
    "    print(f'epoch{epoch+1}/{n_epoch}: [loss:{avg_loss:.4f}, mae:{avg_mae:.4f}, val_loss:{avg_val_loss:.4f}, val_mae:{avg_val_mae:.4f}]')\n",
    "\n",
    "    # リストに追加\n",
    "    train_history.append(avg_loss)\n",
    "    val_history.append(avg_val_loss)\n",
    "\n",
    "print('Done!')\n",
    "display(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "y_pred_log [11.882249 11.601649 11.481923 11.721519 11.585099]\ny_test [12.20918779 11.79810441 11.60823564 12.16525065 11.38509209]\nmae 0.2669870817156004\n"
    }
   ],
   "source": [
    "# 予測\n",
    "## Tensor変換\n",
    "t_X_test = torch.from_numpy(X_test).float()\n",
    "y_pred_log = model(t_X_test).detach().numpy()\n",
    "print(\"y_pred_log\", y_pred_log.flatten()[:5])\n",
    "print(\"y_test\", y_test.flatten()[:5])\n",
    "print(\"mae\", mean_absolute_error(y_test, y_pred_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 362.5625 248.518125\" width=\"362.5625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.518125 \r\nL 362.5625 248.518125 \r\nL 362.5625 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 20.5625 224.64 \r\nL 355.3625 224.64 \r\nL 355.3625 7.2 \r\nL 20.5625 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"mb4c3d85c89\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"35.780682\" xlink:href=\"#mb4c3d85c89\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n      </defs>\r\n      <g transform=\"translate(32.599432 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"97.268285\" xlink:href=\"#mb4c3d85c89\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 20 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      </defs>\r\n      <g transform=\"translate(90.905785 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"158.755888\" xlink:href=\"#mb4c3d85c89\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 40 -->\r\n      <defs>\r\n       <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n      </defs>\r\n      <g transform=\"translate(152.393388 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"220.243492\" xlink:href=\"#mb4c3d85c89\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 60 -->\r\n      <defs>\r\n       <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n      </defs>\r\n      <g transform=\"translate(213.880992 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-54\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"281.731095\" xlink:href=\"#mb4c3d85c89\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 80 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n      </defs>\r\n      <g transform=\"translate(275.368595 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-56\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"343.218698\" xlink:href=\"#mb4c3d85c89\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 100 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g transform=\"translate(333.674948 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"mbac2e0c902\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#mbac2e0c902\" y=\"214.940479\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(7.2 218.739698)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#mbac2e0c902\" y=\"167.866997\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 1 -->\r\n      <g transform=\"translate(7.2 171.666216)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#mbac2e0c902\" y=\"120.793515\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 2 -->\r\n      <g transform=\"translate(7.2 124.592734)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#mbac2e0c902\" y=\"73.720033\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 3 -->\r\n      <defs>\r\n       <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 77.519252)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#mbac2e0c902\" y=\"26.646552\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 4 -->\r\n      <g transform=\"translate(7.2 30.44577)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_12\">\r\n    <path clip-path=\"url(#p9b1ca17334)\" d=\"M 35.780682 21.890695 \r\nL 38.855062 32.213281 \r\nL 41.929442 45.411329 \r\nL 45.003822 62.223739 \r\nL 48.078202 85.353733 \r\nL 51.152583 109.393188 \r\nL 54.226963 129.933024 \r\nL 57.301343 146.798929 \r\nL 60.375723 160.056368 \r\nL 63.450103 169.892082 \r\nL 66.524483 178.284457 \r\nL 69.598864 184.59325 \r\nL 72.673244 188.755972 \r\nL 75.747624 191.789169 \r\nL 78.822004 194.480215 \r\nL 81.896384 196.174694 \r\nL 84.970764 197.927507 \r\nL 88.045145 199.172736 \r\nL 91.119525 199.534608 \r\nL 94.193905 201.070156 \r\nL 97.268285 201.386702 \r\nL 100.342665 202.527716 \r\nL 103.417045 202.636076 \r\nL 106.491426 203.450142 \r\nL 109.565806 203.890991 \r\nL 112.640186 204.329338 \r\nL 115.714566 205.306482 \r\nL 118.788946 205.650286 \r\nL 121.863326 205.632816 \r\nL 124.937707 206.56561 \r\nL 128.012087 206.874378 \r\nL 131.086467 207.357115 \r\nL 134.160847 207.572723 \r\nL 137.235227 208.000364 \r\nL 140.309607 208.330207 \r\nL 143.383988 208.541882 \r\nL 146.458368 209.000456 \r\nL 149.532748 209.316354 \r\nL 152.607128 209.794896 \r\nL 155.681508 209.97386 \r\nL 158.755888 210.288757 \r\nL 161.830269 210.65119 \r\nL 164.904649 210.9158 \r\nL 167.979029 211.141452 \r\nL 171.053409 211.419697 \r\nL 174.127789 211.617373 \r\nL 177.202169 211.620545 \r\nL 180.27655 212.008802 \r\nL 183.35093 212.182393 \r\nL 186.42531 212.438223 \r\nL 189.49969 212.593417 \r\nL 192.57407 212.694529 \r\nL 195.64845 212.933054 \r\nL 198.722831 213.017109 \r\nL 201.797211 213.198496 \r\nL 204.871591 213.333544 \r\nL 207.945971 213.451155 \r\nL 211.020351 213.544405 \r\nL 214.094731 213.654619 \r\nL 217.169112 213.752695 \r\nL 220.243492 213.848873 \r\nL 223.317872 213.920175 \r\nL 226.392252 214.000922 \r\nL 229.466632 214.066922 \r\nL 232.541012 214.14159 \r\nL 235.615393 214.178006 \r\nL 238.689773 214.23878 \r\nL 241.764153 214.233422 \r\nL 244.838533 214.306436 \r\nL 247.912913 214.338463 \r\nL 250.987293 214.382747 \r\nL 254.061674 214.434431 \r\nL 257.136054 214.438044 \r\nL 260.210434 214.466251 \r\nL 263.284814 214.475698 \r\nL 266.359194 214.513892 \r\nL 269.433574 214.526074 \r\nL 272.507955 214.555168 \r\nL 275.582335 214.575415 \r\nL 278.656715 214.593271 \r\nL 281.731095 214.59075 \r\nL 284.805475 214.62052 \r\nL 287.879855 214.635011 \r\nL 290.954236 214.6443 \r\nL 294.028616 214.657301 \r\nL 297.102996 214.663753 \r\nL 300.177376 214.674351 \r\nL 303.251756 214.668809 \r\nL 306.326136 214.679727 \r\nL 309.400517 214.698658 \r\nL 312.474897 214.705121 \r\nL 315.549277 214.715539 \r\nL 318.623657 214.717783 \r\nL 321.698037 214.724541 \r\nL 324.772417 214.728693 \r\nL 327.846798 214.73545 \r\nL 330.921178 214.744413 \r\nL 333.995558 214.747413 \r\nL 337.069938 214.750978 \r\nL 340.144318 214.756364 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_13\">\r\n    <path clip-path=\"url(#p9b1ca17334)\" d=\"M 35.780682 17.083636 \r\nL 38.855062 29.806986 \r\nL 41.929442 46.341222 \r\nL 45.003822 68.08111 \r\nL 48.078202 93.179367 \r\nL 51.152583 115.114823 \r\nL 54.226963 134.04802 \r\nL 57.301343 150.049086 \r\nL 60.375723 162.732526 \r\nL 63.450103 172.572553 \r\nL 66.524483 179.791279 \r\nL 69.598864 185.192367 \r\nL 72.673244 189.204387 \r\nL 75.747624 192.292139 \r\nL 78.822004 194.641707 \r\nL 81.896384 196.559142 \r\nL 84.970764 198.017541 \r\nL 88.045145 199.215194 \r\nL 91.119525 200.174317 \r\nL 94.193905 201.100333 \r\nL 97.268285 201.88674 \r\nL 100.342665 202.588975 \r\nL 103.417045 203.222618 \r\nL 106.491426 203.757203 \r\nL 109.565806 204.352757 \r\nL 112.640186 204.889297 \r\nL 115.714566 205.424908 \r\nL 118.788946 205.884066 \r\nL 121.863326 206.31209 \r\nL 124.937707 206.733189 \r\nL 128.012087 207.169411 \r\nL 131.086467 207.555832 \r\nL 134.160847 207.91833 \r\nL 137.235227 208.302567 \r\nL 140.309607 208.643907 \r\nL 143.383988 209.003146 \r\nL 146.458368 209.305531 \r\nL 149.532748 209.617563 \r\nL 152.607128 209.952314 \r\nL 155.681508 210.224842 \r\nL 158.755888 210.51911 \r\nL 161.830269 210.788388 \r\nL 164.904649 210.999216 \r\nL 167.979029 211.250835 \r\nL 171.053409 211.469882 \r\nL 174.127789 211.708425 \r\nL 177.202169 211.898457 \r\nL 180.27655 212.085386 \r\nL 183.35093 212.283701 \r\nL 186.42531 212.467642 \r\nL 189.49969 212.642273 \r\nL 192.57407 212.802339 \r\nL 195.64845 212.966825 \r\nL 198.722831 213.095258 \r\nL 201.797211 213.224701 \r\nL 204.871591 213.34057 \r\nL 207.945971 213.448127 \r\nL 211.020351 213.55107 \r\nL 214.094731 213.651776 \r\nL 217.169112 213.743856 \r\nL 220.243492 213.815675 \r\nL 223.317872 213.890018 \r\nL 226.392252 213.965052 \r\nL 229.466632 214.026697 \r\nL 232.541012 214.085264 \r\nL 235.615393 214.13538 \r\nL 238.689773 214.184221 \r\nL 241.764153 214.227181 \r\nL 244.838533 214.272119 \r\nL 247.912913 214.302351 \r\nL 250.987293 214.336037 \r\nL 254.061674 214.36687 \r\nL 257.136054 214.387458 \r\nL 260.210434 214.402256 \r\nL 263.284814 214.431528 \r\nL 266.359194 214.453674 \r\nL 269.433574 214.468728 \r\nL 272.507955 214.489349 \r\nL 275.582335 214.504364 \r\nL 278.656715 214.520075 \r\nL 281.731095 214.533364 \r\nL 284.805475 214.552133 \r\nL 287.879855 214.562365 \r\nL 290.954236 214.575347 \r\nL 294.028616 214.586583 \r\nL 297.102996 214.598769 \r\nL 300.177376 214.607623 \r\nL 303.251756 214.61773 \r\nL 306.326136 214.627835 \r\nL 309.400517 214.637682 \r\nL 312.474897 214.645675 \r\nL 315.549277 214.655619 \r\nL 318.623657 214.663317 \r\nL 321.698037 214.66647 \r\nL 324.772417 214.676499 \r\nL 327.846798 214.683318 \r\nL 330.921178 214.690074 \r\nL 333.995558 214.694437 \r\nL 337.069938 214.701832 \r\nL 340.144318 214.707919 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 20.5625 224.64 \r\nL 20.5625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 355.3625 224.64 \r\nL 355.3625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 20.5625 224.64 \r\nL 355.3625 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 20.5625 7.2 \r\nL 355.3625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"legend_1\">\r\n    <g id=\"patch_7\">\r\n     <path d=\"M 277.221875 44.834375 \r\nL 348.3625 44.834375 \r\nQ 350.3625 44.834375 350.3625 42.834375 \r\nL 350.3625 14.2 \r\nQ 350.3625 12.2 348.3625 12.2 \r\nL 277.221875 12.2 \r\nQ 275.221875 12.2 275.221875 14.2 \r\nL 275.221875 42.834375 \r\nQ 275.221875 44.834375 277.221875 44.834375 \r\nz\r\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\r\n    </g>\r\n    <g id=\"line2d_14\">\r\n     <path d=\"M 279.221875 20.298437 \r\nL 299.221875 20.298437 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_15\"/>\r\n    <g id=\"text_12\">\r\n     <!-- loss -->\r\n     <defs>\r\n      <path d=\"M 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\n\" id=\"DejaVuSans-108\"/>\r\n      <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n      <path d=\"M 44.28125 53.078125 \r\nL 44.28125 44.578125 \r\nQ 40.484375 46.53125 36.375 47.5 \r\nQ 32.28125 48.484375 27.875 48.484375 \r\nQ 21.1875 48.484375 17.84375 46.4375 \r\nQ 14.5 44.390625 14.5 40.28125 \r\nQ 14.5 37.15625 16.890625 35.375 \r\nQ 19.28125 33.59375 26.515625 31.984375 \r\nL 29.59375 31.296875 \r\nQ 39.15625 29.25 43.1875 25.515625 \r\nQ 47.21875 21.78125 47.21875 15.09375 \r\nQ 47.21875 7.46875 41.1875 3.015625 \r\nQ 35.15625 -1.421875 24.609375 -1.421875 \r\nQ 20.21875 -1.421875 15.453125 -0.5625 \r\nQ 10.6875 0.296875 5.421875 2 \r\nL 5.421875 11.28125 \r\nQ 10.40625 8.6875 15.234375 7.390625 \r\nQ 20.0625 6.109375 24.8125 6.109375 \r\nQ 31.15625 6.109375 34.5625 8.28125 \r\nQ 37.984375 10.453125 37.984375 14.40625 \r\nQ 37.984375 18.0625 35.515625 20.015625 \r\nQ 33.0625 21.96875 24.703125 23.78125 \r\nL 21.578125 24.515625 \r\nQ 13.234375 26.265625 9.515625 29.90625 \r\nQ 5.8125 33.546875 5.8125 39.890625 \r\nQ 5.8125 47.609375 11.28125 51.796875 \r\nQ 16.75 56 26.8125 56 \r\nQ 31.78125 56 36.171875 55.265625 \r\nQ 40.578125 54.546875 44.28125 53.078125 \r\nz\r\n\" id=\"DejaVuSans-115\"/>\r\n     </defs>\r\n     <g transform=\"translate(307.221875 23.798437)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"line2d_16\">\r\n     <path d=\"M 279.221875 34.976562 \r\nL 299.221875 34.976562 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_17\"/>\r\n    <g id=\"text_13\">\r\n     <!-- val_loss -->\r\n     <defs>\r\n      <path d=\"M 2.984375 54.6875 \r\nL 12.5 54.6875 \r\nL 29.59375 8.796875 \r\nL 46.6875 54.6875 \r\nL 56.203125 54.6875 \r\nL 35.6875 0 \r\nL 23.484375 0 \r\nz\r\n\" id=\"DejaVuSans-118\"/>\r\n      <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n      <path d=\"M 50.984375 -16.609375 \r\nL 50.984375 -23.578125 \r\nL -0.984375 -23.578125 \r\nL -0.984375 -16.609375 \r\nz\r\n\" id=\"DejaVuSans-95\"/>\r\n     </defs>\r\n     <g transform=\"translate(307.221875 38.476562)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-118\"/>\r\n      <use x=\"59.179688\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"120.458984\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"148.242188\" xlink:href=\"#DejaVuSans-95\"/>\r\n      <use x=\"198.242188\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"226.025391\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"287.207031\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"339.306641\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p9b1ca17334\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"20.5625\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAD4CAYAAADFAawfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3deXxc5X3v8c/vzIw0Wq3VlmTZWoxtGS+Y1mAIxElIGtNAoE1yE6dAGpo2bdMSwk0o5XKb0ia57W1uk/YPml5eCSRtSGNKaENjAtnIdUgIIBPvG+BVkmUtXrRZ0izP/WPGxoAXydbozJz5vl+veY3mnDPn/B5Z/urRM885x5xziIhI9vL8LkBERM5NQS0ikuUU1CIiWU5BLSKS5RTUIiJZLpyJndbU1Ljm5uZM7FpEJJA2bNjQ55yrPdO6jAR1c3Mz7e3tmdi1iEggmdn+s63T0IeISJZTUIuIZDkFtYhIlsvIGLWI5J9YLEZHRwejo6N+l5LVotEojY2NRCKRCb9HQS0iU6Kjo4OysjKam5sxM7/LyUrOOfr7++no6KClpWXC79PQh4hMidHRUaqrqxXS52BmVFdXT/qvDgW1iEwZhfT5Xcj3KHuCOj4GP/9HePUnflciIpJVsieoQwWpoN78735XIiI5qrS01O8SMiJ7gtoM5l4N+3/udyUiIlkle4IaoOkaOLYfjnf4XYmI5DDnHHfffTdLlixh6dKlrF27FoBDhw6xatUqli9fzpIlS/jZz35GIpHgox/96Kltv/zlL/tc/Ztl1/S8preknvc/B8v+m7+1iMgF+6v/2sb2roEp3eelDeX85XsXT2jbxx9/nI0bN7Jp0yb6+vq44oorWLVqFd/61rdYvXo19913H4lEgpGRETZu3EhnZydbt24F4NixY1Na91TImh71ifEEd/40TixcouEPEbkozz77LB/+8IcJhULMmjWLt73tbbz44otcccUVPPzww9x///1s2bKFsrIyWltb2bNnD3fccQdPPfUU5eXlfpf/JlnTo45GPDZ2DrIjfCnL9v/C73JE5CJMtOebKWe7afeqVatYv34969at47bbbuPuu+/mIx/5CJs2beLpp5/mgQce4NFHH+Whhx6a5orPLWt61GbGdW0z+cHwJdC3C4b7/C5JRHLUqlWrWLt2LYlEgt7eXtavX8+VV17J/v37mTlzJn/wB3/Axz72MV566SX6+vpIJpO8//3v53Of+xwvvfSS3+W/yYR71GYWAtqBTufcjZko5p1ts/jyLxZACDjwHCx6byYOIyIB99u//ds899xzXHbZZZgZf/d3f0ddXR3f+MY3+OIXv0gkEqG0tJR/+Zd/obOzk9tvv51kMgnA3/zN3/hc/ZvZ2f5EeNOGZv8dWAGUny+oV6xY4S7kxgHj8SQrP/ckL3i3E1n5+3B99n3DROTMduzYwaJFi/wuIyec6XtlZhuccyvOtP2Ehj7MrBG4AfjqRVd4DgVhj6sX1LOJ+Th9oCgiAkx8jPofgD8DkmfbwMw+bmbtZtbe29t7wQVd1zaLn8cWQvcWGD1+wfsREQmK8wa1md0I9DjnNpxrO+fcg865Fc65FbW1Z7w/44S8fWEtLyTbMJeEgy9c8H5ERIJiIj3qa4CbzGwf8G3gOjP7ZqYKqiktJDl7BXFCmk8tIsIEgto5d69zrtE51wysAX7inLs1k0Vde2kTO5JzGD9wzk68iEheyJp51Ke7rm0m25PNJLu3wgRnpYiIBNWkgto599NMzaE+XVtdGZ2F84iOH4HB7kwfTkQkq2Vlj9rMCM9eBoDr3uJzNSISROe6dvW+fftYsmTJNFZzblkZ1AAz56fmfR/fm32nc4qITKesuSjTG112yVwO/rAW9v+KCr+LEZHJ+f6fp86FmEp1S+E3//asq++55x6ampr4xCc+AcD999+PmbF+/XqOHj1KLBbj85//PDfffPOkDjs6Osof//Ef097eTjgc5ktf+hLveMc72LZtG7fffjvj4+Mkk0m+853v0NDQwAc/+EE6OjpIJBL8xV/8BR/60IcuqtmQxUG9YFYZP/WaWdq/3e9SRCQHrFmzhk996lOngvrRRx/lqaee4q677qK8vJy+vj6uuuoqbrrppkndYPaBBx4AYMuWLezcuZN3v/vd7N69m3/+53/mzjvv5JZbbmF8fJxEIsGTTz5JQ0MD69atA+D48ak5aS9rgzrkGQMzFlFzvB3Gh6GgxO+SRGSiztHzzZTLL7+cnp4eurq66O3tpbKykvr6eu666y7Wr1+P53l0dnZy+PBh6urqJrzfZ599ljvuuAOAtrY2mpqa2L17N1dffTVf+MIX6Ojo4H3vex/z589n6dKlfOYzn+Gee+7hxhtv5K1vfeuUtC1rx6gBCmYvw8MxeGCT36WISA74wAc+wGOPPcbatWtZs2YNjzzyCL29vWzYsIGNGzcya9YsRkdHJ7XPs1247nd+53d44oknKCoqYvXq1fzkJz9hwYIFbNiwgaVLl3Lvvffy13/911PRrOwO6oa2KwHo3PGiz5WISC5Ys2YN3/72t3nsscf4wAc+wPHjx5k5cyaRSIRnnnmG/fv3T3qfq1at4pFHHgFg9+7dHDhwgIULF7Jnzx5aW1v55Cc/yU033cTmzZvp6uqiuLiYW2+9lc985jNTdm3rrB36AGhbuJgBV8zIwY1+lyIiOWDx4sUMDg4ye/Zs6uvrueWWW3jve9/LihUrWL58OW1tbZPe5yc+8Qn+6I/+iKVLlxIOh/n6179OYWEha9eu5Zvf/CaRSIS6ujo++9nP8uKLL3L33XfjeR6RSISvfOUrU9KuCV+PejIu9HrUZ7L1C9cQIcHC+345JfsTkczQ9agnLiPXo/bTSOWlNI7vYWx83O9SRER8kdVDHwBFc5dT0vMoW3duZsmyM/6yERG5IFu2bOG222573bLCwkKef/55nyo6s6wP6sZFV0I7dO1qV1CLZDnn3KTmKPtt6dKlbNw4vZ+BXchwc9YPfVTOXUqcELFOTdETyWbRaJT+/v4LCqJ84Zyjv7+faDQ6qfdlfY+aSJSegrnMGNjtdyUicg6NjY10dHRwMbfiywfRaJTGxsZJvSf7gxo4Ud5CXc9OBkZjlEcjfpcjImcQiURoaWnxu4xAyvqhD4BwzTzmWA87O4/5XYqIyLTLiaCuaGyj0OIc2KfhDxHJPzkR1OWzFwBwrGOXz5WIiEy/nAhqq5oHQKznFZ8rERGZfjkR1JTVE7MCCgf3kUhq6o+I5JfcCGrPY6R0Lo2um339w35XIyIyrXIjqEkNfzRbNzsODfhdiojItMqZoC6un0+T9bCjS1P0RCS/5ExQh2vmUWgxujv2+l2KiMi0ypmgpqoVgNHDL/tciIjI9Mq5oC4bOcixEV2bWkTyR+4Edflskl4k/YHioN/ViIhMm9wJai9EsqKFZjusmR8ikldyJ6iBUE0r80IKahHJLzkV1FY1j7l2mN3dCmoRyR85FdRUtVDoxjhxpNPvSkREpk2OBXVq5kflaAcDozGfixERmR65FdTVqavoNXndHDwy4nMxIiLTI7eCurwxPUXvsIJaRPJGbgV1KIyrmEuTdXNAQS0ieSK3ghoIVbXSGupVUItI3si5oKZiLrOtjwNHTvhdiYjItMjBoJ5DuRukr7/P70pERKbFeYPazKJm9oKZbTKzbWb2V9NR2FnNmJN6PnZQt+USkbwwkR71GHCdc+4yYDlwvZldldmyzqGiCYBZrpfugVHfyhARmS7nDWqXMpR+GUk//OvKVqR61I3Wy4F+faAoIsE3oTFqMwuZ2UagB/ihc+75M2zzcTNrN7P23t7eqa7zNSUzcV4Bs61Pc6lFJC9MKKidcwnn3HKgEbjSzJacYZsHnXMrnHMramtrp7rO13geVMxhjvVpip6I5IVJzfpwzh0Dfgpcn5FqJsgq5tAS6VdQi0hemMisj1ozq0h/XQS8C9iZ6cLOacYcGlCPWkTyQ3gC29QD3zCzEKlgf9Q5973MlnUeFU1UJI/S03/U1zJERKbDeYPaObcZuHwaapm49MyP6IlDDI3FKS2cyO8bEZHclHtnJsKpk14080NE8kFuBnXFXCA9l1pBLSIBl5tBXVaPs5B61CKSF3IzqENhbMZsmsOaoiciwZebQQ0wYy4t4SMKahEJvNwN6oo51KPrfYhI8OVwUM+lIt7P4WODJHW5UxEJsNwN6hlz8EhSleild2jM72pERDImd4P61OVONfNDRIIth4M6NZd6tvXRcVT3TxSR4MrdoC5vxGE0Wq961CISaLkb1OECrKyOeZGjHDyqoBaR4MrdoAaomEtzuF9DHyISaDkf1A3usHrUIhJouR3UVa1UxnvpOzZIPJH0uxoRkYzI7aCubMEjSb3r4dDxUb+rERHJiNwO6qpWAObaYY1Ti0hgBSKom03j1CISXLkd1CU1uIIymu0wHZpLLSIBldtBbYZVtbCwoFdDHyISWLkd1ABVrTRp6ENEAiwQQT0r0U1X/5DflYiIZEQAgrqFEAlCQx2MxRN+VyMiMuUCENQnp+j10HVMc6lFJHgCE9TN1q2r6IlIIOV+UJfWkQwX6QNFEQms3A9qz8OqWmjxdHaiiART7gc1YFWtzAv1aOhDRAIpEEFNVQuzXTcdR4b9rkREZMoFI6grWyggxuiRTr8rERGZcsEI6vTMj4rRgxwfiflcjIjI1ApUUDfZYfb2a/hDRIIlGEE9oxHnRWi2bvb06lRyEQmWYAS1F4LKZpq9w+ztU49aRIIlGEFNaore/HAPexTUIhIwgQlqqlqZ47rZ2zPodyUiIlMqOEFdu5BCN8pY/wGSSed3NSIiUyZQQQ0wJ3GAw4O6ip6IBMd5g9rM5pjZM2a2w8y2mdmd01HYpNW2ATDPOtnTq3FqEQmOifSo48CnnXOLgKuAPzGzSzNb1gUoriJRVM1869QHiiISKOcNaufcIefcS+mvB4EdwOxMF3YhvJltLAh1aS61iATKpMaozawZuBx4/gzrPm5m7WbW3tvbOzXVTZLVLmS+18leBbWIBMiEg9rMSoHvAJ9yzg28cb1z7kHn3Arn3Ira2tqprHHiatsoc8Mc7+3w5/giIhkwoaA2swipkH7EOfd4Zku6COmZH8XHX9WNbkUkMCYy68OArwE7nHNfynxJF6EmFdTzrEM3ERCRwJhIj/oa4DbgOjPbmH68J8N1XZiyOhIF5VxiXZqiJyKBET7fBs65ZwGbhlounhmuZgHzD3aySVP0RCQggnNmYlp4VhvzQ13sVY9aRAIicEFNzUJqOEZPT7fflYiITIngBXX6VHKvf5fPhYiITI0ABnVq5kft6D6ODI/7XIyIyMULXlDPmEMiVMR862TnoTedlyMiknOCF9Seh6uZzyXWyXYFtYgEQPCCGgjPbGNhqIud3brbi4jkvkAGNTMXUUcfBzq7/K5EROSiBTOo65YBUNC3jVgi6XMxIiIXJ5hBXZ8K6oVuL3t1hqKI5LhgBnXpTGLFs7jU28cOfaAoIjkumEENhGZfxhJvPzsO6QNFEcltgQ1qr/4yLrFOXun0524zIiJTJbBBTd0yQiSJH9rqdyUiIhcluEFdfxkADaMv0z805nMxIiIXLrhBXTGXWMEMFts+nfgiIjktuEFtBrOWstjbq5kfIpLTghvUQKRxOW1eB7u6jvpdiojIBQt0UFN/GVHGGerc7nclIiIXLNhBnT6VvPTIdp1KLiI5K9hBXTOfRCjKQvayvUvj1CKSm4Id1F6IRO2lLPH20b5f49QikpuCHdRAQeNyFnv7ad/b73cpIiIXJPBBTcPllDFC7/6tOOf8rkZEZNKCH9RN1wAw/8RmDhwZ8bkYEZHJC35QV7USL57Jld5O2vdpnFpEck/wg9qMUPM1XB3aSfu+I35XIyIyacEPasCar6GOfg7u3el3KSIik5YXQU3TWwCYeWQDx0bGfS5GRGRy8iOoaxcRL5jBld5ONmg+tYjkmPwIas/Dmt7CytBOnfgiIjknP4IaCLVcQ4t18+qrr/hdiojIpORNUJ8cpy7qfoGxeMLnYkREJi5/grruMuLhYn7NbefFvRr+EJHckT9BHQpjc1ZydWgnP9ze7Xc1IiITlj9BTWqceoEd5PltL+u6HyKSM/IqqGm9DoBFQ8+ztVPXpxaR3JBfQd1wOcnSOlaH2vmBhj9EJEecN6jN7CEz6zGzrdNRUEZ5Hl7bDbw9vJmfbj3gdzUiIhMykR7114HrM1zH9Gm7gagbY2bfc+zrG/a7GhGR8zpvUDvn1gPBuexc81tJFpTxbm8DP9x+2O9qRETOa8rGqM3s42bWbmbtvb29U7XbqRcuwFuwmusjL/GjbZ1+VyMicl5TFtTOuQedcyuccytqa2unareZ0XYDM9wA7uAL9A2N+V2NiMg55desj5MueRdJr4Df8NpZt/mQ39WIiJxTfgZ1tByv9W3cWPASj2846Hc1IiLnNJHpef8GPAcsNLMOM/tY5suaBm03UJ/sJt61iVd6hvyuRkTkrCYy6+PDzrl651zEOdfonPvadBSWcZfejAsX8ZHwj/iPX3X4XY2IyFnl59AHQHEVtuyD/Fb4F/xow06SSV37Q0SyU/4GNcDKP6TQjfG24af45d5+v6sRETmj/A7qWYtJzr0mNfyxQaeUi0h2yu+gBryr/pBG6+XE1nWcGNedX0Qk++R9ULPwBsZKGljjvs9//EpnKopI9lFQh8IUrPx9rg1t4+lnfkwskfS7IhGR11FQA7bidmKRMj46/DDf3djldzkiIq+joAYoriL8jnt4R2gT7T9cS0JT9UQkiyio0+zKP2S4pImPjXyNJzdpBoiIZA8F9UnhAopu/F/M9zrZ9/QDOgFGRLKGgvo0XtsN9Nas5NYTj/Bfz2/3uxwREUBB/XpmVL3/7ym3EaJP3cXeXl2sSUT8p6B+g1D9Uobfeh+r7Xl+8PD9jMV1EoyI+EtBfQbl132anoZ38nvDD/Gvj/673+WISJ5TUJ+JGTNve4jBwjres+te1v1ys98ViUgeU1CfTVEFZR/5FjU2RNOTt/G957f6XZGI5CkF9TlEGpeT/NA3WeB10rzuFr77nMJaRKafgvo8ootW4z70CAu8Tlq/fwuPrtcwiIhMLwX1BBQuWg1rHqHN62Tlj97PN77znzohRkSmjYJ6ggraVmO3r2NGIazZ/Ps8+uDnGIvF/S5LRPKAgnoSwk0rmfGp5zhc9eus6f57XvjfN/Kz9k04p961iGSOgnqSrKSGuXc8yavLPs2V8Q0s/6/VPPzl/8HmA7rnoohkhoL6Qngh5r3vs3h/8ksGa5bzewP/RPSrb+WRB7/I4WM67VxEppaC+iJEaufR8Kff58TNX6WypJBbuj5P7MvL+fHD99N5SLf1EpGpYZkYX12xYoVrb2+f8v1mtWSS3g3fZfDHX6R1dBtjLsKvSq9lbPEaCi5ZRUP1DBoqioiE9LtRRN7MzDY451accZ2Ceuodfrmdrh//X+Z1r6OcYQZcEc8kL+e58EoWXPtbfOjaJZQUhv0uU0SyiILaJ/GxEfo3/wB2rmPGwR8RHT9CzIX4lS1iqOmdjM+5lsLZy2iuLaOlpsTvckXERwrqbJBMQEc73e3/QWLHk8yO7QfgiCvlheQiOkqXMmvJ27niqrdTOaOUgpCHmflctIhMFwV1FhrvP8DwrmdI7l1PQcdzlJ1Iffg45iLscHPZ7Fp5OTSfcP1SmtsuZ+XC2bTWlFIQ1hi3SBApqHPBYDedW/4f/buepfrYVmoHd1KQHAEg4Yx9ro4drpmO6HwGKhbBrMVUz5pDa20pv95cSXk04nMDRORiKKhzUTIB/a9CzzYG9m9m6MAmSo5uZ8bYoVOb9LsydiXn8ApzSNReyuyFv07DJcuYOXMWNSWFeJ6GTkRyhYI6SEaOQPdm3OHtjHVtJda1lcKjuylInji1Sa8rZ5+r56DVc9DNYj91JCqaWbhoGauWzWdxQ7nGv0WyjII66JJJ3LH9HHr5JYY6d2L9rxAd2MOM0Q7KY32v2/SYK+GAq6M73MCxaCNj5U1Ea1uobLiE6vpmKkuLqCwuoCwaVo9cZBqdK6g1mTcIPA+raqFhZcub140Pw9F9cGQvw927ObJvJ2VH9zL7xMtUjvwcbyQJ3cAWiDuPbqrY6WropobBaD2xsjl4FXMpqJpD6cwmqisrqSguoKI4QlVJAdFIaLpbK5J3FNRBV1ACsxbDrMWULILW09fFx+B4B+N9e+nvfIXR3r14A53MHe6ibeQVysZ/gdefhH7g1dRbjrtiulwN2101h1w1I4W1UFpHYWUDBZUNFFfPpqKmnpbaMhoriwmpVy5y0RTU+SxcCNXzKKieR/3Cd715fSIGA11w/CCj/QcZ6t1P7MhBqgY7qRvqoujEq0TjA3Cc1GNf6m3jLkQPlWx01QwUzOREQRWjhdXEotV4pbOIVDZQUj2bqpkNzK4qZ2aZPvgUORcFtZxdKAKVTVDZRLQZomfaJjYKQ4dh6DCx410M9XUw2t9B/EgH1YNdNJ7YQ9mJDRSPjJzxEEddKXspZ9CbwVi4jPGCGcQLK0kUVUFxLaGyGorKayirrKW0YialFbWUlxbrmimSVxTUcnEi0VNhHpkDlWfbLnYChntxg4cZ7u9isL+D0aPdjA8chqEeSseOUTXeR9GJvZQNHyfK+FkPedwVc5xSBihh0BUz7JWQKCjHi84gVFyBi1ZgRTMIFVUSLaukZEYlpeXVFJdXUlxWSXFhgXrwklMmFNRmdj3wj0AI+Kpz7m8zWpUET6QIKuZiFXMpnQOl59t+fJjxgV4Gjx5m4EgPI8f7iA324kaOYCeOEBk7RjQxREVikEisl0hsL9GBIYoHTpxvzwy7QoYo5oRFGbFixr1iYuFSEpESEuFi4qEikuEiKCgmVFhKuLCYcLSEUGEJkcJiItFiItESIoUlRKLFFESLiBSWUBgtorAwinnq7cvUOm9Qm1kIeAD4DaADeNHMnnDObc90cZLHCkooqCmhuqaZ6sm8LxHHjR5ndOgIw8f6GBo4wvDAUcaHjpA4MQBjA9jYIKHYEOH4MOHYMCXxYSLxwxSOD1PoRokyRpQxPCY/dTXpjFEijFuEGBFiVkDcIsQtQsIiJLwIyfSz88IkvAKcV4BLv8aL4EKpZwtFUsNPFsKFIpgXBi+MhcJ4oTDOQqceJ5d5oQheKIR5Hp6FsFAIz0uv9zxCJ7fzPJx5gGGeh5mHZ6mvMQ+8EJ7n4Xmh9MMwS19/xvPwzMPzvPR7LXUMs9Sy9HZmltqXGWCp5/QxX7fsTc+ctr3+8oGJ9aivBF5xzu0BMLNvAzcDCmrJPqEwVlJNUUk1RbPmU3Oh+3EO4qOMjgwyNHScseFhxkaHiZ0YJDZ2gvjYCPHxEdz4CZKxUVxsNDVeH3/tYYlxvMRY6tnFsGSMUHIcLxknHB8l7MbxXIIwMSIuRogEYRcnTCL9iFNgian87uSsJIYDHIbjtfA++frkstOfX/s1a6dev/Zee92v4dcvP7kPTv2icG/Y/ozbYgyFKmj9nxsupqlnNJGgng0cPO11B7DyjRuZ2ceBjwPMnTt3SooT8Y0ZRIqIzigiOmPmtB/eOUcs4RhKJEnE4yTjMeLxcZKJBPHYOIl4DFwi9UjEcck4sVicRHycRCKBc0mSiTgukcAlEyQScVwymVqXTJB0Ccw5IJk6YcolcQ5cMgE4zCUhmSDpUuuTySTgwCVxLhVbziVxp5Y7SH/tUjs6FY2pFEuCS70Hl0wtc8nT1qe3TTU+XcPJaEyml6W+NncyP0/f92vve21/J7dxr9t/6q3JU9/r02P/5HFO1n7ysMZr25/c1E47xsmvE5Gy10+BnSITCeoz/e3xpl8uzrkHgQchdWbiRdYlktfMjIKwpa6WWBjmLHNuJE9M5FOPDmDOaa8bga7MlCMiIm80kaB+EZhvZi1mVgCsAZ7IbFkiInLSeYc+nHNxM/tT4GlS0/Mecs5ty3hlIiICTHAetXPuSeDJDNciIiJnoJn5IiJZTkEtIpLlFNQiIllOQS0ikuUycisuM+sF9l/g22uAvvNuFSz52GbIz3bnY5shP9s92TY3Oedqz7QiI0F9Mcys/Wz3DQuqfGwz5Ge787HNkJ/tnso2a+hDRCTLKahFRLJcNgb1g34X4IN8bDPkZ7vzsc2Qn+2esjZn3Ri1iIi8Xjb2qEVE5DQKahGRLJc1QW1m15vZLjN7xcz+3O96MsXM5pjZM2a2w8y2mdmd6eVVZvZDM3s5/XzWG3rnKjMLmdmvzOx76df50OYKM3vMzHam/82vDnq7zeyu9M/2VjP7NzOLBrHNZvaQmfWY2dbTlp21nWZ2bzrfdpnZ6skcKyuC+rQb6P4mcCnwYTO71N+qMiYOfNo5twi4CviTdFv/HPixc24+8OP066C5E9hx2ut8aPM/Ak8559qAy0i1P7DtNrPZwCeBFc65JaQujbyGYLb568D1b1h2xnam/4+vARan3/NP6dybGOec7w/gauDp017fC9zrd13T1PbvkrrD+y6gPr2sHtjld21T3M7G9A/udcD30suC3uZyYC/pD+1PWx7YdvPaPVarSF1G+XvAu4PaZqAZ2Hq+f9s3Zhqp6/tfPdHjZEWPmjPfQHe2T7VMGzNrBi4HngdmOecOAaSfp/+Oqpn1D8CfwevuEhr0NrcCvcDD6SGfr5pZCQFut3OuE/g/wAHgEHDcOfcDAtzmNzhbOy8q47IlqCd0A90gMbNS4DvAp5xzA37Xk0lmdiPQ45zb4Hct0ywM/BrwFefc5cAwwfiT/6zSY7I3Ay1AA1BiZrf6W1VWuKiMy5agzqsb6JpZhFRIP+Kcezy9+LCZ1afX1wM9ftWXAdcAN5nZPuDbwHVm9k2C3WZI/Vx3OOeeT79+jFRwB7nd7wL2Oud6nXMx4HHgLQS7zac7WzsvKuOyJajz5ga6ZmbA14AdzrkvnbbqCeB301//Lqmx60Bwzt3rnGt0zjWT+rf9iXPuVgLcZgDnXDdw0MwWphe9E9hOsNt9ALjKzIrTP+vvJPUBapDbfLqztfMJYI2ZFZpZCzAfeGHCe/V7MP60wfX3ALuBV4H7/K4ng+28ltSfPICo6QsAAACCSURBVJuBjenHe4BqUh+2vZx+rvK71gy1/+289mFi4NsMLAfa0//e/wlUBr3dwF8BO4GtwL8ChUFsM/BvpMbhY6R6zB87VzuB+9L5tgv4zckcS6eQi4hkuWwZ+hARkbNQUIuIZDkFtYhIllNQi4hkOQW1iEiWU1CLiGQ5BbWISJb7/+Gkav0e1sEWAAAAAElFTkSuQmCC\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "plt.plot(train_history, label=\"loss\")\n",
    "plt.plot(val_history, label=\"val_loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(48000, 1, 28, 28)\n(12000, 1, 28, 28)\n(10000, 1, 28, 28)\n"
    }
   ],
   "source": [
    "# 【問題6】MNISTをKerasで学習\n",
    "# 読み込み\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.astype(np.float)[:, np.newaxis, :, :]\n",
    "X_test = X_test.astype(np.float)[:, np.newaxis, :, :]\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "NNfor6(\n  (layer): Sequential(\n    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1))\n    (3): ReLU()\n    (4): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n    (5): Flatten()\n    (6): Linear(in_features=2304, out_features=10, bias=True)\n  )\n)\n"
    }
   ],
   "source": [
    "# PyTorchでモデル構築\n",
    "n_channels_1 = 1\n",
    "n_channels_2 = 32\n",
    "activation_1 = nn.ReLU()\n",
    "n_channels_3 = 16\n",
    "activation_2 = nn.ReLU()\n",
    "n_features = int(((X_train.shape[2]-4)/2)*((X_train.shape[3]-4)/2)*n_channels_3)\n",
    "n_output = 10\n",
    "\n",
    "## ネットワーク\n",
    "class NNfor6(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(NNfor6, self).__init__()\n",
    "        self.layer = nn.Sequential(nn.Conv2d(n_channels_1, n_channels_2, kernel_size=(3, 3)), activation_1,\n",
    "                                   nn.Conv2d(n_channels_2, n_channels_3, kernel_size=(3, 3)), activation_2,\n",
    "                                   nn.MaxPool2d(kernel_size=(2, 2)),\n",
    "                                   nn.Flatten(),\n",
    "                                   nn.Linear(n_features, n_output))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.layer(X)\n",
    "\n",
    "        return X\n",
    "\n",
    "## インスタンス作成\n",
    "model = NNfor6()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ミニバッチ\n",
    "batch_size = 128\n",
    "\n",
    "## numpy配列をテンソルに変換\n",
    "t_X_train = torch.from_numpy(X_train).float()\n",
    "t_y_train = torch.from_numpy(y_train).long()\n",
    "t_X_valid = torch.from_numpy(X_val).float()\n",
    "t_y_valid = torch.from_numpy(y_val).long()\n",
    "\n",
    "## 特徴量とラベルをデータセットにまとめる\n",
    "dataset_train = TensorDataset(t_X_train, t_y_train)\n",
    "dataset_valid = TensorDataset(t_X_valid, t_y_valid)\n",
    "\n",
    "# データローダを作成\n",
    "loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "loader_valid = DataLoader(dataset_valid, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "## オプティマイザ\n",
    "lr = 0.001\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=lr\n",
    ")\n",
    "\n",
    "## 損失関数\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 学習(train)\n",
    "def train_step(X_train, y_train):\n",
    "    ## 訓練モード    \n",
    "    model.train()\n",
    "\n",
    "    ## 訓練データに対して予測\n",
    "    y_train_pred = model(X_train)\n",
    "\n",
    "    ## 勾配初期化\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    ## 損失の取得\n",
    "    loss = criterion(y_train_pred, y_train)\n",
    "    ## 勾配を計算\n",
    "    loss.backward()\n",
    "\n",
    "    ## パラメータ更新\n",
    "    optimizer.step()\n",
    "\n",
    "    ## 正解数の確認\n",
    "    with torch.no_grad():\n",
    "        y_train_pred_num = torch.argmax(y_train_pred, axis=1)\n",
    "        acc = (y_train_pred_num == y_train).sum()\n",
    "\n",
    "    return (loss.item(), acc.item())\n",
    "\n",
    "## 評価(validation)\n",
    "def valid_step(X_val, y_val):\n",
    "    ## 評価モード\n",
    "    model.eval()\n",
    "\n",
    "    ## 予測\n",
    "    y_val_pred = model(X_val)\n",
    "\n",
    "    ## 損失計算\n",
    "    loss = criterion(y_val_pred, y_val)\n",
    "\n",
    "    ## 正解数計算\n",
    "    with torch.no_grad():\n",
    "        y_val_pred_num = torch.argmax(y_val_pred, axis=1)\n",
    "        acc = (y_val_pred_num == y_val).sum()\n",
    "\n",
    "    return (loss.item(), acc.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch1/5: [loss:0.0028, acc:0.8965, val_loss:0.0011, val_acc:0.9619]\nepoch2/5: [loss:0.0007, acc:0.9729, val_loss:0.0006, val_acc:0.9768]\nepoch3/5: [loss:0.0005, acc:0.9811, val_loss:0.0006, val_acc:0.9772]\nepoch4/5: [loss:0.0004, acc:0.9841, val_loss:0.0005, val_acc:0.9796]\nepoch5/5: [loss:0.0003, acc:0.9864, val_loss:0.0005, val_acc:0.9822]\nDone!\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "OrderedDict([('layer.0.weight',\n              tensor([[[[-6.6515e-02,  3.8745e-01, -3.6519e-01],\n                        [ 3.0598e-01, -1.6159e-01, -1.1785e-01],\n                        [ 3.4303e-01, -2.1639e-02, -4.2074e-01]]],\n              \n              \n                      [[[-1.7692e-01, -4.7700e-01,  1.8381e-01],\n                        [-3.9763e-01, -2.1087e-01,  3.2134e-01],\n                        [-4.9433e-01, -2.3804e-01,  3.2278e-01]]],\n              \n              \n                      [[[-1.9446e-01, -6.2865e-02,  3.9672e-02],\n                        [-8.6459e-02,  8.3800e-02,  1.0328e-01],\n                        [ 9.9521e-03, -1.0248e-01, -2.0454e-01]]],\n              \n              \n                      [[[ 1.8014e-01,  2.0634e-01,  5.0279e-01],\n                        [-5.4063e-02,  3.6979e-01, -1.1240e-01],\n                        [-2.1484e-01, -4.7952e-01, -4.1213e-01]]],\n              \n              \n                      [[[ 3.4974e-02, -2.2078e-01,  4.2671e-04],\n                        [ 4.7836e-02,  3.0812e-01,  4.5509e-01],\n                        [-7.0214e-02,  3.1230e-03,  3.9253e-01]]],\n              \n              \n                      [[[ 6.4301e-02,  3.4789e-01, -2.4635e-01],\n                        [-7.6108e-02,  3.6388e-01, -9.0382e-02],\n                        [-3.3683e-02, -1.3423e-01,  3.9917e-02]]],\n              \n              \n                      [[[-1.7566e-01,  2.1410e-01, -5.3822e-02],\n                        [-2.6763e-01, -4.1472e-02, -1.4149e-01],\n                        [-2.5842e-01, -2.2276e-01, -2.1377e-01]]],\n              \n              \n                      [[[ 3.0513e-01,  1.6141e-01,  3.2135e-01],\n                        [ 7.4574e-02,  3.4114e-01,  2.8601e-01],\n                        [-8.0268e-02, -7.5144e-02, -2.7421e-01]]],\n              \n              \n                      [[[-6.1171e-02, -1.4000e-01, -2.7185e-01],\n                        [ 3.2271e-02, -2.7588e-01,  2.5436e-01],\n                        [ 2.1230e-01,  4.7722e-01,  2.9668e-03]]],\n              \n              \n                      [[[ 1.8694e-02,  2.5282e-01, -8.6958e-02],\n                        [ 1.7130e-01,  6.4956e-02,  2.6562e-02],\n                        [-6.9015e-02,  3.9525e-01,  4.2432e-01]]],\n              \n              \n                      [[[ 2.0921e-01, -4.0859e-02, -2.6824e-03],\n                        [ 2.4543e-01,  1.1523e-01, -2.1916e-01],\n                        [ 4.8260e-01, -2.3791e-01, -3.0215e-01]]],\n              \n              \n                      [[[-3.9226e-01, -1.6366e-02, -2.4947e-01],\n                        [-2.1387e-01, -3.3981e-01,  4.2220e-01],\n                        [-4.4073e-02,  1.8804e-01,  4.2042e-01]]],\n              \n              \n                      [[[-1.9631e-02,  3.5423e-01,  1.1988e-01],\n                        [ 6.5070e-02,  3.1629e-01,  3.5930e-01],\n                        [ 2.9982e-02,  9.3281e-02,  2.8787e-01]]],\n              \n              \n                      [[[ 8.2560e-02, -2.0924e-01,  3.3401e-02],\n                        [-2.3145e-02, -9.7142e-02,  2.6725e-01],\n                        [ 1.2694e-01,  2.9279e-01,  3.6837e-01]]],\n              \n              \n                      [[[-2.2852e-01,  2.6761e-01,  8.7823e-02],\n                        [ 7.0542e-05,  2.2288e-01,  3.0963e-01],\n                        [-2.6037e-02,  2.1212e-01,  3.8698e-01]]],\n              \n              \n                      [[[-3.1822e-01,  1.8388e-02, -4.4794e-01],\n                        [-3.8603e-02, -1.7543e-01, -3.0777e-01],\n                        [ 5.0238e-01, -7.9589e-02,  3.0741e-01]]],\n              \n              \n                      [[[-2.8024e-01, -2.9269e-03, -3.9760e-01],\n                        [ 3.6218e-01,  4.1095e-01, -1.0546e-01],\n                        [ 3.7192e-01, -1.7599e-02,  9.9257e-02]]],\n              \n              \n                      [[[ 3.5611e-01, -3.2797e-02, -2.5109e-01],\n                        [ 3.4188e-01, -1.9928e-01, -1.6988e-01],\n                        [ 4.2754e-01,  2.0480e-01, -1.9345e-01]]],\n              \n              \n                      [[[ 1.5547e-01, -1.6473e-01, -5.0684e-01],\n                        [ 4.5671e-01, -1.7172e-01, -3.3865e-01],\n                        [ 5.4355e-01, -7.2923e-02,  1.9254e-01]]],\n              \n              \n                      [[[-8.5987e-02, -2.3710e-01, -2.2506e-02],\n                        [ 5.0265e-02, -2.6944e-01, -3.3239e-01],\n                        [-2.9373e-01, -1.3348e-01,  2.1513e-01]]],\n              \n              \n                      [[[-1.0091e-01,  1.0843e-01,  5.4855e-01],\n                        [-5.1980e-01, -2.5502e-01, -7.4180e-02],\n                        [ 1.4080e-01, -1.3198e-01,  1.1986e-01]]],\n              \n              \n                      [[[-3.8550e-01,  1.7839e-01,  4.7418e-01],\n                        [-7.6307e-02, -8.5535e-02,  2.2594e-01],\n                        [ 1.3237e-01,  2.4333e-01,  2.4847e-02]]],\n              \n              \n                      [[[ 6.1352e-01, -4.1537e-02, -1.3062e-01],\n                        [-2.8659e-01,  1.8187e-01, -1.2266e-01],\n                        [-4.2026e-01, -4.3916e-02,  1.6281e-01]]],\n              \n              \n                      [[[ 5.0633e-01,  7.0678e-02, -1.6148e-01],\n                        [-3.3577e-01, -2.7998e-01,  3.6356e-01],\n                        [-2.9783e-01, -1.5459e-01,  3.3441e-01]]],\n              \n              \n                      [[[ 3.5978e-01, -3.0894e-01,  1.8662e-01],\n                        [ 3.3638e-01,  1.5645e-01, -2.8699e-01],\n                        [ 3.4560e-01,  3.2284e-01, -3.1624e-01]]],\n              \n              \n                      [[[-1.1987e-01, -1.6805e-01,  4.1205e-01],\n                        [ 2.1736e-01,  1.1603e-01, -1.4663e-01],\n                        [ 2.1201e-01, -1.4380e-01, -3.3710e-01]]],\n              \n              \n                      [[[-5.1330e-01, -6.4064e-01, -3.0748e-01],\n                        [ 3.7314e-02,  2.5826e-01, -8.1623e-02],\n                        [ 1.0886e-01,  2.9943e-01,  7.6922e-03]]],\n              \n              \n                      [[[-4.1054e-01, -4.9905e-01, -7.9325e-02],\n                        [-6.2180e-02, -1.0704e-01,  3.9541e-01],\n                        [ 1.0498e-01,  5.3149e-01,  2.5450e-01]]],\n              \n              \n                      [[[ 3.0145e-01,  3.7871e-01,  1.4271e-01],\n                        [ 1.7188e-01,  2.1670e-01, -1.0368e-01],\n                        [-1.0346e-01,  2.2421e-01,  2.8162e-01]]],\n              \n              \n                      [[[-4.2704e-01, -5.0034e-01, -4.5250e-01],\n                        [-7.7663e-02,  2.9692e-02,  3.1341e-01],\n                        [-5.9867e-02,  1.4242e-01,  1.3776e-01]]],\n              \n              \n                      [[[ 1.3220e-01,  1.3543e-02,  3.9949e-01],\n                        [ 2.7089e-01,  1.4393e-01, -1.5457e-01],\n                        [-4.8739e-02, -2.2364e-01, -4.3652e-01]]],\n              \n              \n                      [[[ 3.7314e-01, -2.1576e-01, -1.9504e-01],\n                        [ 1.5699e-01, -3.9461e-02, -5.2038e-01],\n                        [ 4.2533e-01, -1.7763e-01, -3.5105e-01]]]])),\n             ('layer.0.bias',\n              tensor([ 0.0500,  0.3090, -0.3004,  0.0401, -0.0018,  0.2657,  0.2131, -0.0005,\n                      -0.0286, -0.0858, -0.0713,  0.0053, -0.0096,  0.2168, -0.0674,  0.2960,\n                       0.1126, -0.1596,  0.0864, -0.2340,  0.0593, -0.1841,  0.0192,  0.0716,\n                      -0.1713,  0.1475,  0.1978,  0.0024, -0.0715,  0.2015,  0.1390,  0.1655])),\n             ('layer.2.weight',\n              tensor([[[[ 2.3714e-01, -1.1059e-01, -2.0515e-01],\n                        [-1.1285e-01, -1.2101e-01, -9.3014e-02],\n                        [-3.3421e-01, -1.7157e-01,  5.3173e-02]],\n              \n                       [[-6.2388e-02,  6.0806e-02,  1.0524e-01],\n                        [ 6.9223e-02,  3.5792e-02,  8.0360e-02],\n                        [ 1.3148e-01,  1.2453e-01,  1.0423e-01]],\n              \n                       [[ 3.5643e-02, -5.2832e-02,  4.4199e-02],\n                        [ 4.5270e-02,  1.5943e-02, -3.0788e-02],\n                        [ 1.6860e-02, -5.2039e-02,  4.1207e-02]],\n              \n                       ...,\n              \n                       [[ 2.9446e-02,  1.0566e-01,  1.7927e-01],\n                        [ 5.6435e-02,  5.6413e-02,  1.0342e-01],\n                        [ 6.2027e-02,  1.1398e-01, -3.4657e-03]],\n              \n                       [[ 7.8852e-02,  6.6529e-03, -1.3400e-01],\n                        [ 4.3958e-02, -1.3956e-01, -9.0035e-02],\n                        [ 2.5964e-02, -1.2516e-01,  4.2241e-03]],\n              \n                       [[ 4.0304e-02,  1.5746e-02, -2.9650e-02],\n                        [ 2.9556e-02, -1.8000e-03, -1.4593e-02],\n                        [-6.2390e-02, -6.7519e-02, -1.4357e-02]]],\n              \n              \n                      [[[-5.9709e-02, -1.0060e-01, -1.3055e-01],\n                        [ 9.9157e-02,  1.0580e-01,  1.9346e-01],\n                        [ 1.2385e-01,  1.6070e-01, -9.6318e-03]],\n              \n                       [[-9.2066e-03,  2.0092e-04, -8.7237e-02],\n                        [-7.6115e-02, -1.0796e-01, -1.5978e-01],\n                        [-8.9966e-02, -1.4895e-01, -6.4670e-02]],\n              \n                       [[-5.2812e-02,  2.6859e-02,  4.4951e-02],\n                        [ 1.4442e-02,  1.2925e-02, -2.2716e-04],\n                        [ 2.0661e-02,  8.7329e-03, -3.1626e-02]],\n              \n                       ...,\n              \n                       [[ 5.4202e-02,  1.5626e-01,  1.9948e-01],\n                        [ 8.9905e-02,  9.7815e-03,  5.2738e-02],\n                        [-4.8621e-02, -9.4499e-02, -3.1017e-02]],\n              \n                       [[-8.5095e-02, -6.7297e-02, -1.9341e-02],\n                        [ 4.0550e-03,  2.2566e-01,  2.5382e-01],\n                        [ 2.1223e-01,  2.5872e-01,  1.9178e-01]],\n              \n                       [[-8.0142e-02, -1.7661e-01, -1.8928e-01],\n                        [-1.5431e-01, -9.8352e-02, -2.3831e-02],\n                        [-3.2456e-03,  5.2834e-03,  9.3945e-02]]],\n              \n              \n                      [[[-3.8551e-02, -4.0540e-02, -1.2853e-02],\n                        [ 2.7101e-02,  2.1555e-02,  2.7575e-02],\n                        [ 4.7060e-02,  3.6763e-02,  3.3877e-02]],\n              \n                       [[-1.1145e-02,  3.2940e-02,  3.5099e-02],\n                        [ 4.7565e-02,  3.0456e-02, -2.4744e-02],\n                        [ 5.0934e-02, -5.0132e-02, -5.9175e-02]],\n              \n                       [[ 3.2919e-02, -3.9451e-02,  2.7782e-02],\n                        [-1.7742e-02, -1.5568e-03, -4.8597e-03],\n                        [-2.9499e-02, -2.8736e-02, -2.7123e-02]],\n              \n                       ...,\n              \n                       [[ 7.9550e-03, -3.1372e-02,  2.4971e-02],\n                        [ 3.8078e-02,  4.5603e-02, -3.5484e-02],\n                        [-1.8255e-02, -4.0508e-02, -5.8039e-02]],\n              \n                       [[-6.3184e-02, -1.8868e-02,  9.3692e-03],\n                        [-4.5116e-02, -1.6788e-03,  1.8662e-02],\n                        [ 1.8150e-02,  7.4842e-03, -4.6673e-02]],\n              \n                       [[-2.8783e-02,  4.4790e-02, -6.1036e-02],\n                        [ 4.3634e-02,  2.7251e-02, -9.5534e-03],\n                        [ 2.2484e-02, -2.5486e-02,  4.1272e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 2.0938e-03, -9.4559e-03,  1.0749e-01],\n                        [ 3.9399e-02, -4.9044e-02,  4.9292e-02],\n                        [ 3.6633e-02,  6.1878e-03,  4.1765e-02]],\n              \n                       [[ 8.6282e-03, -4.0635e-02, -3.7019e-02],\n                        [-9.0008e-02, -3.3659e-02, -3.6247e-02],\n                        [-4.8649e-02, -4.5178e-04, -2.9078e-02]],\n              \n                       [[-5.0262e-02, -6.6506e-04, -3.9723e-02],\n                        [-2.8145e-02, -4.4795e-02,  5.3981e-02],\n                        [-9.7711e-03, -1.2683e-02,  2.0529e-03]],\n              \n                       ...,\n              \n                       [[-3.3458e-02,  4.1526e-02, -1.7782e-03],\n                        [ 1.4445e-02,  1.1694e-01,  1.9593e-02],\n                        [-3.4194e-02, -2.9597e-02,  1.0377e-02]],\n              \n                       [[ 3.8128e-02,  2.1406e-02,  1.4844e-02],\n                        [-6.5313e-02,  4.2187e-02,  8.0188e-02],\n                        [ 4.6804e-03,  3.6388e-02,  2.9657e-02]],\n              \n                       [[-3.3577e-02, -6.3190e-02,  2.8304e-03],\n                        [-3.5750e-02, -3.5776e-02, -6.3324e-02],\n                        [ 1.3279e-02, -8.5281e-03, -2.3759e-02]]],\n              \n              \n                      [[[ 4.3194e-02, -1.5191e-03, -2.5941e-02],\n                        [-4.9480e-02,  3.5976e-02,  3.9093e-02],\n                        [ 1.6828e-02,  2.0407e-02, -1.0974e-02]],\n              \n                       [[ 3.2940e-02, -1.8271e-02,  2.9012e-02],\n                        [-3.4556e-02, -5.5339e-02, -5.3448e-03],\n                        [ 3.7959e-02,  2.6367e-02,  3.4419e-02]],\n              \n                       [[-6.1211e-03,  6.4873e-03, -1.0302e-03],\n                        [ 4.0333e-03,  5.2671e-02, -2.0428e-02],\n                        [ 3.7675e-02,  2.8012e-02,  5.6895e-03]],\n              \n                       ...,\n              \n                       [[-1.4237e-02,  1.6435e-02,  4.2189e-02],\n                        [-2.3356e-02,  4.4715e-02, -5.0905e-02],\n                        [-2.6672e-02, -1.5853e-02,  3.8759e-02]],\n              \n                       [[-3.8424e-02, -6.2103e-02, -3.1056e-02],\n                        [ 2.6747e-02, -3.3035e-02,  1.1462e-02],\n                        [ 1.7370e-02, -1.7473e-02, -1.8101e-03]],\n              \n                       [[-2.2040e-02, -6.5748e-03, -4.5180e-02],\n                        [-2.8398e-02,  4.2019e-03,  5.0833e-02],\n                        [ 4.3343e-02, -1.5562e-02, -4.7738e-02]]],\n              \n              \n                      [[[-7.6542e-02,  1.0389e-01,  3.6178e-01],\n                        [ 3.3476e-02,  2.5803e-01,  3.1017e-01],\n                        [ 2.2692e-01,  4.0846e-01,  2.5154e-01]],\n              \n                       [[ 1.0247e-01,  1.1115e-01,  6.3576e-02],\n                        [ 2.0356e-01,  2.5081e-01,  1.6844e-01],\n                        [ 1.9892e-01,  1.8576e-01,  1.0909e-01]],\n              \n                       [[-5.8568e-02, -7.4715e-03,  3.8269e-02],\n                        [ 1.2504e-02, -3.8520e-02,  5.4148e-02],\n                        [-5.1442e-02,  1.3210e-02,  1.5122e-02]],\n              \n                       ...,\n              \n                       [[-3.4180e-02, -8.2929e-02, -1.4908e-01],\n                        [ 5.4398e-02, -1.2334e-01, -1.4637e-01],\n                        [ 8.1539e-02, -1.1276e-01, -2.4457e-01]],\n              \n                       [[-1.6677e-01, -5.6980e-02,  1.0157e-01],\n                        [-1.8493e-01, -1.4955e-01, -5.5377e-02],\n                        [-1.1580e-01, -8.4117e-02, -1.2010e-01]],\n              \n                       [[-7.5749e-05,  4.7175e-02,  2.0475e-01],\n                        [-1.0766e-01,  1.0423e-01,  1.7214e-01],\n                        [ 3.7585e-02,  2.2463e-01,  1.7937e-01]]]])),\n             ('layer.2.bias',\n              tensor([ 0.0086, -0.0281, -0.0064,  0.0468, -0.0013,  0.0057, -0.0435,  0.0138,\n                       0.0202, -0.0040,  0.0879,  0.0154,  0.0287,  0.0109, -0.0553, -0.0630])),\n             ('layer.6.weight',\n              tensor([[ 0.0027,  0.0047, -0.0633,  ..., -0.2230, -0.0786, -0.0448],\n                      [ 0.0781,  0.0113,  0.0363,  ..., -0.0421, -0.0674, -0.0538],\n                      [-0.0106,  0.0417, -0.0034,  ..., -0.0581, -0.0842, -0.0197],\n                      ...,\n                      [-0.0433,  0.0059, -0.0211,  ...,  0.0178,  0.0405, -0.0309],\n                      [-0.0377,  0.0467,  0.0478,  ..., -0.1675, -0.0955, -0.0273],\n                      [ 0.0202, -0.0026,  0.0084,  ...,  0.0879, -0.1160, -0.1061]])),\n             ('layer.6.bias',\n              tensor([-0.0094,  0.0239,  0.0101, -0.0009, -0.0145,  0.0207, -0.0111,  0.0149,\n                      -0.0283, -0.0095]))])"
     },
     "metadata": {}
    }
   ],
   "source": [
    "## パラメータ初期化関数\n",
    "def initializer(layer):\n",
    "    if type(layer) == nn.Linear:\n",
    "        nn.init.kaiming_normal_(layer.weight)\n",
    "        layer.bias.data.fill_(0.0)\n",
    "\n",
    "## パラメータ初期化する\n",
    "model.apply(initializer)\n",
    "\n",
    "n_epoch = 5\n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "# 1エポック内1データあたりの損失・正解率の平均\n",
    "avg_loss = 0.0\n",
    "avg_acc = 0.0\n",
    "avg_val_loss = 0.0\n",
    "avg_val_acc = 0.0\n",
    "\n",
    "## 学習する\n",
    "for epoch in range(n_epoch):\n",
    "    ## epoch内のloss, accの保存\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    total_val_loss = 0.0\n",
    "    total_val_acc = 0.0\n",
    "    ## 累計データ数の保存\n",
    "    total_train = 0\n",
    "    total_val = 0\n",
    "\n",
    "    ## 訓練\n",
    "    for X_train, y_train in loader_train:\n",
    "       loss, acc = train_step(X_train, y_train)\n",
    "       total_loss += loss\n",
    "       total_acc += acc\n",
    "       total_train += len(y_train)\n",
    "\n",
    "    ## 評価\n",
    "    for X_val, y_val in loader_valid:\n",
    "        val_loss, val_acc = valid_step(X_val, y_val)\n",
    "        total_val_loss += val_loss\n",
    "        total_val_acc += val_acc\n",
    "        total_val += len(y_val)\n",
    "\n",
    "    # 平均計算\n",
    "    avg_loss = total_loss/total_train\n",
    "    avg_val_loss = total_val_loss/total_val\n",
    "    avg_acc = total_acc/total_train\n",
    "    avg_val_acc = total_val_acc/total_val\n",
    "\n",
    "    print(f'epoch{epoch+1}/{n_epoch}: [loss:{avg_loss:.4f}, acc:{avg_acc:.4f}, val_loss:{avg_val_loss:.4f}, val_acc:{avg_val_acc:.4f}]')\n",
    "\n",
    "    # リストに追加\n",
    "    train_history.append(avg_loss)\n",
    "    val_history.append(avg_val_loss)\n",
    "\n",
    "print('Done!')\n",
    "display(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "y_pred_proba tensor([[-6.8542e+00, -5.4845e+00, -1.1071e+00,  ...,  1.5085e+01,\n         -3.2599e+00,  7.0407e-01],\n        [-1.0208e+00, -9.7754e-01,  1.0842e+01,  ..., -1.7415e+01,\n         -1.4150e+00, -1.2467e+01],\n        [-4.0342e+00,  8.6646e+00, -9.9296e-01,  ...,  9.2288e-03,\n         -1.7936e-01, -8.1182e+00],\n        ...,\n        [-1.1781e+01, -5.0670e+00, -1.1936e+01,  ..., -1.9019e+00,\n          5.6381e-03,  6.6801e-01],\n        [-4.3676e+00, -1.1092e+01, -8.3667e+00,  ..., -1.0085e+01,\n          4.5348e+00, -6.4990e+00],\n        [-3.9635e+00, -1.6759e+01, -3.2875e+00,  ..., -1.3846e+01,\n         -2.2791e+00, -1.3644e+01]], grad_fn=<AddmmBackward>)\ny_pred [7 2 1 ... 4 5 6]\ny_test [7 2 1 ... 4 5 6]\nacc:  0.9833\n"
    }
   ],
   "source": [
    "# 予測\n",
    "## Tensor変換\n",
    "t_X_test = torch.from_numpy(X_test).float()\n",
    "y_pred_proba = model(t_X_test)\n",
    "y_pred = torch.argmax(y_pred_proba, axis=1)\n",
    "print(\"y_pred_proba\", y_pred_proba)\n",
    "print(\"y_pred\", y_pred.flatten().int().detach().numpy())\n",
    "print(\"y_test\", y_test.flatten())\n",
    "print(\"acc: \", accuracy_score(y_test.flatten(),  y_pred.flatten().int().detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 391.190625 248.518125\" width=\"391.190625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.518125 \r\nL 391.190625 248.518125 \r\nL 391.190625 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 49.190625 224.64 \r\nL 383.990625 224.64 \r\nL 383.990625 7.2 \r\nL 49.190625 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"ma7fa40723e\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"64.408807\" xlink:href=\"#ma7fa40723e\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0.0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n       <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n      </defs>\r\n      <g transform=\"translate(56.457244 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"102.454261\" xlink:href=\"#ma7fa40723e\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 0.5 -->\r\n      <defs>\r\n       <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n      </defs>\r\n      <g transform=\"translate(94.502699 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"140.499716\" xlink:href=\"#ma7fa40723e\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 1.0 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g transform=\"translate(132.548153 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"178.54517\" xlink:href=\"#ma7fa40723e\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 1.5 -->\r\n      <g transform=\"translate(170.593608 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"216.590625\" xlink:href=\"#ma7fa40723e\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 2.0 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      </defs>\r\n      <g transform=\"translate(208.639063 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"254.63608\" xlink:href=\"#ma7fa40723e\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 2.5 -->\r\n      <g transform=\"translate(246.684517 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_7\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"292.681534\" xlink:href=\"#ma7fa40723e\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 3.0 -->\r\n      <defs>\r\n       <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n      </defs>\r\n      <g transform=\"translate(284.729972 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_8\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"330.726989\" xlink:href=\"#ma7fa40723e\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 3.5 -->\r\n      <g transform=\"translate(322.775426 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_9\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"368.772443\" xlink:href=\"#ma7fa40723e\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 4.0 -->\r\n      <defs>\r\n       <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n      </defs>\r\n      <g transform=\"translate(360.820881 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_10\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m2724c2d602\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"49.190625\" xlink:href=\"#m2724c2d602\" y=\"201.681791\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 0.0005 -->\r\n      <g transform=\"translate(7.2 205.481009)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"286.279297\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"49.190625\" xlink:href=\"#m2724c2d602\" y=\"161.126994\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 0.0010 -->\r\n      <g transform=\"translate(7.2 164.926213)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"286.279297\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"49.190625\" xlink:href=\"#m2724c2d602\" y=\"120.572198\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 0.0015 -->\r\n      <g transform=\"translate(7.2 124.371417)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"286.279297\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"49.190625\" xlink:href=\"#m2724c2d602\" y=\"80.017401\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 0.0020 -->\r\n      <g transform=\"translate(7.2 83.81662)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"286.279297\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"49.190625\" xlink:href=\"#m2724c2d602\" y=\"39.462605\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- 0.0025 -->\r\n      <g transform=\"translate(7.2 43.261824)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"286.279297\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_15\">\r\n    <path clip-path=\"url(#p86892ec347)\" d=\"M 64.408807 17.083636 \r\nL 140.499716 182.844616 \r\nL 216.590625 201.774809 \r\nL 292.681534 209.697085 \r\nL 368.772443 214.756364 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_16\">\r\n    <path clip-path=\"url(#p86892ec347)\" d=\"M 64.408807 155.21009 \r\nL 140.499716 193.031234 \r\nL 216.590625 195.8854 \r\nL 292.681534 199.223078 \r\nL 368.772443 204.226601 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 49.190625 224.64 \r\nL 49.190625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 383.990625 224.64 \r\nL 383.990625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 49.190625 224.64 \r\nL 383.990625 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 49.190625 7.2 \r\nL 383.990625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"legend_1\">\r\n    <g id=\"patch_7\">\r\n     <path d=\"M 305.85 44.834375 \r\nL 376.990625 44.834375 \r\nQ 378.990625 44.834375 378.990625 42.834375 \r\nL 378.990625 14.2 \r\nQ 378.990625 12.2 376.990625 12.2 \r\nL 305.85 12.2 \r\nQ 303.85 12.2 303.85 14.2 \r\nL 303.85 42.834375 \r\nQ 303.85 44.834375 305.85 44.834375 \r\nz\r\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\r\n    </g>\r\n    <g id=\"line2d_17\">\r\n     <path d=\"M 307.85 20.298437 \r\nL 327.85 20.298437 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_18\"/>\r\n    <g id=\"text_15\">\r\n     <!-- loss -->\r\n     <defs>\r\n      <path d=\"M 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\n\" id=\"DejaVuSans-108\"/>\r\n      <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n      <path d=\"M 44.28125 53.078125 \r\nL 44.28125 44.578125 \r\nQ 40.484375 46.53125 36.375 47.5 \r\nQ 32.28125 48.484375 27.875 48.484375 \r\nQ 21.1875 48.484375 17.84375 46.4375 \r\nQ 14.5 44.390625 14.5 40.28125 \r\nQ 14.5 37.15625 16.890625 35.375 \r\nQ 19.28125 33.59375 26.515625 31.984375 \r\nL 29.59375 31.296875 \r\nQ 39.15625 29.25 43.1875 25.515625 \r\nQ 47.21875 21.78125 47.21875 15.09375 \r\nQ 47.21875 7.46875 41.1875 3.015625 \r\nQ 35.15625 -1.421875 24.609375 -1.421875 \r\nQ 20.21875 -1.421875 15.453125 -0.5625 \r\nQ 10.6875 0.296875 5.421875 2 \r\nL 5.421875 11.28125 \r\nQ 10.40625 8.6875 15.234375 7.390625 \r\nQ 20.0625 6.109375 24.8125 6.109375 \r\nQ 31.15625 6.109375 34.5625 8.28125 \r\nQ 37.984375 10.453125 37.984375 14.40625 \r\nQ 37.984375 18.0625 35.515625 20.015625 \r\nQ 33.0625 21.96875 24.703125 23.78125 \r\nL 21.578125 24.515625 \r\nQ 13.234375 26.265625 9.515625 29.90625 \r\nQ 5.8125 33.546875 5.8125 39.890625 \r\nQ 5.8125 47.609375 11.28125 51.796875 \r\nQ 16.75 56 26.8125 56 \r\nQ 31.78125 56 36.171875 55.265625 \r\nQ 40.578125 54.546875 44.28125 53.078125 \r\nz\r\n\" id=\"DejaVuSans-115\"/>\r\n     </defs>\r\n     <g transform=\"translate(335.85 23.798437)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"line2d_19\">\r\n     <path d=\"M 307.85 34.976562 \r\nL 327.85 34.976562 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_20\"/>\r\n    <g id=\"text_16\">\r\n     <!-- val_loss -->\r\n     <defs>\r\n      <path d=\"M 2.984375 54.6875 \r\nL 12.5 54.6875 \r\nL 29.59375 8.796875 \r\nL 46.6875 54.6875 \r\nL 56.203125 54.6875 \r\nL 35.6875 0 \r\nL 23.484375 0 \r\nz\r\n\" id=\"DejaVuSans-118\"/>\r\n      <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n      <path d=\"M 50.984375 -16.609375 \r\nL 50.984375 -23.578125 \r\nL -0.984375 -23.578125 \r\nL -0.984375 -16.609375 \r\nz\r\n\" id=\"DejaVuSans-95\"/>\r\n     </defs>\r\n     <g transform=\"translate(335.85 38.476562)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-118\"/>\r\n      <use x=\"59.179688\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"120.458984\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"148.242188\" xlink:href=\"#DejaVuSans-95\"/>\r\n      <use x=\"198.242188\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"226.025391\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"287.207031\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"339.306641\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p86892ec347\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"49.190625\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "plt.plot(train_history, label=\"loss\")\n",
    "plt.plot(val_history, label=\"val_loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題8】\n",
    "#### （アドバンス課題）フレームワークの比較\n",
    "それぞれのフレームワークにはどのような違いがあるかをまとめてください。\n",
    "\n",
    "\n",
    "**《視点例》**\n",
    "\n",
    "\n",
    "- 計算速度\n",
    "- コードの行数・可読性\n",
    "- 用意されている機能"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}