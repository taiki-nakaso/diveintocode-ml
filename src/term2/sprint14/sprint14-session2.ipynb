{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint\n",
    "## ディープラーニングフレームワーク2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.このSprintについて\n",
    "\n",
    "### Sprintの目的\n",
    "- フレームワークのコードを読めるようにする\n",
    "- フレームワークを習得し続けられるようになる\n",
    "- 理論を知っている範囲をフレームワークで動かす\n",
    "\n",
    "### どのように学ぶか\n",
    "前半はTensorFlowのExampleを動かします。後半ではKerasのコードを書いていきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.公式Example\n",
    "\n",
    "深層学習フレームワークには公式に様々なモデルのExampleコードが公開されています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題1】\n",
    "#### 公式チュートリアルモデルを分担して実行\n",
    "TensorFLowの公式チュートリアルモデルを分担して実行してください。\n",
    "\n",
    "\n",
    "以下の中から1人ひとつ選び実行し、その結果を簡単に発表してください。\n",
    "\n",
    "\n",
    "[models/tutorials at master · tensorflow/models](https://www.tensorflow.org/tutorials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題2】\n",
    "#### （アドバンス課題）様々な手法を実行\n",
    "TensorFLowやGoogle AI ResearchのGitHubリポジトリには、定番のモデルから最新のモデルまで多様なコードが公開されています。これらから興味あるものを選び実行してください。\n",
    "\n",
    "\n",
    "なお、これらのコードは初学者向けではないため、巨大なデータセットのダウンロードが必要な場合など、実行が簡単ではないこともあります。そういった場合は、コードリーディングを行ってください。\n",
    "\n",
    "\n",
    "[models/research at master · tensorflow/models]()\n",
    "\n",
    "\n",
    "[google-research/google-research: Google AI Research]()\n",
    "\n",
    "\n",
    "更新日が古いものはPythonやTensorFlowのバージョンが古く、扱いずらい場合があります。新しいものから見ることを推奨します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.異なるフレームワークへの書き換え\n",
    "\n",
    "「ディープラーニングフレームワーク1」で作成した4種類のデータセットを扱うTensorFLowのコードを異なるフレームワークに変更していきます。\n",
    "\n",
    "\n",
    "- Iris（Iris-versicolorとIris-virginicaのみの2値分類）\n",
    "- Iris（3種類全ての目的変数を使用して多値分類）\n",
    "- House Prices\n",
    "- MNIST\n",
    "\n",
    "### Kerasへの書き換え\n",
    "KerasはTensorFLowに含まれるtf.kerasモジュールを使用してください。\n",
    "\n",
    "\n",
    "KerasにはSequentialモデルかFunctional APIかなど書き方に種類がありますが、これは指定しません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題3】\n",
    "#### Iris（2値分類）をKerasで学習\n",
    "TensorFlowによるIrisデータセットに対する2値分類をKerasに書き換えてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "      Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm  \\\n0      1            5.1           3.5            1.4           0.2   \n1      2            4.9           3.0            1.4           0.2   \n2      3            4.7           3.2            1.3           0.2   \n3      4            4.6           3.1            1.5           0.2   \n4      5            5.0           3.6            1.4           0.2   \n..   ...            ...           ...            ...           ...   \n145  146            6.7           3.0            5.2           2.3   \n146  147            6.3           2.5            5.0           1.9   \n147  148            6.5           3.0            5.2           2.0   \n148  149            6.2           3.4            5.4           2.3   \n149  150            5.9           3.0            5.1           1.8   \n\n            Species  \n0       Iris-setosa  \n1       Iris-setosa  \n2       Iris-setosa  \n3       Iris-setosa  \n4       Iris-setosa  \n..              ...  \n145  Iris-virginica  \n146  Iris-virginica  \n147  Iris-virginica  \n148  Iris-virginica  \n149  Iris-virginica  \n\n[150 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>SepalLengthCm</th>\n      <th>SepalWidthCm</th>\n      <th>PetalLengthCm</th>\n      <th>PetalWidthCm</th>\n      <th>Species</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>5.1</td>\n      <td>3.5</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>4.9</td>\n      <td>3.0</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>4.7</td>\n      <td>3.2</td>\n      <td>1.3</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>4.6</td>\n      <td>3.1</td>\n      <td>1.5</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>5.0</td>\n      <td>3.6</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>146</td>\n      <td>6.7</td>\n      <td>3.0</td>\n      <td>5.2</td>\n      <td>2.3</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>146</th>\n      <td>147</td>\n      <td>6.3</td>\n      <td>2.5</td>\n      <td>5.0</td>\n      <td>1.9</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>148</td>\n      <td>6.5</td>\n      <td>3.0</td>\n      <td>5.2</td>\n      <td>2.0</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>148</th>\n      <td>149</td>\n      <td>6.2</td>\n      <td>3.4</td>\n      <td>5.4</td>\n      <td>2.3</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>149</th>\n      <td>150</td>\n      <td>5.9</td>\n      <td>3.0</td>\n      <td>5.1</td>\n      <td>1.8</td>\n      <td>Iris-virginica</td>\n    </tr>\n  </tbody>\n</table>\n<p>150 rows × 6 columns</p>\n</div>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "(100, 4)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "(100, 1)"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(64, 4) (64, 1)\n(16, 4) (16, 1)\n(20, 4) (20, 1)\n"
    }
   ],
   "source": [
    "# dataset読み込み\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "df_iris = pd.read_csv('Iris.csv')\n",
    "display(df_iris)\n",
    "X = np.array(df_iris[(df_iris['Species'] == 'Iris-versicolor')|(df_iris['Species'] == 'Iris-virginica')].iloc[:, 1:5])\n",
    "y = np.array(df_iris[(df_iris['Species'] == 'Iris-versicolor')|(df_iris['Species'] == 'Iris-virginica')].iloc[:, 5])\n",
    "y[y=='Iris-versicolor'] = 0\n",
    "y[y=='Iris-virginica'] = 1\n",
    "y = y.astype(np.int)[:, np.newaxis]\n",
    "display(X.shape)\n",
    "display(y.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From C:\\Users\\190450781\\Anaconda3\\envs\\dic\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\nInstructions for updating:\nIf using Keras pass *_constraint arguments to layers.\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense (Dense)                (None, 10)                50        \n_________________________________________________________________\ndense_1 (Dense)              (None, 5)                 55        \n_________________________________________________________________\ndense_2 (Dense)              (None, 1)                 6         \n=================================================================\nTotal params: 111\nTrainable params: 111\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "K.clear_session()\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(10, activation = tf.nn.relu, input_shape=(4,)))\n",
    "model.add(tf.keras.layers.Dense(5, activation = tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(1, activation = tf.nn.sigmoid))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From C:\\Users\\190450781\\Anaconda3\\envs\\dic\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.where in 2.0, which has the same broadcast rule as np.where\nTrain on 64 samples, validate on 16 samples\nEpoch 1/10\n64/64 [==============================] - 0s 3ms/sample - loss: 0.2196 - acc: 0.8906 - val_loss: 0.2782 - val_acc: 0.9375\nEpoch 2/10\n64/64 [==============================] - 0s 2ms/sample - loss: 0.1102 - acc: 0.9531 - val_loss: 0.0520 - val_acc: 0.9375\nEpoch 3/10\n64/64 [==============================] - 0s 1ms/sample - loss: 0.0216 - acc: 1.0000 - val_loss: 0.0562 - val_acc: 0.9375\nEpoch 4/10\n64/64 [==============================] - 0s 1ms/sample - loss: 0.0182 - acc: 1.0000 - val_loss: 0.0331 - val_acc: 1.0000\nEpoch 5/10\n64/64 [==============================] - 0s 1ms/sample - loss: 0.0107 - acc: 1.0000 - val_loss: 0.0070 - val_acc: 1.0000\nEpoch 6/10\n64/64 [==============================] - 0s 1ms/sample - loss: 0.0045 - acc: 1.0000 - val_loss: 0.0228 - val_acc: 1.0000\nEpoch 7/10\n64/64 [==============================] - 0s 1ms/sample - loss: 0.0027 - acc: 1.0000 - val_loss: 0.0079 - val_acc: 1.0000\nEpoch 8/10\n64/64 [==============================] - 0s 1ms/sample - loss: 0.0022 - acc: 1.0000 - val_loss: 0.0133 - val_acc: 1.0000\nEpoch 9/10\n64/64 [==============================] - 0s 1ms/sample - loss: 0.0015 - acc: 1.0000 - val_loss: 0.0153 - val_acc: 1.0000\nEpoch 10/10\n64/64 [==============================] - 0s 1ms/sample - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0070 - val_acc: 1.0000\n"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=tf.train.AdamOptimizer(learning_rate=0.03),\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=1,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "y_pred_proba [6.4849854e-05 1.0000000e+00 1.2874603e-05 1.0000000e+00 9.9999893e-01\n 1.0000000e+00 2.6524067e-06 9.9940616e-01 1.0000000e+00 9.9999988e-01\n 9.9999976e-01 9.9999952e-01 1.0000000e+00 2.1427870e-05 0.0000000e+00\n 0.0000000e+00 8.4431535e-01 0.0000000e+00 9.9973124e-01 2.0861626e-07]\ny_pred [0 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 1 0 1 0]\ny_test [0 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 0 0 0 0]\n"
    }
   ],
   "source": [
    "y_pred_proba = model.predict(X_test)[:, 0]\n",
    "y_pred = np.where(y_pred_proba >0.5, 1, 0)\n",
    "print(\"y_pred_proba\", y_pred_proba)\n",
    "print(\"y_pred\", y_pred)\n",
    "print(\"y_test\", y_test.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題4】\n",
    "#### Iris（多値分類）をKerasで学習\n",
    "TensorFlowによるIrisデータセットに対する3値分類をKerasに書き換えてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "(150, 4)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "(150, 3)"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(96, 4) (96, 3)\n(24, 4) (24, 3)\n(30, 4) (30, 3)\n"
    }
   ],
   "source": [
    "# 抽出\n",
    "X = np.array(df_iris.iloc[:, 1:5])\n",
    "y = np.array(pd.get_dummies(df_iris.iloc[:, 5]))\n",
    "y = y.astype(np.int)\n",
    "display(X.shape)\n",
    "display(y.shape)\n",
    "\n",
    "# 分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "# 標準化\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense (Dense)                (None, 100)               500       \n_________________________________________________________________\ndense_1 (Dense)              (None, 10)                1010      \n_________________________________________________________________\ndense_2 (Dense)              (None, 3)                 33        \n=================================================================\nTotal params: 1,543\nTrainable params: 1,543\nNon-trainable params: 0\n_________________________________________________________________\nTrain on 96 samples, validate on 24 samples\nEpoch 1/10\n96/96 [==============================] - 0s 2ms/sample - loss: 0.5803 - acc: 0.7708 - val_loss: 0.3735 - val_acc: 0.7500\nEpoch 2/10\n96/96 [==============================] - 0s 1ms/sample - loss: 0.2276 - acc: 0.9167 - val_loss: 0.6959 - val_acc: 0.7917\nEpoch 3/10\n96/96 [==============================] - 0s 1ms/sample - loss: 0.1346 - acc: 0.9688 - val_loss: 0.7985 - val_acc: 0.7917\nEpoch 4/10\n96/96 [==============================] - 0s 1ms/sample - loss: 0.1533 - acc: 0.9479 - val_loss: 1.0933 - val_acc: 0.8750\nEpoch 5/10\n96/96 [==============================] - 0s 1ms/sample - loss: 0.3249 - acc: 0.8750 - val_loss: 0.1578 - val_acc: 0.9583\nEpoch 6/10\n96/96 [==============================] - 0s 2ms/sample - loss: 0.1250 - acc: 0.9375 - val_loss: 0.4380 - val_acc: 0.8333\nEpoch 7/10\n96/96 [==============================] - 0s 2ms/sample - loss: 0.1501 - acc: 0.9479 - val_loss: 0.1774 - val_acc: 0.9167\nEpoch 8/10\n96/96 [==============================] - 0s 935us/sample - loss: 0.1161 - acc: 0.9583 - val_loss: 0.8584 - val_acc: 0.8333\nEpoch 9/10\n96/96 [==============================] - 0s 1ms/sample - loss: 0.2767 - acc: 0.9583 - val_loss: 0.2026 - val_acc: 0.9167\nEpoch 10/10\n96/96 [==============================] - 0s 1ms/sample - loss: 0.0961 - acc: 0.9688 - val_loss: 0.2581 - val_acc: 0.9167\n"
    }
   ],
   "source": [
    "# モデル構築\n",
    "K.clear_session()\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(100, activation = tf.nn.relu, input_shape=(4,)))\n",
    "model.add(tf.keras.layers.Dense(10, activation = tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(3, activation = tf.nn.softmax))\n",
    "model.summary()\n",
    "\n",
    "# コンパイル\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=tf.train.AdamOptimizer(learning_rate=0.03),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 学習\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=1,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "y_pred_proba [[1.04564852e-27 2.32908914e-09 1.00000000e+00]\n [1.21489335e-02 9.62576985e-01 2.52740923e-02]\n [1.00000000e+00 7.43522355e-09 1.46612967e-14]\n [1.41976076e-16 3.60058766e-05 9.99963999e-01]\n [9.99997854e-01 2.09982340e-06 2.25637321e-11]\n [1.10057165e-36 2.42372691e-13 1.00000000e+00]\n [9.99999046e-01 9.90514081e-07 9.72723378e-12]\n [6.84147747e-03 9.60981131e-01 3.21774036e-02]\n [1.21489335e-02 9.62576985e-01 2.52740923e-02]\n [1.21489335e-02 9.62576985e-01 2.52740923e-02]\n [7.43509781e-06 6.58998191e-01 3.40994388e-01]\n [1.21489335e-02 9.62576985e-01 2.52740923e-02]\n [1.21489335e-02 9.62576985e-01 2.52740923e-02]\n [7.77879078e-03 9.61717963e-01 3.05032376e-02]\n [9.35985427e-03 9.62412775e-01 2.82273069e-02]\n [9.99990702e-01 9.34896616e-06 1.59446206e-10]\n [5.44016343e-03 9.59187508e-01 3.53723243e-02]\n [1.21489335e-02 9.62576985e-01 2.52740923e-02]\n [9.99958277e-01 4.17144438e-05 1.29728839e-09]\n [9.99999762e-01 1.79668831e-07 1.11983513e-12]\n [1.25000980e-16 6.12623408e-05 9.99938726e-01]\n [3.00739566e-03 9.51982379e-01 4.50102873e-02]\n [9.99995589e-01 4.43576664e-06 4.41672636e-11]\n [9.99980211e-01 1.98232501e-05 4.21339436e-10]\n [1.16062147e-08 7.18718618e-02 9.28128183e-01]\n [1.00000000e+00 4.01227425e-08 1.44809349e-13]\n [9.99998808e-01 1.22860195e-06 9.13557077e-12]\n [1.21489335e-02 9.62576985e-01 2.52740923e-02]\n [1.17623797e-02 9.72295880e-01 1.59418639e-02]\n [9.99989748e-01 1.02206241e-05 1.84729385e-10]]\ny_pred [2 1 0 2 0 2 0 1 1 1 1 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0]\ny_test [2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0]\n"
    }
   ],
   "source": [
    "y_pred_proba = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "print(\"y_pred_proba\", y_pred_proba)\n",
    "print(\"y_pred\", y_pred)\n",
    "print(\"y_test\", np.argmax(y_test, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題5】\n",
    "#### House PricesをKerasで学習\n",
    "TensorFlowによるHouse Pricesデータセットに対する回帰をKerasに書き換えてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "        Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n0        1          60       RL         65.0     8450   Pave   NaN      Reg   \n1        2          20       RL         80.0     9600   Pave   NaN      Reg   \n2        3          60       RL         68.0    11250   Pave   NaN      IR1   \n3        4          70       RL         60.0     9550   Pave   NaN      IR1   \n4        5          60       RL         84.0    14260   Pave   NaN      IR1   \n...    ...         ...      ...          ...      ...    ...   ...      ...   \n1455  1456          60       RL         62.0     7917   Pave   NaN      Reg   \n1456  1457          20       RL         85.0    13175   Pave   NaN      Reg   \n1457  1458          70       RL         66.0     9042   Pave   NaN      Reg   \n1458  1459          20       RL         68.0     9717   Pave   NaN      Reg   \n1459  1460          20       RL         75.0     9937   Pave   NaN      Reg   \n\n     LandContour Utilities  ... PoolArea PoolQC  Fence MiscFeature MiscVal  \\\n0            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n1            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n2            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n3            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n4            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n...          ...       ...  ...      ...    ...    ...         ...     ...   \n1455         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n1456         Lvl    AllPub  ...        0    NaN  MnPrv         NaN       0   \n1457         Lvl    AllPub  ...        0    NaN  GdPrv        Shed    2500   \n1458         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n1459         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n\n     MoSold YrSold  SaleType  SaleCondition  SalePrice  \n0         2   2008        WD         Normal     208500  \n1         5   2007        WD         Normal     181500  \n2         9   2008        WD         Normal     223500  \n3         2   2006        WD        Abnorml     140000  \n4        12   2008        WD         Normal     250000  \n...     ...    ...       ...            ...        ...  \n1455      8   2007        WD         Normal     175000  \n1456      2   2010        WD         Normal     210000  \n1457      5   2010        WD         Normal     266500  \n1458      4   2010        WD         Normal     142125  \n1459      6   2008        WD         Normal     147500  \n\n[1460 rows x 81 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>MSSubClass</th>\n      <th>MSZoning</th>\n      <th>LotFrontage</th>\n      <th>LotArea</th>\n      <th>Street</th>\n      <th>Alley</th>\n      <th>LotShape</th>\n      <th>LandContour</th>\n      <th>Utilities</th>\n      <th>...</th>\n      <th>PoolArea</th>\n      <th>PoolQC</th>\n      <th>Fence</th>\n      <th>MiscFeature</th>\n      <th>MiscVal</th>\n      <th>MoSold</th>\n      <th>YrSold</th>\n      <th>SaleType</th>\n      <th>SaleCondition</th>\n      <th>SalePrice</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>65.0</td>\n      <td>8450</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>208500</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>20</td>\n      <td>RL</td>\n      <td>80.0</td>\n      <td>9600</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>5</td>\n      <td>2007</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>181500</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>68.0</td>\n      <td>11250</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>9</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>223500</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>70</td>\n      <td>RL</td>\n      <td>60.0</td>\n      <td>9550</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2006</td>\n      <td>WD</td>\n      <td>Abnorml</td>\n      <td>140000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>84.0</td>\n      <td>14260</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>12</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>250000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1455</th>\n      <td>1456</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>62.0</td>\n      <td>7917</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>8</td>\n      <td>2007</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>175000</td>\n    </tr>\n    <tr>\n      <th>1456</th>\n      <td>1457</td>\n      <td>20</td>\n      <td>RL</td>\n      <td>85.0</td>\n      <td>13175</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>MnPrv</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2010</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>210000</td>\n    </tr>\n    <tr>\n      <th>1457</th>\n      <td>1458</td>\n      <td>70</td>\n      <td>RL</td>\n      <td>66.0</td>\n      <td>9042</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>GdPrv</td>\n      <td>Shed</td>\n      <td>2500</td>\n      <td>5</td>\n      <td>2010</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>266500</td>\n    </tr>\n    <tr>\n      <th>1458</th>\n      <td>1459</td>\n      <td>20</td>\n      <td>RL</td>\n      <td>68.0</td>\n      <td>9717</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>4</td>\n      <td>2010</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>142125</td>\n    </tr>\n    <tr>\n      <th>1459</th>\n      <td>1460</td>\n      <td>20</td>\n      <td>RL</td>\n      <td>75.0</td>\n      <td>9937</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>6</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>147500</td>\n    </tr>\n  </tbody>\n</table>\n<p>1460 rows × 81 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# 読み込み\n",
    "df_price = pd.read_csv('train.csv')\n",
    "display(df_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "(1460, 3)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "(1460, 1)"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(934, 3) (934, 1)\n(234, 3) (234, 1)\n(292, 3) (292, 1)\n"
    }
   ],
   "source": [
    "# 抽出\n",
    "X = df_price[['LotArea', 'GrLivArea', 'YearBuilt']].values\n",
    "y = df_price['SalePrice'].values[:, np.newaxis]\n",
    "display(X.shape)\n",
    "display(y.shape)\n",
    "\n",
    "# 分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "# 標準化\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# 対数変換\n",
    "y_train = np.log(y_train)\n",
    "y_val = np.log(y_val)\n",
    "y_test = np.log(y_test)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense (Dense)                (None, 10)                40        \n_________________________________________________________________\ndense_1 (Dense)              (None, 3)                 33        \n_________________________________________________________________\ndense_2 (Dense)              (None, 1)                 4         \n=================================================================\nTotal params: 77\nTrainable params: 77\nNon-trainable params: 0\n_________________________________________________________________\nTrain on 934 samples, validate on 234 samples\nEpoch 1/100\n934/934 [==============================] - 0s 120us/sample - loss: 148.6039 - mean_absolute_error: 12.1852 - val_loss: 146.2253 - val_mean_absolute_error: 12.0872\nEpoch 2/100\n934/934 [==============================] - 0s 51us/sample - loss: 145.8117 - mean_absolute_error: 12.0695 - val_loss: 143.9844 - val_mean_absolute_error: 11.9934\nEpoch 3/100\n934/934 [==============================] - 0s 54us/sample - loss: 143.4271 - mean_absolute_error: 11.9693 - val_loss: 141.9268 - val_mean_absolute_error: 11.9062\nEpoch 4/100\n934/934 [==============================] - 0s 56us/sample - loss: 141.1294 - mean_absolute_error: 11.8712 - val_loss: 139.7414 - val_mean_absolute_error: 11.8121\nEpoch 5/100\n934/934 [==============================] - 0s 60us/sample - loss: 138.6377 - mean_absolute_error: 11.7630 - val_loss: 137.2841 - val_mean_absolute_error: 11.7046\nEpoch 6/100\n934/934 [==============================] - 0s 65us/sample - loss: 135.8151 - mean_absolute_error: 11.6384 - val_loss: 134.4287 - val_mean_absolute_error: 11.5773\nEpoch 7/100\n934/934 [==============================] - 0s 57us/sample - loss: 132.4670 - mean_absolute_error: 11.4873 - val_loss: 130.9134 - val_mean_absolute_error: 11.4176\nEpoch 8/100\n934/934 [==============================] - 0s 55us/sample - loss: 128.2235 - mean_absolute_error: 11.2924 - val_loss: 126.2198 - val_mean_absolute_error: 11.2002\nEpoch 9/100\n934/934 [==============================] - 0s 54us/sample - loss: 122.5352 - mean_absolute_error: 11.0252 - val_loss: 119.9589 - val_mean_absolute_error: 10.9052\nEpoch 10/100\n934/934 [==============================] - 0s 54us/sample - loss: 115.1508 - mean_absolute_error: 10.6711 - val_loss: 111.9959 - val_mean_absolute_error: 10.5204\nEpoch 11/100\n934/934 [==============================] - 0s 55us/sample - loss: 106.3779 - mean_absolute_error: 10.2324 - val_loss: 103.0056 - val_mean_absolute_error: 10.0645\nEpoch 12/100\n934/934 [==============================] - 0s 53us/sample - loss: 96.6927 - mean_absolute_error: 9.7184 - val_loss: 93.0310 - val_mean_absolute_error: 9.5307\nEpoch 13/100\n934/934 [==============================] - 0s 53us/sample - loss: 86.1891 - mean_absolute_error: 9.1270 - val_loss: 82.2285 - val_mean_absolute_error: 8.9148\nEpoch 14/100\n934/934 [==============================] - 0s 48us/sample - loss: 75.0637 - mean_absolute_error: 8.4463 - val_loss: 70.9422 - val_mean_absolute_error: 8.2168\nEpoch 15/100\n934/934 [==============================] - 0s 54us/sample - loss: 63.7982 - mean_absolute_error: 7.7013 - val_loss: 59.6153 - val_mean_absolute_error: 7.4517\nEpoch 16/100\n934/934 [==============================] - 0s 53us/sample - loss: 52.8980 - mean_absolute_error: 6.9155 - val_loss: 48.7144 - val_mean_absolute_error: 6.6496\nEpoch 17/100\n934/934 [==============================] - 0s 53us/sample - loss: 42.7723 - mean_absolute_error: 6.1185 - val_loss: 38.7561 - val_mean_absolute_error: 5.8443\nEpoch 18/100\n934/934 [==============================] - 0s 92us/sample - loss: 33.8484 - mean_absolute_error: 5.3779 - val_loss: 30.0671 - val_mean_absolute_error: 5.0906\nEpoch 19/100\n934/934 [==============================] - 0s 52us/sample - loss: 26.3530 - mean_absolute_error: 4.7156 - val_loss: 22.8584 - val_mean_absolute_error: 4.3994\nEpoch 20/100\n934/934 [==============================] - 0s 59us/sample - loss: 20.3118 - mean_absolute_error: 4.1101 - val_loss: 17.1742 - val_mean_absolute_error: 3.7835\nEpoch 21/100\n934/934 [==============================] - 0s 53us/sample - loss: 15.5695 - mean_absolute_error: 3.5809 - val_loss: 12.8063 - val_mean_absolute_error: 3.2298\nEpoch 22/100\n934/934 [==============================] - 0s 56us/sample - loss: 12.0307 - mean_absolute_error: 3.0928 - val_loss: 9.6010 - val_mean_absolute_error: 2.7522\nEpoch 23/100\n934/934 [==============================] - 0s 58us/sample - loss: 9.4790 - mean_absolute_error: 2.6853 - val_loss: 7.4308 - val_mean_absolute_error: 2.3800\nEpoch 24/100\n934/934 [==============================] - 0s 56us/sample - loss: 7.7106 - mean_absolute_error: 2.3569 - val_loss: 5.9657 - val_mean_absolute_error: 2.0889\nEpoch 25/100\n934/934 [==============================] - 0s 47us/sample - loss: 6.4740 - mean_absolute_error: 2.0956 - val_loss: 4.9607 - val_mean_absolute_error: 1.8712\nEpoch 26/100\n934/934 [==============================] - 0s 53us/sample - loss: 5.6121 - mean_absolute_error: 1.9020 - val_loss: 4.3070 - val_mean_absolute_error: 1.7186\nEpoch 27/100\n934/934 [==============================] - 0s 46us/sample - loss: 5.0001 - mean_absolute_error: 1.7644 - val_loss: 3.8294 - val_mean_absolute_error: 1.5979\nEpoch 28/100\n934/934 [==============================] - 0s 49us/sample - loss: 4.5292 - mean_absolute_error: 1.6605 - val_loss: 3.4986 - val_mean_absolute_error: 1.5113\nEpoch 29/100\n934/934 [==============================] - 0s 42us/sample - loss: 4.1292 - mean_absolute_error: 1.5710 - val_loss: 3.2031 - val_mean_absolute_error: 1.4323\nEpoch 30/100\n934/934 [==============================] - 0s 49us/sample - loss: 3.7973 - mean_absolute_error: 1.4974 - val_loss: 2.9757 - val_mean_absolute_error: 1.3724\nEpoch 31/100\n934/934 [==============================] - 0s 54us/sample - loss: 3.5002 - mean_absolute_error: 1.4347 - val_loss: 2.7945 - val_mean_absolute_error: 1.3247\nEpoch 32/100\n934/934 [==============================] - 0s 48us/sample - loss: 3.2502 - mean_absolute_error: 1.3809 - val_loss: 2.6198 - val_mean_absolute_error: 1.2782\nEpoch 33/100\n934/934 [==============================] - 0s 43us/sample - loss: 3.0256 - mean_absolute_error: 1.3326 - val_loss: 2.4580 - val_mean_absolute_error: 1.2373\nEpoch 34/100\n934/934 [==============================] - 0s 47us/sample - loss: 2.8145 - mean_absolute_error: 1.2868 - val_loss: 2.2951 - val_mean_absolute_error: 1.1948\nEpoch 35/100\n934/934 [==============================] - 0s 37us/sample - loss: 2.6274 - mean_absolute_error: 1.2416 - val_loss: 2.1534 - val_mean_absolute_error: 1.1558\nEpoch 36/100\n934/934 [==============================] - 0s 41us/sample - loss: 2.4473 - mean_absolute_error: 1.2000 - val_loss: 2.0099 - val_mean_absolute_error: 1.1175\nEpoch 37/100\n934/934 [==============================] - 0s 48us/sample - loss: 2.2757 - mean_absolute_error: 1.1551 - val_loss: 1.8785 - val_mean_absolute_error: 1.0769\nEpoch 38/100\n934/934 [==============================] - 0s 48us/sample - loss: 2.1156 - mean_absolute_error: 1.1151 - val_loss: 1.7478 - val_mean_absolute_error: 1.0400\nEpoch 39/100\n934/934 [==============================] - 0s 42us/sample - loss: 1.9654 - mean_absolute_error: 1.0769 - val_loss: 1.6286 - val_mean_absolute_error: 1.0028\nEpoch 40/100\n934/934 [==============================] - 0s 44us/sample - loss: 1.8191 - mean_absolute_error: 1.0321 - val_loss: 1.5250 - val_mean_absolute_error: 0.9636\nEpoch 41/100\n934/934 [==============================] - 0s 43us/sample - loss: 1.6888 - mean_absolute_error: 0.9943 - val_loss: 1.4275 - val_mean_absolute_error: 0.9335\nEpoch 42/100\n934/934 [==============================] - 0s 44us/sample - loss: 1.5660 - mean_absolute_error: 0.9570 - val_loss: 1.3285 - val_mean_absolute_error: 0.8986\nEpoch 43/100\n934/934 [==============================] - 0s 40us/sample - loss: 1.4509 - mean_absolute_error: 0.9194 - val_loss: 1.2385 - val_mean_absolute_error: 0.8644\nEpoch 44/100\n934/934 [==============================] - 0s 42us/sample - loss: 1.3432 - mean_absolute_error: 0.8818 - val_loss: 1.1526 - val_mean_absolute_error: 0.8314\nEpoch 45/100\n934/934 [==============================] - 0s 40us/sample - loss: 1.2437 - mean_absolute_error: 0.8497 - val_loss: 1.0708 - val_mean_absolute_error: 0.8012\nEpoch 46/100\n934/934 [==============================] - 0s 38us/sample - loss: 1.1477 - mean_absolute_error: 0.8118 - val_loss: 0.9903 - val_mean_absolute_error: 0.7652\nEpoch 47/100\n934/934 [==============================] - 0s 43us/sample - loss: 1.0613 - mean_absolute_error: 0.7808 - val_loss: 0.9176 - val_mean_absolute_error: 0.7391\nEpoch 48/100\n934/934 [==============================] - 0s 74us/sample - loss: 0.9795 - mean_absolute_error: 0.7446 - val_loss: 0.8571 - val_mean_absolute_error: 0.7071\nEpoch 49/100\n934/934 [==============================] - 0s 37us/sample - loss: 0.9055 - mean_absolute_error: 0.7145 - val_loss: 0.7964 - val_mean_absolute_error: 0.6828\nEpoch 50/100\n934/934 [==============================] - 0s 42us/sample - loss: 0.8337 - mean_absolute_error: 0.6877 - val_loss: 0.7382 - val_mean_absolute_error: 0.6546\nEpoch 51/100\n934/934 [==============================] - 0s 39us/sample - loss: 0.7722 - mean_absolute_error: 0.6572 - val_loss: 0.6825 - val_mean_absolute_error: 0.6270\nEpoch 52/100\n934/934 [==============================] - 0s 41us/sample - loss: 0.7129 - mean_absolute_error: 0.6273 - val_loss: 0.6373 - val_mean_absolute_error: 0.6001\nEpoch 53/100\n934/934 [==============================] - 0s 51us/sample - loss: 0.6626 - mean_absolute_error: 0.5986 - val_loss: 0.5896 - val_mean_absolute_error: 0.5733\nEpoch 54/100\n934/934 [==============================] - 0s 45us/sample - loss: 0.6137 - mean_absolute_error: 0.5724 - val_loss: 0.5434 - val_mean_absolute_error: 0.5479\nEpoch 55/100\n934/934 [==============================] - 0s 43us/sample - loss: 0.5699 - mean_absolute_error: 0.5481 - val_loss: 0.5072 - val_mean_absolute_error: 0.5263\nEpoch 56/100\n934/934 [==============================] - 0s 42us/sample - loss: 0.5277 - mean_absolute_error: 0.5291 - val_loss: 0.4683 - val_mean_absolute_error: 0.5041\nEpoch 57/100\n934/934 [==============================] - 0s 40us/sample - loss: 0.4916 - mean_absolute_error: 0.5031 - val_loss: 0.4310 - val_mean_absolute_error: 0.4802\nEpoch 58/100\n934/934 [==============================] - 0s 44us/sample - loss: 0.4541 - mean_absolute_error: 0.4849 - val_loss: 0.4168 - val_mean_absolute_error: 0.4678\nEpoch 59/100\n934/934 [==============================] - 0s 42us/sample - loss: 0.4254 - mean_absolute_error: 0.4673 - val_loss: 0.3799 - val_mean_absolute_error: 0.4444\nEpoch 60/100\n934/934 [==============================] - 0s 43us/sample - loss: 0.3949 - mean_absolute_error: 0.4471 - val_loss: 0.3491 - val_mean_absolute_error: 0.4235\nEpoch 61/100\n934/934 [==============================] - 0s 39us/sample - loss: 0.3692 - mean_absolute_error: 0.4273 - val_loss: 0.3237 - val_mean_absolute_error: 0.4057\nEpoch 62/100\n934/934 [==============================] - 0s 43us/sample - loss: 0.3441 - mean_absolute_error: 0.4104 - val_loss: 0.2990 - val_mean_absolute_error: 0.3882\nEpoch 63/100\n934/934 [==============================] - 0s 44us/sample - loss: 0.3219 - mean_absolute_error: 0.3954 - val_loss: 0.2793 - val_mean_absolute_error: 0.3731\nEpoch 64/100\n934/934 [==============================] - 0s 44us/sample - loss: 0.3003 - mean_absolute_error: 0.3817 - val_loss: 0.2582 - val_mean_absolute_error: 0.3583\nEpoch 65/100\n934/934 [==============================] - 0s 43us/sample - loss: 0.2815 - mean_absolute_error: 0.3662 - val_loss: 0.2417 - val_mean_absolute_error: 0.3440\nEpoch 66/100\n934/934 [==============================] - 0s 45us/sample - loss: 0.2642 - mean_absolute_error: 0.3524 - val_loss: 0.2235 - val_mean_absolute_error: 0.3299\nEpoch 67/100\n934/934 [==============================] - 0s 40us/sample - loss: 0.2476 - mean_absolute_error: 0.3429 - val_loss: 0.2088 - val_mean_absolute_error: 0.3181\nEpoch 68/100\n934/934 [==============================] - 0s 38us/sample - loss: 0.2334 - mean_absolute_error: 0.3290 - val_loss: 0.1948 - val_mean_absolute_error: 0.3046\nEpoch 69/100\n934/934 [==============================] - 0s 44us/sample - loss: 0.2197 - mean_absolute_error: 0.3184 - val_loss: 0.1826 - val_mean_absolute_error: 0.2939\nEpoch 70/100\n934/934 [==============================] - 0s 44us/sample - loss: 0.2073 - mean_absolute_error: 0.3061 - val_loss: 0.1724 - val_mean_absolute_error: 0.2839\nEpoch 71/100\n934/934 [==============================] - 0s 44us/sample - loss: 0.1959 - mean_absolute_error: 0.2987 - val_loss: 0.1624 - val_mean_absolute_error: 0.2749\nEpoch 72/100\n934/934 [==============================] - 0s 37us/sample - loss: 0.1865 - mean_absolute_error: 0.2913 - val_loss: 0.1535 - val_mean_absolute_error: 0.2661\nEpoch 73/100\n934/934 [==============================] - 0s 43us/sample - loss: 0.1766 - mean_absolute_error: 0.2802 - val_loss: 0.1446 - val_mean_absolute_error: 0.2577\nEpoch 74/100\n934/934 [==============================] - 0s 38us/sample - loss: 0.1683 - mean_absolute_error: 0.2745 - val_loss: 0.1364 - val_mean_absolute_error: 0.2488\nEpoch 75/100\n934/934 [==============================] - 0s 42us/sample - loss: 0.1601 - mean_absolute_error: 0.2660 - val_loss: 0.1327 - val_mean_absolute_error: 0.2442\nEpoch 76/100\n934/934 [==============================] - 0s 41us/sample - loss: 0.1540 - mean_absolute_error: 0.2589 - val_loss: 0.1273 - val_mean_absolute_error: 0.2383\nEpoch 77/100\n934/934 [==============================] - 0s 45us/sample - loss: 0.1475 - mean_absolute_error: 0.2536 - val_loss: 0.1219 - val_mean_absolute_error: 0.2326\nEpoch 78/100\n934/934 [==============================] - 0s 47us/sample - loss: 0.1424 - mean_absolute_error: 0.2491 - val_loss: 0.1163 - val_mean_absolute_error: 0.2264\nEpoch 79/100\n934/934 [==============================] - 0s 45us/sample - loss: 0.1366 - mean_absolute_error: 0.2425 - val_loss: 0.1124 - val_mean_absolute_error: 0.2219\nEpoch 80/100\n934/934 [==============================] - 0s 40us/sample - loss: 0.1315 - mean_absolute_error: 0.2379 - val_loss: 0.1066 - val_mean_absolute_error: 0.2158\nEpoch 81/100\n934/934 [==============================] - 0s 45us/sample - loss: 0.1277 - mean_absolute_error: 0.2359 - val_loss: 0.1029 - val_mean_absolute_error: 0.2128\nEpoch 82/100\n934/934 [==============================] - 0s 43us/sample - loss: 0.1232 - mean_absolute_error: 0.2308 - val_loss: 0.0990 - val_mean_absolute_error: 0.2075\nEpoch 83/100\n934/934 [==============================] - 0s 46us/sample - loss: 0.1191 - mean_absolute_error: 0.2273 - val_loss: 0.0962 - val_mean_absolute_error: 0.2042\nEpoch 84/100\n934/934 [==============================] - 0s 42us/sample - loss: 0.1159 - mean_absolute_error: 0.2237 - val_loss: 0.0925 - val_mean_absolute_error: 0.1997\nEpoch 85/100\n934/934 [==============================] - 0s 46us/sample - loss: 0.1123 - mean_absolute_error: 0.2212 - val_loss: 0.0901 - val_mean_absolute_error: 0.1965\nEpoch 86/100\n934/934 [==============================] - 0s 46us/sample - loss: 0.1084 - mean_absolute_error: 0.2168 - val_loss: 0.0867 - val_mean_absolute_error: 0.1922\nEpoch 87/100\n934/934 [==============================] - 0s 44us/sample - loss: 0.1059 - mean_absolute_error: 0.2133 - val_loss: 0.0838 - val_mean_absolute_error: 0.1891\nEpoch 88/100\n934/934 [==============================] - 0s 43us/sample - loss: 0.1034 - mean_absolute_error: 0.2118 - val_loss: 0.0810 - val_mean_absolute_error: 0.1864\nEpoch 89/100\n934/934 [==============================] - 0s 43us/sample - loss: 0.1007 - mean_absolute_error: 0.2087 - val_loss: 0.0800 - val_mean_absolute_error: 0.1851\nEpoch 90/100\n934/934 [==============================] - 0s 44us/sample - loss: 0.0989 - mean_absolute_error: 0.2076 - val_loss: 0.0785 - val_mean_absolute_error: 0.1840\nEpoch 91/100\n934/934 [==============================] - 0s 48us/sample - loss: 0.0959 - mean_absolute_error: 0.2051 - val_loss: 0.0759 - val_mean_absolute_error: 0.1813\nEpoch 92/100\n934/934 [==============================] - 0s 43us/sample - loss: 0.0935 - mean_absolute_error: 0.2028 - val_loss: 0.0754 - val_mean_absolute_error: 0.1815\nEpoch 93/100\n934/934 [==============================] - 0s 43us/sample - loss: 0.0917 - mean_absolute_error: 0.2009 - val_loss: 0.0735 - val_mean_absolute_error: 0.1804\nEpoch 94/100\n934/934 [==============================] - 0s 48us/sample - loss: 0.0910 - mean_absolute_error: 0.2011 - val_loss: 0.0727 - val_mean_absolute_error: 0.1798\nEpoch 95/100\n934/934 [==============================] - 0s 64us/sample - loss: 0.0885 - mean_absolute_error: 0.1986 - val_loss: 0.0699 - val_mean_absolute_error: 0.1775\nEpoch 96/100\n934/934 [==============================] - 0s 43us/sample - loss: 0.0869 - mean_absolute_error: 0.1976 - val_loss: 0.0691 - val_mean_absolute_error: 0.1771\nEpoch 97/100\n934/934 [==============================] - 0s 39us/sample - loss: 0.0852 - mean_absolute_error: 0.1959 - val_loss: 0.0678 - val_mean_absolute_error: 0.1763\nEpoch 98/100\n934/934 [==============================] - 0s 46us/sample - loss: 0.0843 - mean_absolute_error: 0.1959 - val_loss: 0.0661 - val_mean_absolute_error: 0.1752\nEpoch 99/100\n934/934 [==============================] - 0s 49us/sample - loss: 0.0829 - mean_absolute_error: 0.1940 - val_loss: 0.0674 - val_mean_absolute_error: 0.1772\nEpoch 100/100\n934/934 [==============================] - 0s 47us/sample - loss: 0.0817 - mean_absolute_error: 0.1934 - val_loss: 0.0646 - val_mean_absolute_error: 0.1741\n"
    }
   ],
   "source": [
    "# モデル構築\n",
    "K.clear_session()\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(10, activation = tf.nn.relu, input_shape=(X_train.shape[1],)))\n",
    "model.add(tf.keras.layers.Dense(3, activation = tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "model.summary()\n",
    "\n",
    "# コンパイル\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=tf.train.AdamOptimizer(learning_rate=0.001),\n",
    "              metrics=['mae'])\n",
    "\n",
    "# 学習\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=32,\n",
    "                    epochs=100,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "y_pred_log [11.978668 11.850881 11.771934 12.224342 11.689271]\ny_test [12.20918779 11.79810441 11.60823564 12.16525065 11.38509209]\nmae 0.1888617944170851\n"
    }
   ],
   "source": [
    "y_pred_log = model.predict(X_test)\n",
    "print(\"y_pred_log\", y_pred_log.flatten()[:5])\n",
    "print(\"y_test\", y_test.flatten()[:5])\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print(\"mae\", mean_absolute_error(y_test, y_pred_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 375.2875 248.518125\" width=\"375.2875pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.518125 \r\nL 375.2875 248.518125 \r\nL 375.2875 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 33.2875 224.64 \r\nL 368.0875 224.64 \r\nL 368.0875 7.2 \r\nL 33.2875 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m5eab58349d\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"48.505682\" xlink:href=\"#m5eab58349d\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n      </defs>\r\n      <g transform=\"translate(45.324432 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"109.993285\" xlink:href=\"#m5eab58349d\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 20 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      </defs>\r\n      <g transform=\"translate(103.630785 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"171.480888\" xlink:href=\"#m5eab58349d\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 40 -->\r\n      <defs>\r\n       <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n      </defs>\r\n      <g transform=\"translate(165.118388 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"232.968492\" xlink:href=\"#m5eab58349d\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 60 -->\r\n      <defs>\r\n       <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n      </defs>\r\n      <g transform=\"translate(226.605992 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-54\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"294.456095\" xlink:href=\"#m5eab58349d\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 80 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n      </defs>\r\n      <g transform=\"translate(288.093595 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-56\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"355.943698\" xlink:href=\"#m5eab58349d\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 100 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g transform=\"translate(346.399948 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"mc6b4104b60\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mc6b4104b60\" y=\"214.842334\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(19.925 218.641552)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mc6b4104b60\" y=\"188.226784\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(13.5625 192.026002)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mc6b4104b60\" y=\"161.611234\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 40 -->\r\n      <g transform=\"translate(13.5625 165.410452)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mc6b4104b60\" y=\"134.995684\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 60 -->\r\n      <g transform=\"translate(13.5625 138.794902)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-54\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mc6b4104b60\" y=\"108.380134\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 80 -->\r\n      <g transform=\"translate(13.5625 112.179352)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-56\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mc6b4104b60\" y=\"81.764584\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 100 -->\r\n      <g transform=\"translate(7.2 85.563802)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mc6b4104b60\" y=\"55.149034\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 120 -->\r\n      <g transform=\"translate(7.2 58.948252)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_8\">\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mc6b4104b60\" y=\"28.533484\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- 140 -->\r\n      <g transform=\"translate(7.2 32.332702)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_15\">\r\n    <path clip-path=\"url(#pdca22d3dc0)\" d=\"M 48.505682 17.083636 \r\nL 51.580062 20.799417 \r\nL 54.654442 23.972808 \r\nL 57.728822 27.03051 \r\nL 60.803202 30.346412 \r\nL 63.877583 34.102637 \r\nL 66.951963 38.558279 \r\nL 70.026343 44.205449 \r\nL 73.100723 51.775196 \r\nL 76.175103 61.602286 \r\nL 79.249483 73.277017 \r\nL 82.323864 86.165808 \r\nL 85.398244 100.14387 \r\nL 88.472624 114.949275 \r\nL 91.547004 129.941109 \r\nL 94.621384 144.446912 \r\nL 97.695764 157.921903 \r\nL 100.770145 169.797616 \r\nL 103.844525 179.772415 \r\nL 106.918905 187.811832 \r\nL 109.993285 194.122783 \r\nL 113.067665 198.832155 \r\nL 116.142045 202.227955 \r\nL 119.216426 204.581227 \r\nL 122.290806 206.226926 \r\nL 125.365186 207.373869 \r\nL 128.439566 208.188332 \r\nL 131.513946 208.814919 \r\nL 134.588326 209.347287 \r\nL 137.662707 209.788928 \r\nL 140.737087 210.184297 \r\nL 143.811467 210.516985 \r\nL 146.885847 210.815886 \r\nL 149.960227 211.096833 \r\nL 153.034607 211.345909 \r\nL 156.108988 211.585468 \r\nL 159.183368 211.81392 \r\nL 162.257748 212.02692 \r\nL 165.332128 212.226869 \r\nL 168.406508 212.421497 \r\nL 171.480888 212.594865 \r\nL 174.555269 212.758273 \r\nL 177.629649 212.911565 \r\nL 180.704029 213.054768 \r\nL 183.778409 213.187263 \r\nL 186.852789 213.315018 \r\nL 189.927169 213.43 \r\nL 193.00155 213.53882 \r\nL 196.07593 213.637299 \r\nL 199.15031 213.732891 \r\nL 202.22469 213.814757 \r\nL 205.29907 213.89362 \r\nL 208.37345 213.960523 \r\nL 211.447831 214.025649 \r\nL 214.522211 214.083966 \r\nL 217.596591 214.140118 \r\nL 220.670971 214.188124 \r\nL 223.745351 214.238092 \r\nL 226.819731 214.27624 \r\nL 229.894112 214.31676 \r\nL 232.968492 214.35096 \r\nL 236.042872 214.384445 \r\nL 239.117252 214.413987 \r\nL 242.191632 214.442656 \r\nL 245.266012 214.467654 \r\nL 248.340393 214.490722 \r\nL 251.414773 214.512825 \r\nL 254.489153 214.531701 \r\nL 257.563533 214.549907 \r\nL 260.637913 214.566518 \r\nL 263.712293 214.581609 \r\nL 266.786674 214.59411 \r\nL 269.861054 214.607343 \r\nL 272.935434 214.618341 \r\nL 276.009814 214.629337 \r\nL 279.084194 214.637394 \r\nL 282.158574 214.646002 \r\nL 285.232955 214.652896 \r\nL 288.307335 214.660593 \r\nL 291.381715 214.667402 \r\nL 294.456095 214.67234 \r\nL 297.530475 214.678363 \r\nL 300.604855 214.683856 \r\nL 303.679236 214.688123 \r\nL 306.753616 214.692925 \r\nL 309.827996 214.69806 \r\nL 312.902376 214.701388 \r\nL 315.976756 214.704753 \r\nL 319.051136 214.708309 \r\nL 322.125517 214.710683 \r\nL 325.199897 214.714739 \r\nL 328.274277 214.717886 \r\nL 331.348657 214.720275 \r\nL 334.423037 214.721196 \r\nL 337.497417 214.72452 \r\nL 340.571798 214.726742 \r\nL 343.646178 214.728904 \r\nL 346.720558 214.73013 \r\nL 349.794938 214.731968 \r\nL 352.869318 214.73359 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_16\">\r\n    <path clip-path=\"url(#pdca22d3dc0)\" d=\"M 48.505682 20.24893 \r\nL 51.580062 23.231069 \r\nL 54.654442 25.969296 \r\nL 57.728822 28.877608 \r\nL 60.803202 32.147712 \r\nL 63.877583 35.947599 \r\nL 66.951963 40.625716 \r\nL 70.026343 46.871901 \r\nL 73.100723 55.203713 \r\nL 76.175103 65.800728 \r\nL 79.249483 77.764862 \r\nL 82.323864 91.038714 \r\nL 85.398244 105.414494 \r\nL 88.472624 120.434107 \r\nL 91.547004 135.507671 \r\nL 94.621384 150.014296 \r\nL 97.695764 163.266637 \r\nL 100.770145 174.829747 \r\nL 103.844525 184.422939 \r\nL 106.918905 191.987281 \r\nL 109.993285 197.799941 \r\nL 113.067665 202.065514 \r\nL 116.142045 204.953609 \r\nL 119.216426 206.90333 \r\nL 122.290806 208.24068 \r\nL 125.365186 209.110626 \r\nL 128.439566 209.746235 \r\nL 131.513946 210.186468 \r\nL 134.588326 210.579713 \r\nL 137.662707 210.882278 \r\nL 140.737087 211.123473 \r\nL 143.811467 211.355948 \r\nL 146.885847 211.571314 \r\nL 149.960227 211.788043 \r\nL 153.034607 211.976649 \r\nL 156.108988 212.167579 \r\nL 159.183368 212.342486 \r\nL 162.257748 212.516432 \r\nL 165.332128 212.675002 \r\nL 168.406508 212.81288 \r\nL 171.480888 212.942696 \r\nL 174.555269 213.074409 \r\nL 177.629649 213.194117 \r\nL 180.704029 213.308469 \r\nL 183.778409 213.417347 \r\nL 186.852789 213.52448 \r\nL 189.927169 213.62126 \r\nL 193.00155 213.701728 \r\nL 196.07593 213.782509 \r\nL 199.15031 213.860003 \r\nL 202.22469 213.934064 \r\nL 205.29907 213.994183 \r\nL 208.37345 214.057684 \r\nL 211.447831 214.119134 \r\nL 214.522211 214.167301 \r\nL 217.596591 214.219181 \r\nL 220.670971 214.268803 \r\nL 223.745351 214.287608 \r\nL 226.819731 214.33679 \r\nL 229.894112 214.377695 \r\nL 232.968492 214.41156 \r\nL 236.042872 214.444461 \r\nL 239.117252 214.470692 \r\nL 242.191632 214.49879 \r\nL 245.266012 214.520727 \r\nL 248.340393 214.544965 \r\nL 251.414773 214.564479 \r\nL 254.489153 214.583151 \r\nL 257.563533 214.599369 \r\nL 260.637913 214.6129 \r\nL 263.712293 214.626214 \r\nL 266.786674 214.638119 \r\nL 269.861054 214.649954 \r\nL 272.935434 214.660853 \r\nL 276.009814 214.665699 \r\nL 279.084194 214.672965 \r\nL 282.158574 214.680084 \r\nL 285.232955 214.687554 \r\nL 288.307335 214.692745 \r\nL 291.381715 214.700538 \r\nL 294.456095 214.70535 \r\nL 297.530475 214.710644 \r\nL 300.604855 214.714357 \r\nL 303.679236 214.719215 \r\nL 306.753616 214.722467 \r\nL 309.827996 214.726971 \r\nL 312.902376 214.730832 \r\nL 315.976756 214.734542 \r\nL 319.051136 214.735916 \r\nL 322.125517 214.737928 \r\nL 325.199897 214.741389 \r\nL 328.274277 214.741951 \r\nL 331.348657 214.744468 \r\nL 334.423037 214.745622 \r\nL 337.497417 214.749304 \r\nL 340.571798 214.750374 \r\nL 343.646178 214.752042 \r\nL 346.720558 214.754376 \r\nL 349.794938 214.75261 \r\nL 352.869318 214.756364 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 33.2875 224.64 \r\nL 33.2875 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 368.0875 224.64 \r\nL 368.0875 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 33.2875 224.64 \r\nL 368.0875 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 33.2875 7.2 \r\nL 368.0875 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"legend_1\">\r\n    <g id=\"patch_7\">\r\n     <path d=\"M 289.946875 44.834375 \r\nL 361.0875 44.834375 \r\nQ 363.0875 44.834375 363.0875 42.834375 \r\nL 363.0875 14.2 \r\nQ 363.0875 12.2 361.0875 12.2 \r\nL 289.946875 12.2 \r\nQ 287.946875 12.2 287.946875 14.2 \r\nL 287.946875 42.834375 \r\nQ 287.946875 44.834375 289.946875 44.834375 \r\nz\r\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\r\n    </g>\r\n    <g id=\"line2d_17\">\r\n     <path d=\"M 291.946875 20.298437 \r\nL 311.946875 20.298437 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_18\"/>\r\n    <g id=\"text_15\">\r\n     <!-- loss -->\r\n     <defs>\r\n      <path d=\"M 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\n\" id=\"DejaVuSans-108\"/>\r\n      <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n      <path d=\"M 44.28125 53.078125 \r\nL 44.28125 44.578125 \r\nQ 40.484375 46.53125 36.375 47.5 \r\nQ 32.28125 48.484375 27.875 48.484375 \r\nQ 21.1875 48.484375 17.84375 46.4375 \r\nQ 14.5 44.390625 14.5 40.28125 \r\nQ 14.5 37.15625 16.890625 35.375 \r\nQ 19.28125 33.59375 26.515625 31.984375 \r\nL 29.59375 31.296875 \r\nQ 39.15625 29.25 43.1875 25.515625 \r\nQ 47.21875 21.78125 47.21875 15.09375 \r\nQ 47.21875 7.46875 41.1875 3.015625 \r\nQ 35.15625 -1.421875 24.609375 -1.421875 \r\nQ 20.21875 -1.421875 15.453125 -0.5625 \r\nQ 10.6875 0.296875 5.421875 2 \r\nL 5.421875 11.28125 \r\nQ 10.40625 8.6875 15.234375 7.390625 \r\nQ 20.0625 6.109375 24.8125 6.109375 \r\nQ 31.15625 6.109375 34.5625 8.28125 \r\nQ 37.984375 10.453125 37.984375 14.40625 \r\nQ 37.984375 18.0625 35.515625 20.015625 \r\nQ 33.0625 21.96875 24.703125 23.78125 \r\nL 21.578125 24.515625 \r\nQ 13.234375 26.265625 9.515625 29.90625 \r\nQ 5.8125 33.546875 5.8125 39.890625 \r\nQ 5.8125 47.609375 11.28125 51.796875 \r\nQ 16.75 56 26.8125 56 \r\nQ 31.78125 56 36.171875 55.265625 \r\nQ 40.578125 54.546875 44.28125 53.078125 \r\nz\r\n\" id=\"DejaVuSans-115\"/>\r\n     </defs>\r\n     <g transform=\"translate(319.946875 23.798437)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"line2d_19\">\r\n     <path d=\"M 291.946875 34.976562 \r\nL 311.946875 34.976562 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_20\"/>\r\n    <g id=\"text_16\">\r\n     <!-- val_loss -->\r\n     <defs>\r\n      <path d=\"M 2.984375 54.6875 \r\nL 12.5 54.6875 \r\nL 29.59375 8.796875 \r\nL 46.6875 54.6875 \r\nL 56.203125 54.6875 \r\nL 35.6875 0 \r\nL 23.484375 0 \r\nz\r\n\" id=\"DejaVuSans-118\"/>\r\n      <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n      <path d=\"M 50.984375 -16.609375 \r\nL 50.984375 -23.578125 \r\nL -0.984375 -23.578125 \r\nL -0.984375 -16.609375 \r\nz\r\n\" id=\"DejaVuSans-95\"/>\r\n     </defs>\r\n     <g transform=\"translate(319.946875 38.476562)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-118\"/>\r\n      <use x=\"59.179688\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"120.458984\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"148.242188\" xlink:href=\"#DejaVuSans-95\"/>\r\n      <use x=\"198.242188\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"226.025391\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"287.207031\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"339.306641\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"pdca22d3dc0\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"33.2875\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(len(history.history['loss'])), history.history['loss'], label='loss')\n",
    "plt.plot(range(len(history.history['val_loss'])), history.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題6】\n",
    "#### MNISTをKerasで学習\n",
    "TensorFlowによるMNISTデータセットによる画像の多値分類をKerasに書き換えてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(48000, 28, 28, 1)\n(12000, 28, 28, 1)\n(10000, 28, 28, 1)\n"
    }
   ],
   "source": [
    "# 読み込み\n",
    "from tensorflow.keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.astype(np.float)[:, :, :, np.newaxis]\n",
    "X_test = X_test.astype(np.float)[:, :, :, np.newaxis]\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(48000, 10)\n(12000, 10)\n(10000, 10)\n"
    }
   ],
   "source": [
    "y_train = np.identity(10)[y_train]\n",
    "y_val = np.identity(10)[y_val]\n",
    "y_test = np.identity(10)[y_test]\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題7】\n",
    "#### （アドバンス課題）PyTorchへの書き換え\n",
    "4種類の問題をPyTorchに書き換えてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras: (N, H, W, C)\n",
    "\n",
    "PyTorch: (N, C, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "      Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm  \\\n0      1            5.1           3.5            1.4           0.2   \n1      2            4.9           3.0            1.4           0.2   \n2      3            4.7           3.2            1.3           0.2   \n3      4            4.6           3.1            1.5           0.2   \n4      5            5.0           3.6            1.4           0.2   \n..   ...            ...           ...            ...           ...   \n145  146            6.7           3.0            5.2           2.3   \n146  147            6.3           2.5            5.0           1.9   \n147  148            6.5           3.0            5.2           2.0   \n148  149            6.2           3.4            5.4           2.3   \n149  150            5.9           3.0            5.1           1.8   \n\n            Species  \n0       Iris-setosa  \n1       Iris-setosa  \n2       Iris-setosa  \n3       Iris-setosa  \n4       Iris-setosa  \n..              ...  \n145  Iris-virginica  \n146  Iris-virginica  \n147  Iris-virginica  \n148  Iris-virginica  \n149  Iris-virginica  \n\n[150 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>SepalLengthCm</th>\n      <th>SepalWidthCm</th>\n      <th>PetalLengthCm</th>\n      <th>PetalWidthCm</th>\n      <th>Species</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>5.1</td>\n      <td>3.5</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>4.9</td>\n      <td>3.0</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>4.7</td>\n      <td>3.2</td>\n      <td>1.3</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>4.6</td>\n      <td>3.1</td>\n      <td>1.5</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>5.0</td>\n      <td>3.6</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>146</td>\n      <td>6.7</td>\n      <td>3.0</td>\n      <td>5.2</td>\n      <td>2.3</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>146</th>\n      <td>147</td>\n      <td>6.3</td>\n      <td>2.5</td>\n      <td>5.0</td>\n      <td>1.9</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>148</td>\n      <td>6.5</td>\n      <td>3.0</td>\n      <td>5.2</td>\n      <td>2.0</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>148</th>\n      <td>149</td>\n      <td>6.2</td>\n      <td>3.4</td>\n      <td>5.4</td>\n      <td>2.3</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>149</th>\n      <td>150</td>\n      <td>5.9</td>\n      <td>3.0</td>\n      <td>5.1</td>\n      <td>1.8</td>\n      <td>Iris-virginica</td>\n    </tr>\n  </tbody>\n</table>\n<p>150 rows × 6 columns</p>\n</div>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "(100, 4)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "(100, 1)"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(64, 4) (64, 1)\n(16, 4) (16, 1)\n(20, 4) (20, 1)\n"
    }
   ],
   "source": [
    "# 【問題3】Iris（2値分類）\n",
    "## 読み込み\n",
    "df_iris = pd.read_csv('Iris.csv')\n",
    "display(df_iris)\n",
    "X = np.array(df_iris[(df_iris['Species'] == 'Iris-versicolor')|(df_iris['Species'] == 'Iris-virginica')].iloc[:, 1:5])\n",
    "y = np.array(df_iris[(df_iris['Species'] == 'Iris-versicolor')|(df_iris['Species'] == 'Iris-virginica')].iloc[:, 5])\n",
    "y[y=='Iris-versicolor'] = 0\n",
    "y[y=='Iris-virginica'] = 1\n",
    "y = y.astype(np.int)[:, np.newaxis]\n",
    "display(X.shape)\n",
    "display(y.shape)\n",
    "\n",
    "## 分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "## 標準化\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "NNfor2(\n  (layer_1): Linear(in_features=4, out_features=10, bias=True)\n  (layer_2): Linear(in_features=10, out_features=5, bias=True)\n  (layer_output): Linear(in_features=5, out_features=1, bias=True)\n)\n"
    }
   ],
   "source": [
    "# PyTorchでモデル構築\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "n_features = 4\n",
    "n_nodes_1 = 10\n",
    "activation_1 = nn.ReLU()\n",
    "n_nodes_2 = 5\n",
    "activation_2 = nn.ReLU()\n",
    "n_output = 1\n",
    "activation_output = nn.Sigmoid()\n",
    "\n",
    "## ネットワーク\n",
    "class NNfor2(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(NNfor2, self).__init__()\n",
    "        self.layer_1 = nn.Linear(\n",
    "            n_features,\n",
    "            n_nodes_1)\n",
    "        self.layer_2 = nn.Linear(\n",
    "            n_nodes_1,\n",
    "            n_nodes_2)\n",
    "        self.layer_output = nn.Linear(\n",
    "            n_nodes_2,\n",
    "            n_output)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = activation_1(self.layer_1(X))\n",
    "        X = activation_2(self.layer_2(X))\n",
    "        X = activation_output(self.layer_output(X))\n",
    "\n",
    "        return X\n",
    "\n",
    "## インスタンス作成\n",
    "model = NNfor2()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ミニバッチ\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "## numpy配列をテンソルに変換\n",
    "t_X_train = torch.from_numpy(X_train).float()\n",
    "t_y_train = torch.from_numpy(y_train).float()\n",
    "t_X_valid = torch.from_numpy(X_val).float()\n",
    "t_y_valid = torch.from_numpy(y_val).float()\n",
    "\n",
    "## 特徴量とラベルをデータセットにまとめる\n",
    "dataset_train = TensorDataset(t_X_train, t_y_train)\n",
    "dataset_valid = TensorDataset(t_X_valid, t_y_valid)\n",
    "\n",
    "# データローダを作成\n",
    "loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "loader_valid = DataLoader(dataset_valid, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "## オプティマイザ\n",
    "import torch.optim as optim\n",
    "\n",
    "lr = 0.003\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=lr\n",
    ")\n",
    "\n",
    "## 損失関数\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 学習(train)\n",
    "def train_step(X_train, y_train):\n",
    "    ## 訓練モード    \n",
    "    model.train()\n",
    "\n",
    "    ## 訓練データに対して予測\n",
    "    y_train_pred = model(X_train)\n",
    "\n",
    "    ## 勾配初期化\n",
    "    optimizer.zero_grad()\n",
    "    ## 損失の取得\n",
    "    loss = criterion(y_train_pred, y_train)\n",
    "    ## 勾配を計算\n",
    "    loss.backward()\n",
    "\n",
    "    ## パラメータ更新\n",
    "    optimizer.step()\n",
    "\n",
    "    ## 正解率の確認\n",
    "    with torch.no_grad():\n",
    "        y_train_pred_binary = np.where(y_train_pred>0.5, 1, 0)\n",
    "        acc = (y_train_pred_binary == y_train).sum()\n",
    "\n",
    "    return (loss.item(), acc.item())\n",
    "\n",
    "## 評価(validation)\n",
    "def valid_step(X_val, y_val):\n",
    "    ## 評価モード\n",
    "    model.eval()\n",
    "\n",
    "    ## 予測\n",
    "    y_val_pred = model(X_val)\n",
    "\n",
    "    ## 損失計算\n",
    "    loss = criterion(y_val_pred, y_val)\n",
    "\n",
    "    ## 正解率計算\n",
    "    with torch.no_grad():\n",
    "        y_val_pred_binary = np.where(y_val_pred>0.5, 1, 0)\n",
    "        acc = (y_val_pred_binary == y_val).sum()\n",
    "\n",
    "    return (loss.item(), acc.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## パラメータ初期化関数\n",
    "def initializer(layer):\n",
    "    if type(layer) == nn.Linear:\n",
    "        nn.init.kaiming_normal_(layer.weight)\n",
    "        layer.bias.data.fill(0.0)\n",
    "\n",
    "## パラメータ初期化する\n",
    "model.apply(initializer)\n",
    "\n",
    "n_epoch = 10\n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "avg_loss = 0.0\n",
    "avg_acc = 0.0\n",
    "avg_val_loss = 0.0\n",
    "avg_val_acc = 0.0\n",
    "\n",
    "## 学習する\n",
    "for epoch in range(n_epoch):\n",
    "    ## epoch内のloss, accの保存\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    total_val_loss = 0.0\n",
    "    total_val_acc = 0.0\n",
    "    ## 累計データ数の保存\n",
    "    total_train = 0\n",
    "    total_val = 0\n",
    "\n",
    "    ## 訓練\n",
    "    for X_train, y_train in loader_train:\n",
    "       loss, acc = train_step(X_train, y_train)\n",
    "       total_loss += loss\n",
    "       total_acc += acc\n",
    "       total_train += len(y_train)\n",
    "\n",
    "    ## 評価\n",
    "    for X_val, y_val in loader_valid:\n",
    "        val_loss, val_acc = valid_step(X_val, y_val)\n",
    "        total_val_loss += val_loss\n",
    "        total_val_acc += val_acc\n",
    "        total_val += len(y_val)\n",
    "\n",
    "    n = epoch+1\n",
    "    avg_loss = total_loss/n\n",
    "    avg_val_loss = total_val_loss/n\n",
    "    avg_acc = total_acc/total_train\n",
    "    avg_val_acc = total_val_acc/total_val\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題8】\n",
    "#### （アドバンス課題）フレームワークの比較\n",
    "それぞれのフレームワークにはどのような違いがあるかをまとめてください。\n",
    "\n",
    "\n",
    "**《視点例》**\n",
    "\n",
    "\n",
    "- 計算速度\n",
    "- コードの行数・可読性\n",
    "- 用意されている機能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bitdicconda58dbae13a5ad45af92cdb395e5ca7493",
   "display_name": "Python 3.7.7 64-bit ('dic': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}