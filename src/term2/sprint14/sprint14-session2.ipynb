{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint\n",
    "## ディープラーニングフレームワーク2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.このSprintについて\n",
    "\n",
    "### Sprintの目的\n",
    "- フレームワークのコードを読めるようにする\n",
    "- フレームワークを習得し続けられるようになる\n",
    "- 理論を知っている範囲をフレームワークで動かす\n",
    "\n",
    "### どのように学ぶか\n",
    "前半はTensorFlowのExampleを動かします。後半ではKerasのコードを書いていきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.公式Example\n",
    "\n",
    "深層学習フレームワークには公式に様々なモデルのExampleコードが公開されています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題1】\n",
    "#### 公式チュートリアルモデルを分担して実行\n",
    "TensorFLowの公式チュートリアルモデルを分担して実行してください。\n",
    "\n",
    "\n",
    "以下の中から1人ひとつ選び実行し、その結果を簡単に発表してください。\n",
    "\n",
    "\n",
    "[models/tutorials at master · tensorflow/models](https://www.tensorflow.org/tutorials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kerasによる分散トレーニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "WARNING:absl:Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n"
    }
   ],
   "source": [
    "# mnistデータセットの読み込み\n",
    "tfds.disable_progress_bar()\n",
    "datasets, info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
    "\n",
    "mnist_train, mnist_test = datasets['train'], datasets['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用可能デバイス探索\n",
    "def get_available_devices():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x for x in local_device_protos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[name: \"/device:CPU:0\"\n device_type: \"CPU\"\n memory_limit: 268435456\n locality {\n }\n incarnation: 8140574946194694337]"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "get_available_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:There is non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\nWARNING:tensorflow:There is non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n"
    }
   ],
   "source": [
    "# 複数GPUでの計算を可能にするAPI\n",
    "# このスコープ内でモデルを作成すると分散処理ができる\n",
    "strategy = tf.distribute.MirroredStrategy(devices=['/cpu:0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Number of devices: 1\n"
    }
   ],
   "source": [
    "# デバイス数表示\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ分割\n",
    "num_train_examples = info.splits['train'].num_examples\n",
    "num_test_examples = info.splits['test'].num_examples\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "BATCH_SIZE_PER_REPLICA = 64\n",
    "BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# スケーリング\n",
    "def scale(image, label):\n",
    "  image = tf.cast(image, tf.float32)\n",
    "  image /= 255\n",
    "\n",
    "  return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = mnist_train.map(scale).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "eval_dataset = mnist_test.map(scale).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From C:\\Users\\190450781\\Anaconda3\\envs\\dic\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\nInstructions for updating:\nIf using Keras pass *_constraint arguments to layers.\nWARNING:tensorflow:From C:\\Users\\190450781\\Anaconda3\\envs\\dic\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\nInstructions for updating:\nIf using Keras pass *_constraint arguments to layers.\n"
    }
   ],
   "source": [
    "# model作成\n",
    "# strategyのスコープ内で作成することで計算をうまく複数デバイスに分散してくれる\n",
    "with strategy.scope():\n",
    "  model = tf.keras.Sequential([\n",
    "      tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\n",
    "      tf.keras.layers.MaxPooling2D(),\n",
    "      tf.keras.layers.Flatten(),\n",
    "      tf.keras.layers.Dense(64, activation='relu'),\n",
    "      tf.keras.layers.Dense(10)\n",
    "  ])\n",
    "\n",
    "  model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint作成\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習率をエポックごとに変更する\n",
    "def decay(epoch):\n",
    "  if epoch < 3:\n",
    "    return 1e-3\n",
    "  elif epoch >= 3 and epoch < 7:\n",
    "    return 1e-4\n",
    "  else:\n",
    "    return 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習率を表示する\n",
    "class PrintLR(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    print('\\nLearning rate for epoch {} is {}'.format(epoch + 1,\n",
    "                                                      model.optimizer.lr.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コールバック設定\n",
    "callbacks = [\n",
    "    #tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,\n",
    "                                       save_weights_only=True),\n",
    "    tf.keras.callbacks.LearningRateScheduler(decay),\n",
    "    PrintLR()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習\n",
    "# model.fit(train_dataset, epochs=12, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> localだとうまく動かない/CPU1基のみなのでGoogle Colabを利用: ./keras_distribute.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題2】\n",
    "#### （アドバンス課題）様々な手法を実行\n",
    "TensorFLowやGoogle AI ResearchのGitHubリポジトリには、定番のモデルから最新のモデルまで多様なコードが公開されています。これらから興味あるものを選び実行してください。\n",
    "\n",
    "\n",
    "なお、これらのコードは初学者向けではないため、巨大なデータセットのダウンロードが必要な場合など、実行が簡単ではないこともあります。そういった場合は、コードリーディングを行ってください。\n",
    "\n",
    "\n",
    "[models/research at master · tensorflow/models]()\n",
    "\n",
    "\n",
    "[google-research/google-research: Google AI Research]()\n",
    "\n",
    "\n",
    "更新日が古いものはPythonやTensorFlowのバージョンが古く、扱いずらい場合があります。新しいものから見ることを推奨します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.異なるフレームワークへの書き換え\n",
    "\n",
    "「ディープラーニングフレームワーク1」で作成した4種類のデータセットを扱うTensorFLowのコードを異なるフレームワークに変更していきます。\n",
    "\n",
    "\n",
    "- Iris（Iris-versicolorとIris-virginicaのみの2値分類）\n",
    "- Iris（3種類全ての目的変数を使用して多値分類）\n",
    "- House Prices\n",
    "- MNIST\n",
    "\n",
    "### Kerasへの書き換え\n",
    "KerasはTensorFLowに含まれるtf.kerasモジュールを使用してください。\n",
    "\n",
    "\n",
    "KerasにはSequentialモデルかFunctional APIかなど書き方に種類がありますが、これは指定しません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題3】\n",
    "#### Iris（2値分類）をKerasで学習\n",
    "TensorFlowによるIrisデータセットに対する2値分類をKerasに書き換えてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "      Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm  \\\n0      1            5.1           3.5            1.4           0.2   \n1      2            4.9           3.0            1.4           0.2   \n2      3            4.7           3.2            1.3           0.2   \n3      4            4.6           3.1            1.5           0.2   \n4      5            5.0           3.6            1.4           0.2   \n..   ...            ...           ...            ...           ...   \n145  146            6.7           3.0            5.2           2.3   \n146  147            6.3           2.5            5.0           1.9   \n147  148            6.5           3.0            5.2           2.0   \n148  149            6.2           3.4            5.4           2.3   \n149  150            5.9           3.0            5.1           1.8   \n\n            Species  \n0       Iris-setosa  \n1       Iris-setosa  \n2       Iris-setosa  \n3       Iris-setosa  \n4       Iris-setosa  \n..              ...  \n145  Iris-virginica  \n146  Iris-virginica  \n147  Iris-virginica  \n148  Iris-virginica  \n149  Iris-virginica  \n\n[150 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>SepalLengthCm</th>\n      <th>SepalWidthCm</th>\n      <th>PetalLengthCm</th>\n      <th>PetalWidthCm</th>\n      <th>Species</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>5.1</td>\n      <td>3.5</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>4.9</td>\n      <td>3.0</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>4.7</td>\n      <td>3.2</td>\n      <td>1.3</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>4.6</td>\n      <td>3.1</td>\n      <td>1.5</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>5.0</td>\n      <td>3.6</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>146</td>\n      <td>6.7</td>\n      <td>3.0</td>\n      <td>5.2</td>\n      <td>2.3</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>146</th>\n      <td>147</td>\n      <td>6.3</td>\n      <td>2.5</td>\n      <td>5.0</td>\n      <td>1.9</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>148</td>\n      <td>6.5</td>\n      <td>3.0</td>\n      <td>5.2</td>\n      <td>2.0</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>148</th>\n      <td>149</td>\n      <td>6.2</td>\n      <td>3.4</td>\n      <td>5.4</td>\n      <td>2.3</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>149</th>\n      <td>150</td>\n      <td>5.9</td>\n      <td>3.0</td>\n      <td>5.1</td>\n      <td>1.8</td>\n      <td>Iris-virginica</td>\n    </tr>\n  </tbody>\n</table>\n<p>150 rows × 6 columns</p>\n</div>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "(100, 4)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "(100, 1)"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(64, 4) (64, 1)\n(16, 4) (16, 1)\n(20, 4) (20, 1)\n"
    }
   ],
   "source": [
    "# dataset読み込み\n",
    "df_iris = pd.read_csv('Iris.csv')\n",
    "display(df_iris)\n",
    "X = np.array(df_iris[(df_iris['Species'] == 'Iris-versicolor')|(df_iris['Species'] == 'Iris-virginica')].iloc[:, 1:5])\n",
    "y = np.array(df_iris[(df_iris['Species'] == 'Iris-versicolor')|(df_iris['Species'] == 'Iris-virginica')].iloc[:, 5])\n",
    "y[y=='Iris-versicolor'] = 0\n",
    "y[y=='Iris-virginica'] = 1\n",
    "y = y.astype(np.int)[:, np.newaxis]\n",
    "display(X.shape)\n",
    "display(y.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense (Dense)                (None, 10)                50        \n_________________________________________________________________\ndense_1 (Dense)              (None, 5)                 55        \n_________________________________________________________________\ndense_2 (Dense)              (None, 1)                 6         \n=================================================================\nTotal params: 111\nTrainable params: 111\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "K.clear_session()\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(10, activation = tf.nn.relu, input_shape=(4,)))\n",
    "model.add(tf.keras.layers.Dense(5, activation = tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(1, activation = tf.nn.sigmoid))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From C:\\Users\\190450781\\Anaconda3\\envs\\dic\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.where in 2.0, which has the same broadcast rule as np.where\nWARNING:tensorflow:From C:\\Users\\190450781\\Anaconda3\\envs\\dic\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.where in 2.0, which has the same broadcast rule as np.where\nTrain on 64 samples, validate on 16 samples\nEpoch 1/10\n64/64 [==============================] - 0s 4ms/sample - loss: 0.4242 - acc: 0.8125 - val_loss: 0.0420 - val_acc: 1.0000\nEpoch 2/10\n64/64 [==============================] - 0s 2ms/sample - loss: 0.1144 - acc: 0.9531 - val_loss: 0.0866 - val_acc: 0.9375\nEpoch 3/10\n64/64 [==============================] - 0s 2ms/sample - loss: 0.0836 - acc: 0.9688 - val_loss: 0.0987 - val_acc: 0.9375\nEpoch 4/10\n64/64 [==============================] - 0s 2ms/sample - loss: 0.0289 - acc: 1.0000 - val_loss: 0.0043 - val_acc: 1.0000\nEpoch 5/10\n64/64 [==============================] - 0s 2ms/sample - loss: 0.0319 - acc: 0.9844 - val_loss: 0.1492 - val_acc: 0.9375\nEpoch 6/10\n64/64 [==============================] - 0s 2ms/sample - loss: 0.0409 - acc: 0.9844 - val_loss: 0.0412 - val_acc: 1.0000\nEpoch 7/10\n64/64 [==============================] - 0s 2ms/sample - loss: 0.0131 - acc: 1.0000 - val_loss: 0.0281 - val_acc: 1.0000\nEpoch 8/10\n64/64 [==============================] - 0s 2ms/sample - loss: 0.0090 - acc: 1.0000 - val_loss: 0.0213 - val_acc: 1.0000\nEpoch 9/10\n64/64 [==============================] - 0s 2ms/sample - loss: 0.0066 - acc: 1.0000 - val_loss: 0.0222 - val_acc: 1.0000\nEpoch 10/10\n64/64 [==============================] - 0s 2ms/sample - loss: 0.0028 - acc: 1.0000 - val_loss: 0.0144 - val_acc: 1.0000\n"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=tf.train.AdamOptimizer(learning_rate=0.03),\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=1,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "y_pred_proba [6.5353513e-04 1.0000000e+00 6.1103702e-04 1.0000000e+00 1.0000000e+00\n 1.0000000e+00 2.1481514e-04 9.9960184e-01 1.0000000e+00 1.0000000e+00\n 1.0000000e+00 9.9999988e-01 1.0000000e+00 4.2793155e-04 0.0000000e+00\n 0.0000000e+00 8.9575833e-01 1.2814999e-06 9.9995333e-01 2.0921230e-05]\ny_pred [0 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 1 0 1 0]\ny_test [0 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 0 0 0 0]\n"
    }
   ],
   "source": [
    "y_pred_proba = model.predict(X_test)[:, 0]\n",
    "y_pred = np.where(y_pred_proba >0.5, 1, 0)\n",
    "print(\"y_pred_proba\", y_pred_proba)\n",
    "print(\"y_pred\", y_pred)\n",
    "print(\"y_test\", y_test.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題4】\n",
    "#### Iris（多値分類）をKerasで学習\n",
    "TensorFlowによるIrisデータセットに対する3値分類をKerasに書き換えてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "(150, 4)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "(150, 3)"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(96, 4) (96, 3)\n(24, 4) (24, 3)\n(30, 4) (30, 3)\n"
    }
   ],
   "source": [
    "# 抽出\n",
    "X = np.array(df_iris.iloc[:, 1:5])\n",
    "y = np.array(pd.get_dummies(df_iris.iloc[:, 5]))\n",
    "y = y.astype(np.int)\n",
    "display(X.shape)\n",
    "display(y.shape)\n",
    "\n",
    "# 分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "# 標準化\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense (Dense)                (None, 100)               500       \n_________________________________________________________________\ndense_1 (Dense)              (None, 10)                1010      \n_________________________________________________________________\ndense_2 (Dense)              (None, 3)                 33        \n=================================================================\nTotal params: 1,543\nTrainable params: 1,543\nNon-trainable params: 0\n_________________________________________________________________\nTrain on 96 samples, validate on 24 samples\nEpoch 1/10\n96/96 [==============================] - 0s 2ms/sample - loss: 0.5897 - acc: 0.8021 - val_loss: 0.3130 - val_acc: 0.8750\nEpoch 2/10\n96/96 [==============================] - 0s 2ms/sample - loss: 0.4551 - acc: 0.9062 - val_loss: 0.2427 - val_acc: 0.9167\nEpoch 3/10\n96/96 [==============================] - 0s 2ms/sample - loss: 0.1639 - acc: 0.9271 - val_loss: 0.7197 - val_acc: 0.8333\nEpoch 4/10\n96/96 [==============================] - 0s 2ms/sample - loss: 0.3039 - acc: 0.9479 - val_loss: 0.3211 - val_acc: 0.8333\nEpoch 5/10\n96/96 [==============================] - 0s 2ms/sample - loss: 0.2284 - acc: 0.9375 - val_loss: 0.8867 - val_acc: 0.8333\nEpoch 6/10\n96/96 [==============================] - 0s 2ms/sample - loss: 0.0925 - acc: 0.9583 - val_loss: 0.6100 - val_acc: 0.8333\nEpoch 7/10\n96/96 [==============================] - 0s 2ms/sample - loss: 0.1265 - acc: 0.9479 - val_loss: 0.2055 - val_acc: 0.9167\nEpoch 8/10\n96/96 [==============================] - 0s 2ms/sample - loss: 0.0778 - acc: 0.9688 - val_loss: 0.3839 - val_acc: 0.8750\nEpoch 9/10\n96/96 [==============================] - 0s 1ms/sample - loss: 0.0639 - acc: 0.9688 - val_loss: 0.6815 - val_acc: 0.8333\nEpoch 10/10\n96/96 [==============================] - 0s 2ms/sample - loss: 0.3510 - acc: 0.9062 - val_loss: 0.4975 - val_acc: 0.7083\n"
    }
   ],
   "source": [
    "# モデル構築\n",
    "K.clear_session()\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(100, activation = tf.nn.relu, input_shape=(4,)))\n",
    "model.add(tf.keras.layers.Dense(10, activation = tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(3, activation = tf.nn.softmax))\n",
    "model.summary()\n",
    "\n",
    "# コンパイル\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=tf.train.AdamOptimizer(learning_rate=0.03),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 学習\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=1,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "y_pred_proba [[1.3322644e-09 3.0437014e-01 6.9562984e-01]\n [2.1102066e-04 9.9788874e-01 1.9002205e-03]\n [1.0000000e+00 0.0000000e+00 8.9128140e-27]\n [1.6556097e-03 8.1835383e-01 1.7999050e-01]\n [1.0000000e+00 5.6482033e-32 7.4052302e-21]\n [1.9489766e-13 9.8898217e-02 9.0110177e-01]\n [1.0000000e+00 5.7513239e-33 1.7498283e-21]\n [2.8673441e-03 9.8445052e-01 1.2682107e-02]\n [2.1001108e-03 9.8835677e-01 9.5430175e-03]\n [1.0392122e-03 9.9183756e-01 7.1232389e-03]\n [3.7480392e-02 8.5434127e-01 1.0817821e-01]\n [3.2238055e-03 9.8099434e-01 1.5781913e-02]\n [1.8574912e-03 9.8572332e-01 1.2419293e-02]\n [1.0397405e-02 9.5760596e-01 3.1996641e-02]\n [1.1976196e-02 9.4360131e-01 4.4422537e-02]\n [1.0000000e+00 2.7207004e-29 3.6620149e-19]\n [1.9804521e-02 9.1809434e-01 6.2101174e-02]\n [7.0870682e-03 9.6572101e-01 2.7191881e-02]\n [1.0000000e+00 7.4953040e-26 5.4471167e-17]\n [1.0000000e+00 1.4670540e-35 4.0299606e-23]\n [4.6910272e-06 6.2480384e-01 3.7519151e-01]\n [3.6118742e-02 8.5799110e-01 1.0589018e-01]\n [1.0000000e+00 1.1232757e-30 4.8932354e-20]\n [1.0000000e+00 3.0403319e-27 7.1977237e-18]\n [1.8078107e-02 8.5799748e-01 1.2392436e-01]\n [1.0000000e+00 0.0000000e+00 4.1282477e-25]\n [1.0000000e+00 2.8994271e-32 4.8601942e-21]\n [1.2702357e-03 9.9035990e-01 8.3699152e-03]\n [6.5741182e-04 9.9500000e-01 4.3427180e-03]\n [1.0000000e+00 1.2375526e-28 9.5318659e-19]]\ny_pred [2 1 0 1 0 2 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 0 1 1 0]\ny_test [2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0]\n"
    }
   ],
   "source": [
    "y_pred_proba = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "print(\"y_pred_proba\", y_pred_proba)\n",
    "print(\"y_pred\", y_pred)\n",
    "print(\"y_test\", np.argmax(y_test, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題5】\n",
    "#### House PricesをKerasで学習\n",
    "TensorFlowによるHouse Pricesデータセットに対する回帰をKerasに書き換えてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "        Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n0        1          60       RL         65.0     8450   Pave   NaN      Reg   \n1        2          20       RL         80.0     9600   Pave   NaN      Reg   \n2        3          60       RL         68.0    11250   Pave   NaN      IR1   \n3        4          70       RL         60.0     9550   Pave   NaN      IR1   \n4        5          60       RL         84.0    14260   Pave   NaN      IR1   \n...    ...         ...      ...          ...      ...    ...   ...      ...   \n1455  1456          60       RL         62.0     7917   Pave   NaN      Reg   \n1456  1457          20       RL         85.0    13175   Pave   NaN      Reg   \n1457  1458          70       RL         66.0     9042   Pave   NaN      Reg   \n1458  1459          20       RL         68.0     9717   Pave   NaN      Reg   \n1459  1460          20       RL         75.0     9937   Pave   NaN      Reg   \n\n     LandContour Utilities  ... PoolArea PoolQC  Fence MiscFeature MiscVal  \\\n0            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n1            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n2            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n3            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n4            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n...          ...       ...  ...      ...    ...    ...         ...     ...   \n1455         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n1456         Lvl    AllPub  ...        0    NaN  MnPrv         NaN       0   \n1457         Lvl    AllPub  ...        0    NaN  GdPrv        Shed    2500   \n1458         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n1459         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n\n     MoSold YrSold  SaleType  SaleCondition  SalePrice  \n0         2   2008        WD         Normal     208500  \n1         5   2007        WD         Normal     181500  \n2         9   2008        WD         Normal     223500  \n3         2   2006        WD        Abnorml     140000  \n4        12   2008        WD         Normal     250000  \n...     ...    ...       ...            ...        ...  \n1455      8   2007        WD         Normal     175000  \n1456      2   2010        WD         Normal     210000  \n1457      5   2010        WD         Normal     266500  \n1458      4   2010        WD         Normal     142125  \n1459      6   2008        WD         Normal     147500  \n\n[1460 rows x 81 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>MSSubClass</th>\n      <th>MSZoning</th>\n      <th>LotFrontage</th>\n      <th>LotArea</th>\n      <th>Street</th>\n      <th>Alley</th>\n      <th>LotShape</th>\n      <th>LandContour</th>\n      <th>Utilities</th>\n      <th>...</th>\n      <th>PoolArea</th>\n      <th>PoolQC</th>\n      <th>Fence</th>\n      <th>MiscFeature</th>\n      <th>MiscVal</th>\n      <th>MoSold</th>\n      <th>YrSold</th>\n      <th>SaleType</th>\n      <th>SaleCondition</th>\n      <th>SalePrice</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>65.0</td>\n      <td>8450</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>208500</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>20</td>\n      <td>RL</td>\n      <td>80.0</td>\n      <td>9600</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>5</td>\n      <td>2007</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>181500</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>68.0</td>\n      <td>11250</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>9</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>223500</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>70</td>\n      <td>RL</td>\n      <td>60.0</td>\n      <td>9550</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2006</td>\n      <td>WD</td>\n      <td>Abnorml</td>\n      <td>140000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>84.0</td>\n      <td>14260</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>12</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>250000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1455</th>\n      <td>1456</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>62.0</td>\n      <td>7917</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>8</td>\n      <td>2007</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>175000</td>\n    </tr>\n    <tr>\n      <th>1456</th>\n      <td>1457</td>\n      <td>20</td>\n      <td>RL</td>\n      <td>85.0</td>\n      <td>13175</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>MnPrv</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2010</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>210000</td>\n    </tr>\n    <tr>\n      <th>1457</th>\n      <td>1458</td>\n      <td>70</td>\n      <td>RL</td>\n      <td>66.0</td>\n      <td>9042</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>GdPrv</td>\n      <td>Shed</td>\n      <td>2500</td>\n      <td>5</td>\n      <td>2010</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>266500</td>\n    </tr>\n    <tr>\n      <th>1458</th>\n      <td>1459</td>\n      <td>20</td>\n      <td>RL</td>\n      <td>68.0</td>\n      <td>9717</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>4</td>\n      <td>2010</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>142125</td>\n    </tr>\n    <tr>\n      <th>1459</th>\n      <td>1460</td>\n      <td>20</td>\n      <td>RL</td>\n      <td>75.0</td>\n      <td>9937</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>6</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>147500</td>\n    </tr>\n  </tbody>\n</table>\n<p>1460 rows × 81 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# 読み込み\n",
    "df_price = pd.read_csv('train.csv')\n",
    "display(df_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "(1460, 3)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "(1460, 1)"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(934, 3) (934, 1)\n(234, 3) (234, 1)\n(292, 3) (292, 1)\n"
    }
   ],
   "source": [
    "# 抽出\n",
    "X = df_price[['LotArea', 'GrLivArea', 'YearBuilt']].values\n",
    "y = df_price['SalePrice'].values[:, np.newaxis]\n",
    "display(X.shape)\n",
    "display(y.shape)\n",
    "\n",
    "# 分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "# 標準化\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# 対数変換\n",
    "y_train = np.log(y_train)\n",
    "y_val = np.log(y_val)\n",
    "y_test = np.log(y_test)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense (Dense)                (None, 10)                40        \n_________________________________________________________________\ndense_1 (Dense)              (None, 3)                 33        \n_________________________________________________________________\ndense_2 (Dense)              (None, 1)                 4         \n=================================================================\nTotal params: 77\nTrainable params: 77\nNon-trainable params: 0\n_________________________________________________________________\nTrain on 934 samples, validate on 234 samples\nEpoch 1/100\n934/934 [==============================] - 0s 146us/sample - loss: 146.1875 - mean_absolute_error: 12.0859 - val_loss: 143.5800 - val_mean_absolute_error: 11.9776\nEpoch 2/100\n934/934 [==============================] - 0s 53us/sample - loss: 142.0533 - mean_absolute_error: 11.9129 - val_loss: 139.7229 - val_mean_absolute_error: 11.8143\nEpoch 3/100\n934/934 [==============================] - 0s 56us/sample - loss: 137.6583 - mean_absolute_error: 11.7252 - val_loss: 135.1140 - val_mean_absolute_error: 11.6161\nEpoch 4/100\n934/934 [==============================] - 0s 57us/sample - loss: 132.6161 - mean_absolute_error: 11.5062 - val_loss: 129.7819 - val_mean_absolute_error: 11.3830\nEpoch 5/100\n934/934 [==============================] - 0s 56us/sample - loss: 126.8408 - mean_absolute_error: 11.2494 - val_loss: 123.6158 - val_mean_absolute_error: 11.1070\nEpoch 6/100\n934/934 [==============================] - 0s 52us/sample - loss: 120.1807 - mean_absolute_error: 10.9440 - val_loss: 116.5420 - val_mean_absolute_error: 10.7808\nEpoch 7/100\n934/934 [==============================] - 0s 55us/sample - loss: 112.6471 - mean_absolute_error: 10.5876 - val_loss: 108.6039 - val_mean_absolute_error: 10.4012\nEpoch 8/100\n934/934 [==============================] - 0s 52us/sample - loss: 104.2726 - mean_absolute_error: 10.1781 - val_loss: 99.7983 - val_mean_absolute_error: 9.9613\nEpoch 9/100\n934/934 [==============================] - 0s 51us/sample - loss: 94.9200 - mean_absolute_error: 9.6994 - val_loss: 89.9546 - val_mean_absolute_error: 9.4420\nEpoch 10/100\n934/934 [==============================] - 0s 57us/sample - loss: 84.6797 - mean_absolute_error: 9.1454 - val_loss: 79.1850 - val_mean_absolute_error: 8.8345\nEpoch 11/100\n934/934 [==============================] - 0s 51us/sample - loss: 73.4002 - mean_absolute_error: 8.4882 - val_loss: 67.3456 - val_mean_absolute_error: 8.1044\nEpoch 12/100\n934/934 [==============================] - 0s 53us/sample - loss: 61.2369 - mean_absolute_error: 7.7044 - val_loss: 55.0300 - val_mean_absolute_error: 7.2563\nEpoch 13/100\n934/934 [==============================] - 0s 48us/sample - loss: 48.9627 - mean_absolute_error: 6.8074 - val_loss: 43.0914 - val_mean_absolute_error: 6.3080\nEpoch 14/100\n934/934 [==============================] - 0s 51us/sample - loss: 37.5987 - mean_absolute_error: 5.8410 - val_loss: 32.5532 - val_mean_absolute_error: 5.3282\nEpoch 15/100\n934/934 [==============================] - 0s 48us/sample - loss: 28.0247 - mean_absolute_error: 4.8663 - val_loss: 23.8510 - val_mean_absolute_error: 4.3679\nEpoch 16/100\n934/934 [==============================] - 0s 57us/sample - loss: 20.4965 - mean_absolute_error: 3.9779 - val_loss: 17.4156 - val_mean_absolute_error: 3.5419\nEpoch 17/100\n934/934 [==============================] - 0s 49us/sample - loss: 15.0271 - mean_absolute_error: 3.2459 - val_loss: 12.6807 - val_mean_absolute_error: 2.8797\nEpoch 18/100\n934/934 [==============================] - 0s 53us/sample - loss: 11.3294 - mean_absolute_error: 2.6945 - val_loss: 9.4602 - val_mean_absolute_error: 2.4169\nEpoch 19/100\n934/934 [==============================] - 0s 47us/sample - loss: 8.8844 - mean_absolute_error: 2.3085 - val_loss: 7.3637 - val_mean_absolute_error: 2.0974\nEpoch 20/100\n934/934 [==============================] - 0s 50us/sample - loss: 7.3038 - mean_absolute_error: 2.0352 - val_loss: 6.0509 - val_mean_absolute_error: 1.8773\nEpoch 21/100\n934/934 [==============================] - 0s 49us/sample - loss: 6.2389 - mean_absolute_error: 1.8436 - val_loss: 5.1169 - val_mean_absolute_error: 1.7094\nEpoch 22/100\n934/934 [==============================] - 0s 54us/sample - loss: 5.4923 - mean_absolute_error: 1.7058 - val_loss: 4.4587 - val_mean_absolute_error: 1.5967\nEpoch 23/100\n934/934 [==============================] - 0s 48us/sample - loss: 4.9340 - mean_absolute_error: 1.6141 - val_loss: 4.0134 - val_mean_absolute_error: 1.5243\nEpoch 24/100\n934/934 [==============================] - 0s 63us/sample - loss: 4.5257 - mean_absolute_error: 1.5553 - val_loss: 3.6654 - val_mean_absolute_error: 1.4692\nEpoch 25/100\n934/934 [==============================] - 0s 61us/sample - loss: 4.1993 - mean_absolute_error: 1.5110 - val_loss: 3.3957 - val_mean_absolute_error: 1.4226\nEpoch 26/100\n934/934 [==============================] - 0s 59us/sample - loss: 3.9269 - mean_absolute_error: 1.4665 - val_loss: 3.1787 - val_mean_absolute_error: 1.3822\nEpoch 27/100\n934/934 [==============================] - 0s 55us/sample - loss: 3.6622 - mean_absolute_error: 1.4258 - val_loss: 2.9842 - val_mean_absolute_error: 1.3432\nEpoch 28/100\n934/934 [==============================] - 0s 73us/sample - loss: 3.4528 - mean_absolute_error: 1.3905 - val_loss: 2.8153 - val_mean_absolute_error: 1.3079\nEpoch 29/100\n934/934 [==============================] - 0s 65us/sample - loss: 3.2430 - mean_absolute_error: 1.3519 - val_loss: 2.6476 - val_mean_absolute_error: 1.2738\nEpoch 30/100\n934/934 [==============================] - 0s 54us/sample - loss: 3.0451 - mean_absolute_error: 1.3121 - val_loss: 2.4945 - val_mean_absolute_error: 1.2362\nEpoch 31/100\n934/934 [==============================] - 0s 63us/sample - loss: 2.8698 - mean_absolute_error: 1.2739 - val_loss: 2.3445 - val_mean_absolute_error: 1.1980\nEpoch 32/100\n934/934 [==============================] - 0s 52us/sample - loss: 2.7020 - mean_absolute_error: 1.2344 - val_loss: 2.2104 - val_mean_absolute_error: 1.1603\nEpoch 33/100\n934/934 [==============================] - 0s 57us/sample - loss: 2.5434 - mean_absolute_error: 1.1954 - val_loss: 2.0741 - val_mean_absolute_error: 1.1221\nEpoch 34/100\n934/934 [==============================] - 0s 48us/sample - loss: 2.3951 - mean_absolute_error: 1.1579 - val_loss: 1.9545 - val_mean_absolute_error: 1.0872\nEpoch 35/100\n934/934 [==============================] - 0s 53us/sample - loss: 2.2507 - mean_absolute_error: 1.1200 - val_loss: 1.8312 - val_mean_absolute_error: 1.0499\nEpoch 36/100\n934/934 [==============================] - 0s 51us/sample - loss: 2.1080 - mean_absolute_error: 1.0813 - val_loss: 1.7125 - val_mean_absolute_error: 1.0120\nEpoch 37/100\n934/934 [==============================] - 0s 54us/sample - loss: 1.9721 - mean_absolute_error: 1.0417 - val_loss: 1.5943 - val_mean_absolute_error: 0.9729\nEpoch 38/100\n934/934 [==============================] - 0s 47us/sample - loss: 1.8533 - mean_absolute_error: 1.0065 - val_loss: 1.4791 - val_mean_absolute_error: 0.9346\nEpoch 39/100\n934/934 [==============================] - 0s 48us/sample - loss: 1.7326 - mean_absolute_error: 0.9706 - val_loss: 1.3817 - val_mean_absolute_error: 0.8996\nEpoch 40/100\n934/934 [==============================] - 0s 48us/sample - loss: 1.6096 - mean_absolute_error: 0.9362 - val_loss: 1.2725 - val_mean_absolute_error: 0.8638\nEpoch 41/100\n934/934 [==============================] - 0s 56us/sample - loss: 1.5013 - mean_absolute_error: 0.9032 - val_loss: 1.1805 - val_mean_absolute_error: 0.8286\nEpoch 42/100\n934/934 [==============================] - 0s 48us/sample - loss: 1.3948 - mean_absolute_error: 0.8682 - val_loss: 1.0813 - val_mean_absolute_error: 0.7929\nEpoch 43/100\n934/934 [==============================] - 0s 50us/sample - loss: 1.2939 - mean_absolute_error: 0.8321 - val_loss: 0.9993 - val_mean_absolute_error: 0.7615\nEpoch 44/100\n934/934 [==============================] - 0s 52us/sample - loss: 1.2033 - mean_absolute_error: 0.7995 - val_loss: 0.9208 - val_mean_absolute_error: 0.7309\nEpoch 45/100\n934/934 [==============================] - 0s 50us/sample - loss: 1.1206 - mean_absolute_error: 0.7681 - val_loss: 0.8477 - val_mean_absolute_error: 0.6999\nEpoch 46/100\n934/934 [==============================] - 0s 92us/sample - loss: 1.0417 - mean_absolute_error: 0.7377 - val_loss: 0.7832 - val_mean_absolute_error: 0.6704\nEpoch 47/100\n934/934 [==============================] - 0s 64us/sample - loss: 0.9694 - mean_absolute_error: 0.7073 - val_loss: 0.7163 - val_mean_absolute_error: 0.6409\nEpoch 48/100\n934/934 [==============================] - 0s 56us/sample - loss: 0.9028 - mean_absolute_error: 0.6779 - val_loss: 0.6615 - val_mean_absolute_error: 0.6140\nEpoch 49/100\n934/934 [==============================] - 0s 54us/sample - loss: 0.8404 - mean_absolute_error: 0.6532 - val_loss: 0.6115 - val_mean_absolute_error: 0.5884\nEpoch 50/100\n934/934 [==============================] - 0s 68us/sample - loss: 0.7853 - mean_absolute_error: 0.6312 - val_loss: 0.5669 - val_mean_absolute_error: 0.5655\nEpoch 51/100\n934/934 [==============================] - 0s 59us/sample - loss: 0.7286 - mean_absolute_error: 0.6028 - val_loss: 0.5257 - val_mean_absolute_error: 0.5423\nEpoch 52/100\n934/934 [==============================] - 0s 56us/sample - loss: 0.6825 - mean_absolute_error: 0.5822 - val_loss: 0.4879 - val_mean_absolute_error: 0.5220\nEpoch 53/100\n934/934 [==============================] - 0s 59us/sample - loss: 0.6372 - mean_absolute_error: 0.5594 - val_loss: 0.4512 - val_mean_absolute_error: 0.5004\nEpoch 54/100\n934/934 [==============================] - 0s 53us/sample - loss: 0.5974 - mean_absolute_error: 0.5399 - val_loss: 0.4172 - val_mean_absolute_error: 0.4797\nEpoch 55/100\n934/934 [==============================] - 0s 49us/sample - loss: 0.5586 - mean_absolute_error: 0.5227 - val_loss: 0.3907 - val_mean_absolute_error: 0.4628\nEpoch 56/100\n934/934 [==============================] - 0s 53us/sample - loss: 0.5258 - mean_absolute_error: 0.5061 - val_loss: 0.3617 - val_mean_absolute_error: 0.4442\nEpoch 57/100\n934/934 [==============================] - 0s 50us/sample - loss: 0.4957 - mean_absolute_error: 0.4878 - val_loss: 0.3418 - val_mean_absolute_error: 0.4289\nEpoch 58/100\n934/934 [==============================] - 0s 49us/sample - loss: 0.4658 - mean_absolute_error: 0.4724 - val_loss: 0.3188 - val_mean_absolute_error: 0.4131\nEpoch 59/100\n934/934 [==============================] - 0s 54us/sample - loss: 0.4402 - mean_absolute_error: 0.4590 - val_loss: 0.2989 - val_mean_absolute_error: 0.3980\nEpoch 60/100\n934/934 [==============================] - 0s 55us/sample - loss: 0.4177 - mean_absolute_error: 0.4501 - val_loss: 0.2813 - val_mean_absolute_error: 0.3871\nEpoch 61/100\n934/934 [==============================] - 0s 50us/sample - loss: 0.3952 - mean_absolute_error: 0.4352 - val_loss: 0.2652 - val_mean_absolute_error: 0.3726\nEpoch 62/100\n934/934 [==============================] - 0s 51us/sample - loss: 0.3749 - mean_absolute_error: 0.4186 - val_loss: 0.2506 - val_mean_absolute_error: 0.3607\nEpoch 63/100\n934/934 [==============================] - 0s 48us/sample - loss: 0.3584 - mean_absolute_error: 0.4065 - val_loss: 0.2392 - val_mean_absolute_error: 0.3513\nEpoch 64/100\n934/934 [==============================] - 0s 47us/sample - loss: 0.3423 - mean_absolute_error: 0.3991 - val_loss: 0.2273 - val_mean_absolute_error: 0.3422\nEpoch 65/100\n934/934 [==============================] - 0s 53us/sample - loss: 0.3266 - mean_absolute_error: 0.3877 - val_loss: 0.2163 - val_mean_absolute_error: 0.3327\nEpoch 66/100\n934/934 [==============================] - 0s 58us/sample - loss: 0.3136 - mean_absolute_error: 0.3780 - val_loss: 0.2076 - val_mean_absolute_error: 0.3250\nEpoch 67/100\n934/934 [==============================] - 0s 53us/sample - loss: 0.3022 - mean_absolute_error: 0.3720 - val_loss: 0.1966 - val_mean_absolute_error: 0.3177\nEpoch 68/100\n934/934 [==============================] - 0s 50us/sample - loss: 0.2902 - mean_absolute_error: 0.3619 - val_loss: 0.1924 - val_mean_absolute_error: 0.3131\nEpoch 69/100\n934/934 [==============================] - 0s 54us/sample - loss: 0.2810 - mean_absolute_error: 0.3549 - val_loss: 0.1837 - val_mean_absolute_error: 0.3058\nEpoch 70/100\n934/934 [==============================] - 0s 52us/sample - loss: 0.2699 - mean_absolute_error: 0.3481 - val_loss: 0.1787 - val_mean_absolute_error: 0.3020\nEpoch 71/100\n934/934 [==============================] - 0s 49us/sample - loss: 0.2600 - mean_absolute_error: 0.3411 - val_loss: 0.1725 - val_mean_absolute_error: 0.2969\nEpoch 72/100\n934/934 [==============================] - 0s 54us/sample - loss: 0.2510 - mean_absolute_error: 0.3356 - val_loss: 0.1666 - val_mean_absolute_error: 0.2921\nEpoch 73/100\n934/934 [==============================] - 0s 52us/sample - loss: 0.2419 - mean_absolute_error: 0.3276 - val_loss: 0.1604 - val_mean_absolute_error: 0.2863\nEpoch 74/100\n934/934 [==============================] - 0s 52us/sample - loss: 0.2347 - mean_absolute_error: 0.3218 - val_loss: 0.1556 - val_mean_absolute_error: 0.2824\nEpoch 75/100\n934/934 [==============================] - 0s 52us/sample - loss: 0.2259 - mean_absolute_error: 0.3173 - val_loss: 0.1518 - val_mean_absolute_error: 0.2778\nEpoch 76/100\n934/934 [==============================] - 0s 50us/sample - loss: 0.2187 - mean_absolute_error: 0.3114 - val_loss: 0.1469 - val_mean_absolute_error: 0.2731\nEpoch 77/100\n934/934 [==============================] - 0s 54us/sample - loss: 0.2111 - mean_absolute_error: 0.3065 - val_loss: 0.1405 - val_mean_absolute_error: 0.2689\nEpoch 78/100\n934/934 [==============================] - 0s 51us/sample - loss: 0.2035 - mean_absolute_error: 0.3032 - val_loss: 0.1378 - val_mean_absolute_error: 0.2657\nEpoch 79/100\n934/934 [==============================] - 0s 50us/sample - loss: 0.1975 - mean_absolute_error: 0.2996 - val_loss: 0.1330 - val_mean_absolute_error: 0.2615\nEpoch 80/100\n934/934 [==============================] - 0s 56us/sample - loss: 0.1890 - mean_absolute_error: 0.2926 - val_loss: 0.1289 - val_mean_absolute_error: 0.2588\nEpoch 81/100\n934/934 [==============================] - 0s 51us/sample - loss: 0.1821 - mean_absolute_error: 0.2885 - val_loss: 0.1257 - val_mean_absolute_error: 0.2549\nEpoch 82/100\n934/934 [==============================] - 0s 53us/sample - loss: 0.1753 - mean_absolute_error: 0.2840 - val_loss: 0.1210 - val_mean_absolute_error: 0.2516\nEpoch 83/100\n934/934 [==============================] - 0s 49us/sample - loss: 0.1703 - mean_absolute_error: 0.2798 - val_loss: 0.1187 - val_mean_absolute_error: 0.2494\nEpoch 84/100\n934/934 [==============================] - 0s 52us/sample - loss: 0.1647 - mean_absolute_error: 0.2764 - val_loss: 0.1154 - val_mean_absolute_error: 0.2464\nEpoch 85/100\n934/934 [==============================] - 0s 95us/sample - loss: 0.1595 - mean_absolute_error: 0.2710 - val_loss: 0.1122 - val_mean_absolute_error: 0.2446\nEpoch 86/100\n934/934 [==============================] - 0s 49us/sample - loss: 0.1563 - mean_absolute_error: 0.2715 - val_loss: 0.1093 - val_mean_absolute_error: 0.2431\nEpoch 87/100\n934/934 [==============================] - 0s 57us/sample - loss: 0.1522 - mean_absolute_error: 0.2673 - val_loss: 0.1081 - val_mean_absolute_error: 0.2417\nEpoch 88/100\n934/934 [==============================] - 0s 53us/sample - loss: 0.1494 - mean_absolute_error: 0.2668 - val_loss: 0.1066 - val_mean_absolute_error: 0.2401\nEpoch 89/100\n934/934 [==============================] - 0s 52us/sample - loss: 0.1442 - mean_absolute_error: 0.2616 - val_loss: 0.1028 - val_mean_absolute_error: 0.2385\nEpoch 90/100\n934/934 [==============================] - 0s 49us/sample - loss: 0.1415 - mean_absolute_error: 0.2615 - val_loss: 0.1013 - val_mean_absolute_error: 0.2364\nEpoch 91/100\n934/934 [==============================] - 0s 54us/sample - loss: 0.1385 - mean_absolute_error: 0.2572 - val_loss: 0.0983 - val_mean_absolute_error: 0.2334\nEpoch 92/100\n934/934 [==============================] - 0s 53us/sample - loss: 0.1348 - mean_absolute_error: 0.2567 - val_loss: 0.0962 - val_mean_absolute_error: 0.2326\nEpoch 93/100\n934/934 [==============================] - 0s 51us/sample - loss: 0.1331 - mean_absolute_error: 0.2563 - val_loss: 0.0994 - val_mean_absolute_error: 0.2364\nEpoch 94/100\n934/934 [==============================] - 0s 50us/sample - loss: 0.1309 - mean_absolute_error: 0.2554 - val_loss: 0.0954 - val_mean_absolute_error: 0.2331\nEpoch 95/100\n934/934 [==============================] - 0s 52us/sample - loss: 0.1268 - mean_absolute_error: 0.2502 - val_loss: 0.0913 - val_mean_absolute_error: 0.2288\nEpoch 96/100\n934/934 [==============================] - 0s 64us/sample - loss: 0.1241 - mean_absolute_error: 0.2477 - val_loss: 0.0900 - val_mean_absolute_error: 0.2281\nEpoch 97/100\n934/934 [==============================] - 0s 58us/sample - loss: 0.1227 - mean_absolute_error: 0.2487 - val_loss: 0.0886 - val_mean_absolute_error: 0.2268\nEpoch 98/100\n934/934 [==============================] - 0s 57us/sample - loss: 0.1191 - mean_absolute_error: 0.2443 - val_loss: 0.0856 - val_mean_absolute_error: 0.2248\nEpoch 99/100\n934/934 [==============================] - 0s 59us/sample - loss: 0.1172 - mean_absolute_error: 0.2447 - val_loss: 0.0846 - val_mean_absolute_error: 0.2232\nEpoch 100/100\n934/934 [==============================] - 0s 52us/sample - loss: 0.1145 - mean_absolute_error: 0.2396 - val_loss: 0.0813 - val_mean_absolute_error: 0.2207\n"
    }
   ],
   "source": [
    "# モデル構築\n",
    "K.clear_session()\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(10, activation = tf.nn.relu, input_shape=(X_train.shape[1],)))\n",
    "model.add(tf.keras.layers.Dense(3, activation = tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "model.summary()\n",
    "\n",
    "# コンパイル\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=tf.train.AdamOptimizer(learning_rate=0.001),\n",
    "              metrics=['mae'])\n",
    "\n",
    "# 学習\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=32,\n",
    "                    epochs=100,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "y_pred_log [11.675801 11.450873 11.439458 12.391041 11.86787 ]\ny_test [12.20918779 11.79810441 11.60823564 12.16525065 11.38509209]\nmae 0.24325394728377292\n"
    }
   ],
   "source": [
    "y_pred_log = model.predict(X_test)\n",
    "print(\"y_pred_log\", y_pred_log.flatten()[:5])\n",
    "print(\"y_test\", y_test.flatten()[:5])\n",
    "print(\"mae\", mean_absolute_error(y_test, y_pred_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 375.2875 248.518125\" width=\"375.2875pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.518125 \r\nL 375.2875 248.518125 \r\nL 375.2875 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 33.2875 224.64 \r\nL 368.0875 224.64 \r\nL 368.0875 7.2 \r\nL 33.2875 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m51cdc5edec\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"48.505682\" xlink:href=\"#m51cdc5edec\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n      </defs>\r\n      <g transform=\"translate(45.324432 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"109.993285\" xlink:href=\"#m51cdc5edec\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 20 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      </defs>\r\n      <g transform=\"translate(103.630785 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"171.480888\" xlink:href=\"#m51cdc5edec\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 40 -->\r\n      <defs>\r\n       <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n      </defs>\r\n      <g transform=\"translate(165.118388 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"232.968492\" xlink:href=\"#m51cdc5edec\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 60 -->\r\n      <defs>\r\n       <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n      </defs>\r\n      <g transform=\"translate(226.605992 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-54\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"294.456095\" xlink:href=\"#m51cdc5edec\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 80 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n      </defs>\r\n      <g transform=\"translate(288.093595 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-56\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"355.943698\" xlink:href=\"#m51cdc5edec\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 100 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g transform=\"translate(346.399948 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m539ead05da\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m539ead05da\" y=\"214.866409\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(19.925 218.665628)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m539ead05da\" y=\"187.807636\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(13.5625 191.606855)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m539ead05da\" y=\"160.748862\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 40 -->\r\n      <g transform=\"translate(13.5625 164.548081)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m539ead05da\" y=\"133.690089\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 60 -->\r\n      <g transform=\"translate(13.5625 137.489307)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-54\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m539ead05da\" y=\"106.631315\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 80 -->\r\n      <g transform=\"translate(13.5625 110.430534)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-56\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m539ead05da\" y=\"79.572541\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 100 -->\r\n      <g transform=\"translate(7.2 83.37176)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m539ead05da\" y=\"52.513768\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 120 -->\r\n      <g transform=\"translate(7.2 56.312987)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_8\">\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m539ead05da\" y=\"25.454994\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- 140 -->\r\n      <g transform=\"translate(7.2 29.254213)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_15\">\r\n    <path clip-path=\"url(#pb57bd49115)\" d=\"M 48.505682 17.083636 \r\nL 51.580062 22.676962 \r\nL 54.654442 28.623148 \r\nL 57.728822 35.444957 \r\nL 60.803202 43.258558 \r\nL 63.877583 52.269283 \r\nL 66.951963 62.461757 \r\nL 70.026343 73.791979 \r\nL 73.100723 86.445403 \r\nL 76.175103 100.300015 \r\nL 79.249483 115.560439 \r\nL 82.323864 132.016699 \r\nL 85.398244 148.622914 \r\nL 88.472624 163.997701 \r\nL 91.547004 176.950645 \r\nL 94.621384 187.135902 \r\nL 97.695764 194.535696 \r\nL 100.770145 199.53842 \r\nL 103.844525 202.84642 \r\nL 106.918905 204.984762 \r\nL 109.993285 206.42555 \r\nL 113.067665 207.435704 \r\nL 116.142045 208.191067 \r\nL 119.216426 208.743361 \r\nL 122.290806 209.185076 \r\nL 125.365186 209.553545 \r\nL 128.439566 209.911666 \r\nL 131.513946 210.194962 \r\nL 134.588326 210.478815 \r\nL 137.662707 210.746525 \r\nL 140.737087 210.983681 \r\nL 143.811467 211.21079 \r\nL 146.885847 211.425366 \r\nL 149.960227 211.625927 \r\nL 153.034607 211.821349 \r\nL 156.108988 212.014364 \r\nL 159.183368 212.198246 \r\nL 162.257748 212.359069 \r\nL 165.332128 212.522292 \r\nL 168.406508 212.68867 \r\nL 171.480888 212.835234 \r\nL 174.555269 212.979316 \r\nL 177.629649 213.115802 \r\nL 180.704029 213.238391 \r\nL 183.778409 213.350321 \r\nL 186.852789 213.457081 \r\nL 189.927169 213.554838 \r\nL 193.00155 213.644928 \r\nL 196.07593 213.729362 \r\nL 199.15031 213.803884 \r\nL 202.22469 213.880641 \r\nL 205.29907 213.943065 \r\nL 208.37345 214.004321 \r\nL 211.447831 214.058217 \r\nL 214.522211 214.110646 \r\nL 217.596591 214.154976 \r\nL 220.670971 214.195732 \r\nL 223.745351 214.236227 \r\nL 226.819731 214.270876 \r\nL 229.894112 214.301249 \r\nL 232.968492 214.331696 \r\nL 236.042872 214.35919 \r\nL 239.117252 214.381556 \r\nL 242.191632 214.403365 \r\nL 245.266012 214.424569 \r\nL 248.340393 214.442146 \r\nL 251.414773 214.457564 \r\nL 254.489153 214.473759 \r\nL 257.563533 214.486185 \r\nL 260.637913 214.501205 \r\nL 263.712293 214.514595 \r\nL 266.786674 214.526859 \r\nL 269.861054 214.53908 \r\nL 272.935434 214.548834 \r\nL 276.009814 214.56076 \r\nL 279.084194 214.57047 \r\nL 282.158574 214.580766 \r\nL 285.232955 214.591063 \r\nL 288.307335 214.599246 \r\nL 291.381715 214.610645 \r\nL 294.456095 214.620078 \r\nL 297.530475 214.629263 \r\nL 300.604855 214.635993 \r\nL 303.679236 214.643638 \r\nL 306.753616 214.650677 \r\nL 309.827996 214.655007 \r\nL 312.902376 214.660534 \r\nL 315.976756 214.664264 \r\nL 319.051136 214.671266 \r\nL 322.125517 214.674925 \r\nL 325.199897 214.678972 \r\nL 328.274277 214.683996 \r\nL 331.348657 214.686337 \r\nL 334.423037 214.68931 \r\nL 337.497417 214.694851 \r\nL 340.571798 214.698555 \r\nL 343.646178 214.700457 \r\nL 346.720558 214.705218 \r\nL 349.794938 214.707834 \r\nL 352.869318 214.7115 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_16\">\r\n    <path clip-path=\"url(#pb57bd49115)\" d=\"M 48.505682 20.611431 \r\nL 51.580062 25.829867 \r\nL 54.654442 32.065494 \r\nL 57.728822 39.2795 \r\nL 60.803202 47.62179 \r\nL 63.877583 57.192174 \r\nL 66.951963 67.931938 \r\nL 70.026343 79.845423 \r\nL 73.100723 93.163291 \r\nL 76.175103 107.733916 \r\nL 79.249483 123.751933 \r\nL 82.323864 140.414197 \r\nL 85.398244 156.566349 \r\nL 88.472624 170.823903 \r\nL 91.547004 182.597444 \r\nL 94.621384 191.304106 \r\nL 97.695764 197.71022 \r\nL 100.770145 202.067273 \r\nL 103.844525 204.903814 \r\nL 106.918905 206.679909 \r\nL 109.993285 207.943607 \r\nL 113.067665 208.834104 \r\nL 116.142045 209.436513 \r\nL 119.216426 209.907377 \r\nL 122.290806 210.272271 \r\nL 125.365186 210.565873 \r\nL 128.439566 210.828922 \r\nL 131.513946 211.057525 \r\nL 134.588326 211.28432 \r\nL 137.662707 211.491548 \r\nL 140.737087 211.694501 \r\nL 143.811467 211.875924 \r\nL 146.885847 212.060338 \r\nL 149.960227 212.222115 \r\nL 153.034607 212.388951 \r\nL 156.108988 212.549452 \r\nL 159.183368 212.70946 \r\nL 162.257748 212.865311 \r\nL 165.332128 212.99709 \r\nL 168.406508 213.144857 \r\nL 171.480888 213.26931 \r\nL 174.555269 213.403532 \r\nL 177.629649 213.51444 \r\nL 180.704029 213.620669 \r\nL 183.778409 213.719549 \r\nL 186.852789 213.806725 \r\nL 189.927169 213.897256 \r\nL 193.00155 213.97146 \r\nL 196.07593 214.039071 \r\nL 199.15031 214.099438 \r\nL 202.22469 214.155184 \r\nL 205.29907 214.206285 \r\nL 208.37345 214.255911 \r\nL 211.447831 214.301897 \r\nL 214.522211 214.337878 \r\nL 217.596591 214.377054 \r\nL 220.670971 214.403957 \r\nL 223.745351 214.43514 \r\nL 226.819731 214.461957 \r\nL 229.894112 214.485796 \r\nL 232.968492 214.507676 \r\nL 236.042872 214.527344 \r\nL 239.117252 214.542772 \r\nL 242.191632 214.558851 \r\nL 245.266012 214.573808 \r\nL 248.340393 214.585581 \r\nL 251.414773 214.600434 \r\nL 254.489153 214.60605 \r\nL 257.563533 214.617873 \r\nL 260.637913 214.624676 \r\nL 263.712293 214.633059 \r\nL 266.786674 214.641016 \r\nL 269.861054 214.649354 \r\nL 272.935434 214.655854 \r\nL 276.009814 214.661101 \r\nL 279.084194 214.667711 \r\nL 282.158574 214.676374 \r\nL 285.232955 214.679931 \r\nL 288.307335 214.686427 \r\nL 291.381715 214.69208 \r\nL 294.456095 214.696373 \r\nL 297.530475 214.702648 \r\nL 300.604855 214.705808 \r\nL 303.679236 214.710245 \r\nL 306.753616 214.714636 \r\nL 309.827996 214.718512 \r\nL 312.902376 214.720152 \r\nL 315.976756 214.722229 \r\nL 319.051136 214.727349 \r\nL 322.125517 214.729292 \r\nL 325.199897 214.733451 \r\nL 328.274277 214.73628 \r\nL 331.348657 214.731983 \r\nL 334.423037 214.737365 \r\nL 337.497417 214.742819 \r\nL 340.571798 214.744617 \r\nL 343.646178 214.746598 \r\nL 346.720558 214.750612 \r\nL 349.794938 214.752012 \r\nL 352.869318 214.756364 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 33.2875 224.64 \r\nL 33.2875 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 368.0875 224.64 \r\nL 368.0875 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 33.2875 224.64 \r\nL 368.0875 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 33.2875 7.2 \r\nL 368.0875 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"legend_1\">\r\n    <g id=\"patch_7\">\r\n     <path d=\"M 289.946875 44.834375 \r\nL 361.0875 44.834375 \r\nQ 363.0875 44.834375 363.0875 42.834375 \r\nL 363.0875 14.2 \r\nQ 363.0875 12.2 361.0875 12.2 \r\nL 289.946875 12.2 \r\nQ 287.946875 12.2 287.946875 14.2 \r\nL 287.946875 42.834375 \r\nQ 287.946875 44.834375 289.946875 44.834375 \r\nz\r\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\r\n    </g>\r\n    <g id=\"line2d_17\">\r\n     <path d=\"M 291.946875 20.298437 \r\nL 311.946875 20.298437 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_18\"/>\r\n    <g id=\"text_15\">\r\n     <!-- loss -->\r\n     <defs>\r\n      <path d=\"M 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\n\" id=\"DejaVuSans-108\"/>\r\n      <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n      <path d=\"M 44.28125 53.078125 \r\nL 44.28125 44.578125 \r\nQ 40.484375 46.53125 36.375 47.5 \r\nQ 32.28125 48.484375 27.875 48.484375 \r\nQ 21.1875 48.484375 17.84375 46.4375 \r\nQ 14.5 44.390625 14.5 40.28125 \r\nQ 14.5 37.15625 16.890625 35.375 \r\nQ 19.28125 33.59375 26.515625 31.984375 \r\nL 29.59375 31.296875 \r\nQ 39.15625 29.25 43.1875 25.515625 \r\nQ 47.21875 21.78125 47.21875 15.09375 \r\nQ 47.21875 7.46875 41.1875 3.015625 \r\nQ 35.15625 -1.421875 24.609375 -1.421875 \r\nQ 20.21875 -1.421875 15.453125 -0.5625 \r\nQ 10.6875 0.296875 5.421875 2 \r\nL 5.421875 11.28125 \r\nQ 10.40625 8.6875 15.234375 7.390625 \r\nQ 20.0625 6.109375 24.8125 6.109375 \r\nQ 31.15625 6.109375 34.5625 8.28125 \r\nQ 37.984375 10.453125 37.984375 14.40625 \r\nQ 37.984375 18.0625 35.515625 20.015625 \r\nQ 33.0625 21.96875 24.703125 23.78125 \r\nL 21.578125 24.515625 \r\nQ 13.234375 26.265625 9.515625 29.90625 \r\nQ 5.8125 33.546875 5.8125 39.890625 \r\nQ 5.8125 47.609375 11.28125 51.796875 \r\nQ 16.75 56 26.8125 56 \r\nQ 31.78125 56 36.171875 55.265625 \r\nQ 40.578125 54.546875 44.28125 53.078125 \r\nz\r\n\" id=\"DejaVuSans-115\"/>\r\n     </defs>\r\n     <g transform=\"translate(319.946875 23.798437)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"line2d_19\">\r\n     <path d=\"M 291.946875 34.976562 \r\nL 311.946875 34.976562 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_20\"/>\r\n    <g id=\"text_16\">\r\n     <!-- val_loss -->\r\n     <defs>\r\n      <path d=\"M 2.984375 54.6875 \r\nL 12.5 54.6875 \r\nL 29.59375 8.796875 \r\nL 46.6875 54.6875 \r\nL 56.203125 54.6875 \r\nL 35.6875 0 \r\nL 23.484375 0 \r\nz\r\n\" id=\"DejaVuSans-118\"/>\r\n      <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n      <path d=\"M 50.984375 -16.609375 \r\nL 50.984375 -23.578125 \r\nL -0.984375 -23.578125 \r\nL -0.984375 -16.609375 \r\nz\r\n\" id=\"DejaVuSans-95\"/>\r\n     </defs>\r\n     <g transform=\"translate(319.946875 38.476562)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-118\"/>\r\n      <use x=\"59.179688\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"120.458984\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"148.242188\" xlink:href=\"#DejaVuSans-95\"/>\r\n      <use x=\"198.242188\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"226.025391\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"287.207031\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"339.306641\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"pb57bd49115\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"33.2875\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "plt.plot(range(len(history.history['loss'])), history.history['loss'], label='loss')\n",
    "plt.plot(range(len(history.history['val_loss'])), history.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題6】\n",
    "#### MNISTをKerasで学習\n",
    "TensorFlowによるMNISTデータセットによる画像の多値分類をKerasに書き換えてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(48000, 28, 28, 1)\n(12000, 28, 28, 1)\n(10000, 28, 28, 1)\n"
    }
   ],
   "source": [
    "# 読み込み\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.astype(np.float)[:, :, :, np.newaxis]\n",
    "X_test = X_test.astype(np.float)[:, :, :, np.newaxis]\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(48000, 10)\n(12000, 10)\n(10000, 10)\n"
    }
   ],
   "source": [
    "y_train = np.identity(10)[y_train]\n",
    "y_val = np.identity(10)[y_val]\n",
    "y_test = np.identity(10)[y_test]\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d (Conv2D)              (None, 26, 26, 32)        320       \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 24, 24, 16)        4624      \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 12, 12, 16)        0         \n_________________________________________________________________\nflatten (Flatten)            (None, 2304)              0         \n_________________________________________________________________\ndense (Dense)                (None, 10)                23050     \n=================================================================\nTotal params: 27,994\nTrainable params: 27,994\nNon-trainable params: 0\n_________________________________________________________________\nTrain on 48000 samples, validate on 12000 samples\nEpoch 1/5\n48000/48000 [==============================] - 46s 948us/sample - loss: 0.1538 - acc: 0.9532 - val_loss: 0.0653 - val_acc: 0.9803\nEpoch 2/5\n48000/48000 [==============================] - 45s 939us/sample - loss: 0.0590 - acc: 0.9827 - val_loss: 0.0660 - val_acc: 0.9814\nEpoch 3/5\n48000/48000 [==============================] - 45s 934us/sample - loss: 0.0455 - acc: 0.9850 - val_loss: 0.0563 - val_acc: 0.9842\nEpoch 4/5\n48000/48000 [==============================] - 46s 964us/sample - loss: 0.0358 - acc: 0.9890 - val_loss: 0.0559 - val_acc: 0.9844\nEpoch 5/5\n48000/48000 [==============================] - 45s 932us/sample - loss: 0.0320 - acc: 0.9896 - val_loss: 0.0668 - val_acc: 0.9817\n"
    }
   ],
   "source": [
    "# モデル構築\n",
    "import tensorflow.keras.layers as layers\n",
    "K.clear_session()\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Conv2D(filters=32, kernel_size=(3, 3), activation = tf.nn.relu, input_shape=X_train.shape[1:]))\n",
    "model.add(layers.Conv2D(filters=16, kernel_size=(3, 3), activation = tf.nn.relu))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(10, activation = tf.nn.softmax))\n",
    "model.summary()\n",
    "\n",
    "# コンパイル\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=tf.train.AdamOptimizer(learning_rate=0.01),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 学習\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=5,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "y_pred_proba [[1.2916088e-09 6.6264597e-08 7.4886488e-09 2.9229781e-07 5.8498592e-12\n  4.2715078e-11 1.9714833e-16 9.9999952e-01 1.2345558e-09 1.7191481e-07]\n [1.4542872e-08 4.3511964e-06 9.9999297e-01 1.6282771e-06 3.3174841e-10\n  9.4051997e-12 1.0032347e-06 1.6101337e-11 2.3446418e-09 2.5198368e-09]\n [9.7111055e-08 9.9992442e-01 5.5533076e-07 3.3952252e-09 3.0839205e-05\n  4.1466046e-07 1.4558340e-07 6.1216347e-06 3.7472102e-05 4.6998277e-08]\n [1.0000000e+00 2.9455544e-12 1.8291066e-09 1.6199894e-12 6.2520336e-11\n  3.0815041e-11 4.8991303e-08 2.8426128e-11 4.4245159e-09 1.7011955e-09]\n [2.2802447e-09 4.4857021e-10 2.7575282e-09 6.8324468e-10 9.9990833e-01\n  9.4161146e-10 1.4230159e-09 6.0260206e-08 3.3191637e-07 9.1159360e-05]]\ny_pred [7 2 1 ... 4 5 6]\ny_test [7 2 1 ... 4 5 6]\ntest_acc 0.9814\n"
    }
   ],
   "source": [
    "y_pred_proba = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "print(\"y_pred_proba\", y_pred_proba[:5])\n",
    "print(\"y_pred\", y_pred)\n",
    "print(\"y_test\", np.argmax(y_test, axis=1))\n",
    "print(\"test_acc\", accuracy_score(np.argmax(y_test, axis=1), y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題7】\n",
    "#### （アドバンス課題）PyTorchへの書き換え\n",
    "4種類の問題をPyTorchに書き換えてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras: (N, H, W, C)\n",
    "\n",
    "PyTorch: (N, C, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "      Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm  \\\n0      1            5.1           3.5            1.4           0.2   \n1      2            4.9           3.0            1.4           0.2   \n2      3            4.7           3.2            1.3           0.2   \n3      4            4.6           3.1            1.5           0.2   \n4      5            5.0           3.6            1.4           0.2   \n..   ...            ...           ...            ...           ...   \n145  146            6.7           3.0            5.2           2.3   \n146  147            6.3           2.5            5.0           1.9   \n147  148            6.5           3.0            5.2           2.0   \n148  149            6.2           3.4            5.4           2.3   \n149  150            5.9           3.0            5.1           1.8   \n\n            Species  \n0       Iris-setosa  \n1       Iris-setosa  \n2       Iris-setosa  \n3       Iris-setosa  \n4       Iris-setosa  \n..              ...  \n145  Iris-virginica  \n146  Iris-virginica  \n147  Iris-virginica  \n148  Iris-virginica  \n149  Iris-virginica  \n\n[150 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>SepalLengthCm</th>\n      <th>SepalWidthCm</th>\n      <th>PetalLengthCm</th>\n      <th>PetalWidthCm</th>\n      <th>Species</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>5.1</td>\n      <td>3.5</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>4.9</td>\n      <td>3.0</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>4.7</td>\n      <td>3.2</td>\n      <td>1.3</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>4.6</td>\n      <td>3.1</td>\n      <td>1.5</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>5.0</td>\n      <td>3.6</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>146</td>\n      <td>6.7</td>\n      <td>3.0</td>\n      <td>5.2</td>\n      <td>2.3</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>146</th>\n      <td>147</td>\n      <td>6.3</td>\n      <td>2.5</td>\n      <td>5.0</td>\n      <td>1.9</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>148</td>\n      <td>6.5</td>\n      <td>3.0</td>\n      <td>5.2</td>\n      <td>2.0</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>148</th>\n      <td>149</td>\n      <td>6.2</td>\n      <td>3.4</td>\n      <td>5.4</td>\n      <td>2.3</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>149</th>\n      <td>150</td>\n      <td>5.9</td>\n      <td>3.0</td>\n      <td>5.1</td>\n      <td>1.8</td>\n      <td>Iris-virginica</td>\n    </tr>\n  </tbody>\n</table>\n<p>150 rows × 6 columns</p>\n</div>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "(100, 4)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "(100, 1)"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(64, 4) (64, 1)\n(16, 4) (16, 1)\n(20, 4) (20, 1)\n"
    }
   ],
   "source": [
    "# 【問題3】Iris（2値分類）\n",
    "## 読み込み\n",
    "df_iris = pd.read_csv('Iris.csv')\n",
    "display(df_iris)\n",
    "X = np.array(df_iris[(df_iris['Species'] == 'Iris-versicolor')|(df_iris['Species'] == 'Iris-virginica')].iloc[:, 1:5])\n",
    "y = np.array(df_iris[(df_iris['Species'] == 'Iris-versicolor')|(df_iris['Species'] == 'Iris-virginica')].iloc[:, 5])\n",
    "y[y=='Iris-versicolor'] = 0\n",
    "y[y=='Iris-virginica'] = 1\n",
    "y = y.astype(np.int)[:, np.newaxis]\n",
    "display(X.shape)\n",
    "display(y.shape)\n",
    "\n",
    "## 分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "## 標準化\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "NNfor2(\n  (layer_1): Linear(in_features=4, out_features=10, bias=True)\n  (layer_2): Linear(in_features=10, out_features=5, bias=True)\n  (layer_output): Linear(in_features=5, out_features=1, bias=True)\n)\n"
    }
   ],
   "source": [
    "# PyTorchでモデル構築\n",
    "n_features = 4\n",
    "n_nodes_1 = 10\n",
    "activation_1 = nn.ReLU()\n",
    "n_nodes_2 = 5\n",
    "activation_2 = nn.ReLU()\n",
    "n_output = 1\n",
    "activation_output = nn.Sigmoid()\n",
    "\n",
    "## ネットワーク\n",
    "class NNfor2(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(NNfor2, self).__init__()\n",
    "        self.layer_1 = nn.Linear(\n",
    "            n_features,\n",
    "            n_nodes_1)\n",
    "        self.layer_2 = nn.Linear(\n",
    "            n_nodes_1,\n",
    "            n_nodes_2)\n",
    "        self.layer_output = nn.Linear(\n",
    "            n_nodes_2,\n",
    "            n_output)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = activation_1(self.layer_1(X))\n",
    "        X = activation_2(self.layer_2(X))\n",
    "        X = activation_output(self.layer_output(X))\n",
    "\n",
    "        return X\n",
    "\n",
    "## インスタンス作成\n",
    "model = NNfor2()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ミニバッチ\n",
    "batch_size = 1\n",
    "\n",
    "## numpy配列をテンソルに変換\n",
    "t_X_train = torch.from_numpy(X_train).float()\n",
    "t_y_train = torch.from_numpy(y_train).float()\n",
    "t_X_valid = torch.from_numpy(X_val).float()\n",
    "t_y_valid = torch.from_numpy(y_val).float()\n",
    "\n",
    "## 特徴量とラベルをデータセットにまとめる\n",
    "dataset_train = TensorDataset(t_X_train, t_y_train)\n",
    "dataset_valid = TensorDataset(t_X_valid, t_y_valid)\n",
    "\n",
    "# データローダを作成\n",
    "loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "loader_valid = DataLoader(dataset_valid, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "## オプティマイザ\n",
    "lr = 0.003\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=lr\n",
    ")\n",
    "\n",
    "## 損失関数\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 学習(train)\n",
    "def train_step(X_train, y_train):\n",
    "    ## 訓練モード    \n",
    "    model.train()\n",
    "\n",
    "    ## 訓練データに対して予測\n",
    "    y_train_pred = model(X_train)\n",
    "\n",
    "    ## 勾配初期化\n",
    "    optimizer.zero_grad()\n",
    "    ## 損失の取得\n",
    "    loss = criterion(y_train_pred, y_train)\n",
    "    ## 勾配を計算\n",
    "    loss.backward()\n",
    "\n",
    "    ## パラメータ更新\n",
    "    optimizer.step()\n",
    "\n",
    "    ## 正解率の確認\n",
    "    with torch.no_grad():\n",
    "        y_train_pred_binary = torch.where(y_train_pred>0.5, torch.ones(1), torch.zeros(1))\n",
    "        acc = (y_train_pred_binary == y_train).sum()\n",
    "\n",
    "    return (loss.item(), acc.item())\n",
    "\n",
    "## 評価(validation)\n",
    "def valid_step(X_val, y_val):\n",
    "    ## 評価モード\n",
    "    model.eval()\n",
    "\n",
    "    ## 予測\n",
    "    y_val_pred = model(X_val)\n",
    "\n",
    "    ## 損失計算\n",
    "    loss = criterion(y_val_pred, y_val)\n",
    "\n",
    "    ## 正解率計算\n",
    "    with torch.no_grad():\n",
    "        y_val_pred_binary = torch.where(y_val_pred>0.5, torch.ones(1), torch.zeros(1))\n",
    "        acc = (y_val_pred_binary == y_val).sum()\n",
    "\n",
    "    return (loss.item(), acc.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch1/10: [loss:0.6132, acc:0.7500, val_loss:0.2326, val_acc:1.0000]\nepoch2/10: [loss:0.3086, acc:0.9062, val_loss:0.1296, val_acc:1.0000]\nepoch3/10: [loss:0.2130, acc:0.9219, val_loss:0.0788, val_acc:1.0000]\nepoch4/10: [loss:0.1675, acc:0.9219, val_loss:0.0566, val_acc:1.0000]\nepoch5/10: [loss:0.1371, acc:0.9531, val_loss:0.0420, val_acc:1.0000]\nepoch6/10: [loss:0.1167, acc:0.9688, val_loss:0.0352, val_acc:1.0000]\nepoch7/10: [loss:0.0994, acc:0.9688, val_loss:0.0294, val_acc:1.0000]\nepoch8/10: [loss:0.0867, acc:0.9531, val_loss:0.0261, val_acc:1.0000]\nepoch9/10: [loss:0.0793, acc:0.9688, val_loss:0.0251, val_acc:1.0000]\nepoch10/10: [loss:0.0710, acc:0.9688, val_loss:0.0210, val_acc:1.0000]\nDone!\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "OrderedDict([('layer_1.weight',\n              tensor([[ 0.5623,  0.6748, -0.6573,  0.7426],\n                      [ 0.1654, -0.5562,  0.5469, -0.7611],\n                      [-0.1390,  0.7942,  0.3495,  1.3400],\n                      [ 0.5125, -0.2368, -1.6103, -0.6610],\n                      [ 0.3432, -0.3872,  0.9416,  1.1465],\n                      [-0.3240, -1.3878, -0.5829,  1.1936],\n                      [-0.3032,  0.1143, -1.0057, -0.1303],\n                      [-0.4265, -0.4313, -0.0837,  1.3517],\n                      [ 0.1817, -0.0308,  0.2598, -0.2765],\n                      [ 0.6418, -0.0529,  0.6314, -0.3959]])),\n             ('layer_1.bias',\n              tensor([ 0.0775,  0.0639,  0.0039,  0.2467,  0.3989,  0.2067,  0.1645,  0.1696,\n                      -0.0546, -0.0508])),\n             ('layer_2.weight',\n              tensor([[-0.2276,  0.6432,  0.5755, -0.0767,  0.1582, -0.1999, -0.7034,  0.5008,\n                       -0.8101,  0.2060],\n                      [ 0.2911,  0.8101, -0.9744,  0.9040, -0.6072, -0.2418,  0.6695,  0.4042,\n                       -0.8839, -0.6983],\n                      [ 0.6637,  1.1315,  0.3298,  0.4343,  0.1736,  0.5183, -0.0352,  0.5671,\n                       -0.3560,  0.4117],\n                      [-0.0984,  0.3701,  0.2390, -0.2551,  1.2620,  0.4003, -0.0999,  0.3166,\n                        0.6490, -0.2237],\n                      [ 0.7740,  0.2067,  0.3721,  1.0154, -0.4764, -0.1589,  0.5978, -0.3937,\n                       -0.1488,  0.4305]])),\n             ('layer_2.bias',\n              tensor([0.0945, 0.0231, 0.0580, 0.1014, 0.0215])),\n             ('layer_output.weight',\n              tensor([[ 0.7277, -1.3901,  0.7972,  0.5882, -1.1241]])),\n             ('layer_output.bias', tensor([0.0583]))])"
     },
     "metadata": {}
    }
   ],
   "source": [
    "## パラメータ初期化関数\n",
    "def initializer(layer):\n",
    "    if type(layer) == nn.Linear:\n",
    "        nn.init.kaiming_normal_(layer.weight)\n",
    "        layer.bias.data.fill_(0.0)\n",
    "\n",
    "## パラメータ初期化する\n",
    "model.apply(initializer)\n",
    "\n",
    "n_epoch = 10\n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "# 1エポック内1データあたりの損失・正解率の平均\n",
    "avg_loss = 0.0\n",
    "avg_acc = 0.0\n",
    "avg_val_loss = 0.0\n",
    "avg_val_acc = 0.0\n",
    "\n",
    "## 学習する\n",
    "for epoch in range(n_epoch):\n",
    "    ## epoch内のloss, accの保存\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    total_val_loss = 0.0\n",
    "    total_val_acc = 0.0\n",
    "    ## 累計データ数の保存\n",
    "    total_train = 0\n",
    "    total_val = 0\n",
    "\n",
    "    ## 訓練\n",
    "    for X_train, y_train in loader_train:\n",
    "       loss, acc = train_step(X_train, y_train)\n",
    "       total_loss += loss\n",
    "       total_acc += acc\n",
    "       total_train += len(y_train)\n",
    "\n",
    "    ## 評価\n",
    "    for X_val, y_val in loader_valid:\n",
    "        val_loss, val_acc = valid_step(X_val, y_val)\n",
    "        total_val_loss += val_loss\n",
    "        total_val_acc += val_acc\n",
    "        total_val += len(y_val)\n",
    "\n",
    "    # 平均計算\n",
    "    avg_loss = total_loss/total_train\n",
    "    avg_val_loss = total_val_loss/total_val\n",
    "    avg_acc = total_acc/total_train\n",
    "    avg_val_acc = total_val_acc/total_val\n",
    "\n",
    "    print(f'epoch{epoch+1}/{n_epoch}: [loss:{avg_loss:.4f}, acc:{avg_acc:.4f}, val_loss:{avg_val_loss:.4f}, val_acc:{avg_val_acc:.4f}]')\n",
    "\n",
    "    # リストに追加\n",
    "    train_history.append(avg_loss)\n",
    "    val_history.append(avg_val_loss)\n",
    "\n",
    "print('Done!')\n",
    "display(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "y_pred_proba tensor([[8.0147e-02],\n        [9.9931e-01],\n        [1.4008e-01],\n        [9.9946e-01],\n        [9.7083e-01],\n        [9.9933e-01],\n        [3.4314e-02],\n        [9.4256e-01],\n        [9.9908e-01],\n        [9.9940e-01],\n        [9.6409e-01],\n        [9.8199e-01],\n        [9.9781e-01],\n        [3.9398e-02],\n        [2.2401e-05],\n        [2.6243e-04],\n        [5.9972e-01],\n        [2.0875e-03],\n        [8.7462e-01],\n        [1.4746e-02]], grad_fn=<SigmoidBackward>)\ny_pred [0 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 1 0 1 0]\ny_test [0 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 0 0 0 0]\n"
    }
   ],
   "source": [
    "# 予測\n",
    "## Tensor変換\n",
    "t_X_test = torch.from_numpy(X_test).float()\n",
    "t_y_test = torch.from_numpy(y_test).float()\n",
    "y_pred_proba = model(t_X_test)\n",
    "y_pred = torch.where(y_pred_proba>0.5, torch.ones(1), torch.zeros(1))\n",
    "print(\"y_pred_proba\", y_pred_proba)\n",
    "print(\"y_pred\", y_pred.flatten().int().detach().numpy())\n",
    "print(\"y_test\", y_test.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 372.103125 248.518125\" width=\"372.103125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.518125 \r\nL 372.103125 248.518125 \r\nL 372.103125 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 30.103125 224.64 \r\nL 364.903125 224.64 \r\nL 364.903125 7.2 \r\nL 30.103125 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m88e384fdbc\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"45.321307\" xlink:href=\"#m88e384fdbc\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n      </defs>\r\n      <g transform=\"translate(42.140057 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"112.95767\" xlink:href=\"#m88e384fdbc\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 2 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      </defs>\r\n      <g transform=\"translate(109.77642 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"180.594034\" xlink:href=\"#m88e384fdbc\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 4 -->\r\n      <defs>\r\n       <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n      </defs>\r\n      <g transform=\"translate(177.412784 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"248.230398\" xlink:href=\"#m88e384fdbc\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 6 -->\r\n      <defs>\r\n       <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n      </defs>\r\n      <g transform=\"translate(245.049148 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"315.866761\" xlink:href=\"#m88e384fdbc\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 8 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n      </defs>\r\n      <g transform=\"translate(312.685511 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-56\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_6\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m54e1181f29\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m54e1181f29\" y=\"221.782007\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 0.0 -->\r\n      <defs>\r\n       <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 225.581226)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m54e1181f29\" y=\"188.397404\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0.1 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 192.196623)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-49\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m54e1181f29\" y=\"155.012801\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0.2 -->\r\n      <g transform=\"translate(7.2 158.81202)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m54e1181f29\" y=\"121.628198\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 0.3 -->\r\n      <defs>\r\n       <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 125.427417)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-51\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m54e1181f29\" y=\"88.243595\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 0.4 -->\r\n      <g transform=\"translate(7.2 92.042814)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m54e1181f29\" y=\"54.858992\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 0.5 -->\r\n      <defs>\r\n       <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 58.658211)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m54e1181f29\" y=\"21.474389\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 0.6 -->\r\n      <g transform=\"translate(7.2 25.273608)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_13\">\r\n    <path clip-path=\"url(#p6b553cf80d)\" d=\"M 45.321307 17.083636 \r\nL 79.139489 118.74462 \r\nL 112.95767 150.673095 \r\nL 146.775852 165.848751 \r\nL 180.594034 175.999214 \r\nL 214.412216 182.80755 \r\nL 248.230398 188.605861 \r\nL 282.04858 192.838476 \r\nL 315.866761 195.303592 \r\nL 349.684943 198.086769 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_14\">\r\n    <path clip-path=\"url(#p6b553cf80d)\" d=\"M 45.321307 144.124308 \r\nL 79.139489 178.513802 \r\nL 112.95767 195.460742 \r\nL 146.775852 202.8716 \r\nL 180.594034 207.74599 \r\nL 214.412216 210.025806 \r\nL 248.230398 211.965026 \r\nL 282.04858 213.081438 \r\nL 315.866761 213.404704 \r\nL 349.684943 214.756364 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 30.103125 224.64 \r\nL 30.103125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 364.903125 224.64 \r\nL 364.903125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 30.103125 224.64 \r\nL 364.903125 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 30.103125 7.2 \r\nL 364.903125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"legend_1\">\r\n    <g id=\"patch_7\">\r\n     <path d=\"M 286.7625 44.834375 \r\nL 357.903125 44.834375 \r\nQ 359.903125 44.834375 359.903125 42.834375 \r\nL 359.903125 14.2 \r\nQ 359.903125 12.2 357.903125 12.2 \r\nL 286.7625 12.2 \r\nQ 284.7625 12.2 284.7625 14.2 \r\nL 284.7625 42.834375 \r\nQ 284.7625 44.834375 286.7625 44.834375 \r\nz\r\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\r\n    </g>\r\n    <g id=\"line2d_15\">\r\n     <path d=\"M 288.7625 20.298437 \r\nL 308.7625 20.298437 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_16\"/>\r\n    <g id=\"text_13\">\r\n     <!-- loss -->\r\n     <defs>\r\n      <path d=\"M 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\n\" id=\"DejaVuSans-108\"/>\r\n      <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n      <path d=\"M 44.28125 53.078125 \r\nL 44.28125 44.578125 \r\nQ 40.484375 46.53125 36.375 47.5 \r\nQ 32.28125 48.484375 27.875 48.484375 \r\nQ 21.1875 48.484375 17.84375 46.4375 \r\nQ 14.5 44.390625 14.5 40.28125 \r\nQ 14.5 37.15625 16.890625 35.375 \r\nQ 19.28125 33.59375 26.515625 31.984375 \r\nL 29.59375 31.296875 \r\nQ 39.15625 29.25 43.1875 25.515625 \r\nQ 47.21875 21.78125 47.21875 15.09375 \r\nQ 47.21875 7.46875 41.1875 3.015625 \r\nQ 35.15625 -1.421875 24.609375 -1.421875 \r\nQ 20.21875 -1.421875 15.453125 -0.5625 \r\nQ 10.6875 0.296875 5.421875 2 \r\nL 5.421875 11.28125 \r\nQ 10.40625 8.6875 15.234375 7.390625 \r\nQ 20.0625 6.109375 24.8125 6.109375 \r\nQ 31.15625 6.109375 34.5625 8.28125 \r\nQ 37.984375 10.453125 37.984375 14.40625 \r\nQ 37.984375 18.0625 35.515625 20.015625 \r\nQ 33.0625 21.96875 24.703125 23.78125 \r\nL 21.578125 24.515625 \r\nQ 13.234375 26.265625 9.515625 29.90625 \r\nQ 5.8125 33.546875 5.8125 39.890625 \r\nQ 5.8125 47.609375 11.28125 51.796875 \r\nQ 16.75 56 26.8125 56 \r\nQ 31.78125 56 36.171875 55.265625 \r\nQ 40.578125 54.546875 44.28125 53.078125 \r\nz\r\n\" id=\"DejaVuSans-115\"/>\r\n     </defs>\r\n     <g transform=\"translate(316.7625 23.798437)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"line2d_17\">\r\n     <path d=\"M 288.7625 34.976562 \r\nL 308.7625 34.976562 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_18\"/>\r\n    <g id=\"text_14\">\r\n     <!-- val_loss -->\r\n     <defs>\r\n      <path d=\"M 2.984375 54.6875 \r\nL 12.5 54.6875 \r\nL 29.59375 8.796875 \r\nL 46.6875 54.6875 \r\nL 56.203125 54.6875 \r\nL 35.6875 0 \r\nL 23.484375 0 \r\nz\r\n\" id=\"DejaVuSans-118\"/>\r\n      <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n      <path d=\"M 50.984375 -16.609375 \r\nL 50.984375 -23.578125 \r\nL -0.984375 -23.578125 \r\nL -0.984375 -16.609375 \r\nz\r\n\" id=\"DejaVuSans-95\"/>\r\n     </defs>\r\n     <g transform=\"translate(316.7625 38.476562)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-118\"/>\r\n      <use x=\"59.179688\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"120.458984\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"148.242188\" xlink:href=\"#DejaVuSans-95\"/>\r\n      <use x=\"198.242188\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"226.025391\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"287.207031\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"339.306641\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p6b553cf80d\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"30.103125\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "plt.plot(train_history, label=\"loss\")\n",
    "plt.plot(val_history, label=\"val_loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "(150, 4)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "(150,)"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(96, 4) (96,)\n(24, 4) (24,)\n(30, 4) (30,)\n"
    }
   ],
   "source": [
    "# 【問題4】Iris（多値分類）をKerasで学習\n",
    "# 抽出\n",
    "X = np.array(df_iris.iloc[:, 1:5])\n",
    "y = np.array(df_iris.iloc[:, 5])\n",
    "y[y=='Iris-versicolor'] = 0\n",
    "y[y=='Iris-virginica'] = 1\n",
    "y[y=='Iris-setosa'] = 2\n",
    "y = y.astype(np.int)\n",
    "display(X.shape)\n",
    "display(y.shape)\n",
    "\n",
    "# 分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "# 標準化\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "NNfor4(\n  (layer): Sequential(\n    (0): Linear(in_features=4, out_features=100, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=100, out_features=10, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=10, out_features=3, bias=True)\n  )\n)\n"
    }
   ],
   "source": [
    "# PyTorchでモデル構築\n",
    "n_features = 4\n",
    "n_nodes_1 = 100\n",
    "activation_1 = nn.ReLU()\n",
    "n_nodes_2 = 10\n",
    "activation_2 = nn.ReLU()\n",
    "n_output = 3\n",
    "\n",
    "## ネットワーク\n",
    "class NNfor4(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(NNfor4, self).__init__()\n",
    "        self.layer = nn.Sequential(nn.Linear(n_features, n_nodes_1), activation_1,\n",
    "                                   nn.Linear(n_nodes_1, n_nodes_2), activation_2,\n",
    "                                   nn.Linear(n_nodes_2, n_output))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.layer(X)\n",
    "\n",
    "        return X\n",
    "\n",
    "## インスタンス作成\n",
    "model = NNfor4()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ミニバッチ\n",
    "batch_size = 1\n",
    "\n",
    "## numpy配列をテンソルに変換\n",
    "t_X_train = torch.from_numpy(X_train).float()\n",
    "t_y_train = torch.from_numpy(y_train).long()\n",
    "t_X_valid = torch.from_numpy(X_val).float()\n",
    "t_y_valid = torch.from_numpy(y_val).long()\n",
    "\n",
    "## 特徴量とラベルをデータセットにまとめる\n",
    "dataset_train = TensorDataset(t_X_train, t_y_train)\n",
    "dataset_valid = TensorDataset(t_X_valid, t_y_valid)\n",
    "\n",
    "# データローダを作成\n",
    "loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "loader_valid = DataLoader(dataset_valid, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "## オプティマイザ\n",
    "lr = 0.003\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=lr\n",
    ")\n",
    "\n",
    "## 損失関数\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 学習(train)\n",
    "def train_step(X_train, y_train):\n",
    "    ## 訓練モード    \n",
    "    model.train()\n",
    "\n",
    "    ## 訓練データに対して予測\n",
    "    y_train_pred = model(X_train)\n",
    "\n",
    "    ## 勾配初期化\n",
    "    optimizer.zero_grad()\n",
    "    ## 損失の取得\n",
    "    loss = criterion(y_train_pred, y_train)\n",
    "    ## 勾配を計算\n",
    "    loss.backward()\n",
    "\n",
    "    ## パラメータ更新\n",
    "    optimizer.step()\n",
    "\n",
    "    ## 正解率の確認\n",
    "    with torch.no_grad():\n",
    "        y_train_pred_num = torch.argmax(y_train_pred, axis=1)\n",
    "        acc = (y_train_pred_num == y_train).sum()\n",
    "\n",
    "    return (loss.item(), acc.item())\n",
    "\n",
    "## 評価(validation)\n",
    "def valid_step(X_val, y_val):\n",
    "    ## 評価モード\n",
    "    model.eval()\n",
    "\n",
    "    ## 予測\n",
    "    y_val_pred = model(X_val)\n",
    "\n",
    "    ## 損失計算\n",
    "    loss = criterion(y_val_pred, y_val)\n",
    "\n",
    "    ## 正解率計算\n",
    "    with torch.no_grad():\n",
    "        y_val_pred_num = torch.argmax(y_val_pred, axis=1)\n",
    "        acc = (y_val_pred_num == y_val).sum()\n",
    "\n",
    "    return (loss.item(), acc.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch1/10: [loss:0.4220, acc:0.8333, val_loss:0.4692, val_acc:0.7083]\nepoch2/10: [loss:0.1982, acc:0.9271, val_loss:0.3541, val_acc:0.7917]\nepoch3/10: [loss:0.1977, acc:0.9375, val_loss:0.3702, val_acc:0.7917]\nepoch4/10: [loss:0.1370, acc:0.9479, val_loss:0.3252, val_acc:0.7917]\nepoch5/10: [loss:0.1399, acc:0.9583, val_loss:0.2980, val_acc:0.8333]\nepoch6/10: [loss:0.0975, acc:0.9479, val_loss:0.3033, val_acc:0.8750]\nepoch7/10: [loss:0.0781, acc:0.9896, val_loss:0.2751, val_acc:0.9167]\nepoch8/10: [loss:0.0720, acc:0.9688, val_loss:0.3611, val_acc:0.8333]\nepoch9/10: [loss:0.0664, acc:0.9792, val_loss:0.3129, val_acc:0.8750]\nepoch10/10: [loss:0.0540, acc:0.9896, val_loss:0.2127, val_acc:0.9167]\nDone!\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "OrderedDict([('layer.0.weight',\n              tensor([[ 1.0737e+00,  3.2816e-01,  1.3993e+00,  3.1433e-01],\n                      [ 5.1457e-02, -2.1611e-01, -7.8271e-01,  3.1715e-01],\n                      [-9.3054e-01,  2.1747e-01, -4.1210e-01,  8.2064e-01],\n                      [-9.5670e-01,  4.0058e-02,  2.5462e-01, -4.8524e-01],\n                      [-3.1320e-01, -1.2357e-01, -5.6607e-01, -8.4612e-01],\n                      [ 7.6623e-01,  4.3919e-01, -1.9775e-01, -1.4759e-01],\n                      [-5.0564e-01,  7.9602e-01, -9.8386e-01,  9.4235e-01],\n                      [-6.1817e-01,  2.8316e-01,  6.6848e-01, -1.1235e+00],\n                      [-4.4396e-02, -2.8688e-01, -9.5955e-01, -2.1558e-01],\n                      [-1.2544e+00, -6.5028e-01, -6.0540e-02, -2.0199e-01],\n                      [ 3.1921e-01, -3.4090e-01, -2.4880e-01, -4.0654e-01],\n                      [-4.7069e-01, -5.6203e-01, -6.1135e-02,  1.1754e+00],\n                      [ 1.7280e+00,  9.0811e-01, -2.6736e-01,  5.8076e-01],\n                      [ 1.4320e-01, -6.2474e-01,  7.5240e-02,  2.3539e-02],\n                      [-3.9959e-01,  8.9541e-02, -2.6662e-01,  1.6950e+00],\n                      [ 4.7023e-02,  2.9369e-01,  3.5460e-01,  3.8992e-01],\n                      [ 7.5135e-01,  4.6553e-01,  5.0364e-01, -2.4428e-01],\n                      [-8.3911e-01,  9.4292e-01, -1.2339e-01,  2.7269e-01],\n                      [-1.2606e+00,  2.3134e-01,  2.0209e-01,  9.6047e-02],\n                      [ 8.8759e-01, -3.8413e-01, -1.2421e+00,  5.4144e-01],\n                      [-4.8029e-02,  1.4555e-03, -8.9473e-01,  8.4558e-02],\n                      [-2.4752e-01, -1.2996e-01,  1.5335e+00,  6.1230e-01],\n                      [ 1.9185e+00,  3.1651e-01,  5.7782e-01, -5.8285e-01],\n                      [-1.3414e-01, -4.9105e-01,  1.3982e-01,  1.8514e-02],\n                      [-3.1326e-01, -8.0022e-01,  1.3639e+00, -4.6727e-01],\n                      [ 1.9541e-01,  4.5479e-02, -1.0586e+00,  3.1788e-01],\n                      [ 9.1488e-01, -1.1127e+00,  2.8708e-01, -1.1098e+00],\n                      [-4.4048e-01,  4.8154e-01, -6.2703e-02, -8.0842e-01],\n                      [ 3.9072e-01, -3.1861e-01,  1.0831e-01,  5.6643e-01],\n                      [-2.7621e-01, -1.1256e+00, -1.8190e-01, -4.1833e-01],\n                      [-7.4823e-01,  4.6426e-01,  4.8439e-01,  1.5389e-02],\n                      [-1.1535e+00, -1.7319e+00, -7.7466e-02,  1.4142e+00],\n                      [-1.7016e-01,  1.3008e-01,  2.5685e-01, -3.2106e-01],\n                      [ 2.9845e-01, -2.0835e-01,  5.7738e-01,  2.9601e-01],\n                      [-1.8531e-01, -7.3945e-02,  2.1441e-01,  1.2416e+00],\n                      [ 1.3716e+00,  3.5888e-01, -9.4623e-01, -1.5031e-01],\n                      [-3.5510e-01,  2.3371e-01, -7.6951e-01,  2.2166e-01],\n                      [ 5.6217e-01,  2.1991e-01, -3.8736e-01,  3.0739e-01],\n                      [ 1.4488e-01,  7.2112e-02,  4.6728e-01,  6.5820e-01],\n                      [ 1.8335e-01, -1.9801e-01,  8.1390e-01, -5.3649e-01],\n                      [-9.5255e-01,  8.3869e-01, -6.0321e-01,  1.9410e-01],\n                      [-1.2380e+00, -2.3950e-01, -4.6437e-01,  6.3689e-01],\n                      [ 1.2785e-01,  6.5880e-01,  1.2595e-01,  1.4703e+00],\n                      [ 1.1042e+00,  4.4450e-01, -3.2983e-01, -2.9986e-01],\n                      [ 5.1060e-01, -5.5895e-01,  1.1118e-01, -7.0353e-01],\n                      [ 4.9780e-01,  1.1057e-01, -4.8631e-02,  6.1041e-01],\n                      [ 6.4806e-01, -1.6560e+00,  3.6425e-01,  2.6407e-01],\n                      [ 6.5077e-02,  3.1884e-02,  1.1325e-01, -2.2067e-01],\n                      [ 6.2299e-01,  5.5031e-01, -1.8320e+00, -2.6104e-01],\n                      [-9.8073e-01, -9.3236e-02, -8.7546e-01, -1.7009e+00],\n                      [-6.4159e-01,  5.1205e-01, -7.7049e-02,  5.6874e-01],\n                      [-5.1662e-01,  5.0282e-01,  3.6102e-01, -6.4333e-01],\n                      [ 5.1975e-01,  9.9278e-01,  1.3316e-01,  1.1079e-01],\n                      [-5.0348e-01,  7.5840e-01, -1.6391e+00,  2.3760e-01],\n                      [ 1.6636e-01,  1.7716e+00, -5.9009e-01,  1.1734e-01],\n                      [ 1.8037e-01, -1.3330e+00, -4.8762e-01,  1.4643e+00],\n                      [-3.6702e-01, -1.5559e+00, -6.8614e-01,  3.8679e-01],\n                      [ 8.0522e-01,  5.4921e-01, -3.8141e-02,  8.5357e-01],\n                      [ 7.9608e-01,  2.4169e-01,  4.5804e-01,  1.0754e+00],\n                      [-8.2096e-01,  1.0554e-01,  3.5422e-01, -7.3377e-01],\n                      [-7.0358e-02,  1.2232e-01,  8.2059e-01,  3.4379e-01],\n                      [ 2.8437e-01,  7.4634e-02,  2.2603e-01, -1.2692e+00],\n                      [ 2.4409e-01,  4.3146e-01, -1.1327e+00,  7.2724e-01],\n                      [ 7.5951e-01, -8.6551e-01, -2.9019e-01, -7.4351e-01],\n                      [ 5.2520e-01, -5.1731e-01,  4.6427e-03,  2.1105e+00],\n                      [-1.3116e-01, -1.3391e-01,  1.1800e+00,  6.0418e-01],\n                      [-1.2088e+00,  7.0989e-01, -1.1110e+00, -3.9832e-01],\n                      [ 7.0985e-01,  4.1367e-02,  5.4629e-01,  7.9620e-01],\n                      [ 2.7853e-01, -4.1216e-01,  9.9874e-02, -3.1825e-01],\n                      [-2.5376e-01, -1.1306e+00,  1.2560e+00,  1.1320e+00],\n                      [-8.0369e-02,  7.6128e-01, -9.6088e-01, -5.6634e-01],\n                      [-6.3921e-01,  1.0210e+00, -4.6065e-01,  2.0515e+00],\n                      [-9.4779e-01, -4.0946e-01, -6.7458e-01, -2.4430e-01],\n                      [ 7.7154e-01, -2.1307e-01, -5.8019e-01, -8.0458e-01],\n                      [-3.2463e-01, -5.3728e-01,  1.1907e+00, -2.8250e-01],\n                      [-2.4058e-01,  8.9390e-01,  4.3806e-01, -2.3583e-02],\n                      [ 3.6629e-01, -6.5035e-01, -1.2952e-01,  9.7123e-01],\n                      [ 3.7137e-01, -6.8547e-02,  3.8613e-01,  3.1800e-01],\n                      [ 9.2443e-01, -1.6258e+00, -2.1133e-02,  6.4698e-01],\n                      [ 5.5232e-01,  5.7348e-01,  4.8062e-01,  3.5432e-01],\n                      [ 9.0653e-01,  9.0814e-02, -4.2296e-01,  4.9883e-01],\n                      [-4.3133e-01,  8.3032e-01,  5.7301e-01,  4.6355e-01],\n                      [ 5.8656e-01, -8.7597e-02, -5.1382e-01, -1.2633e-01],\n                      [-6.1963e-01,  2.8557e-01, -1.0347e+00, -2.0882e-01],\n                      [-5.9275e-01, -1.2811e+00,  6.0221e-01, -9.5090e-01],\n                      [ 1.6357e-01, -7.9296e-01, -1.1207e+00,  8.9022e-02],\n                      [ 1.0377e+00,  9.5109e-01,  1.3173e+00,  5.3254e-01],\n                      [ 4.8930e-01,  4.0985e-01,  7.0023e-01, -1.2897e+00],\n                      [-6.1414e-02,  2.2749e-01, -2.8863e-01,  1.2434e+00],\n                      [ 1.7199e+00, -2.0270e-01, -1.5236e-01,  4.7811e-01],\n                      [ 1.4436e-01,  8.7465e-01, -3.5361e-01,  8.9963e-01],\n                      [-2.4810e-01, -1.0814e+00,  6.5753e-01,  1.3495e-01],\n                      [-1.4878e+00,  1.1872e-01,  3.1125e-01,  3.5679e-01],\n                      [ 2.6895e-01,  8.9311e-01,  1.8302e-01,  4.9055e-03],\n                      [ 1.4880e+00,  4.5041e-01, -2.0716e-01,  1.3466e-03],\n                      [ 5.9283e-01, -3.1898e-01, -1.0570e+00,  5.6090e-01],\n                      [ 1.2388e+00,  7.7312e-01, -2.1844e-01, -9.2291e-01],\n                      [ 3.6673e-01,  5.0845e-01,  2.2466e-01, -6.3720e-01],\n                      [-6.0298e-01,  2.2321e-01,  8.9312e-02,  6.8061e-01],\n                      [-8.0032e-01, -3.0184e-01,  4.8534e-01, -1.0371e+00]])),\n             ('layer.0.bias',\n              tensor([-0.1977, -0.0269, -0.1112,  0.2048,  0.0286,  0.1715,  0.2019,  0.0985,\n                       0.1888,  0.0913,  0.3312, -0.2114,  0.1487,  0.1829, -0.0759, -0.1428,\n                      -0.0836, -0.0260,  0.2314,  0.1088, -0.1180, -0.2527,  0.1250,  0.2338,\n                      -0.2528, -0.0800,  0.3227, -0.0667, -0.1738,  0.0969,  0.1622,  0.1542,\n                       0.5016,  0.1725, -0.2023, -0.1225,  0.0080,  0.0793, -0.1394,  0.0197,\n                      -0.0010,  0.1175, -0.1235,  0.1146,  0.3321,  0.1149, -0.0296, -0.1261,\n                       0.0957,  0.2293, -0.0823,  0.4924, -0.1364, -0.0696,  0.0084, -0.2210,\n                       0.1619, -0.1184, -0.1369, -0.2156, -0.1409, -0.0047, -0.0014,  0.3254,\n                       0.1294, -0.2502,  0.1499, -0.1333,  0.0762, -0.2090,  0.1437, -0.1481,\n                       0.1526,  0.2826, -0.1627, -0.1022, -0.1654, -0.1271,  0.1554, -0.2110,\n                       0.1372,  0.3036,  0.1960,  0.1599, -0.1584, -0.0841, -0.0161, -0.1810,\n                      -0.1598,  0.1739,  0.0968, -0.3569, -0.0484,  0.0967, -0.1629,  0.1453,\n                       0.2940,  0.2015,  0.2601, -0.1871])),\n             ('layer.2.weight',\n              tensor([[-9.8224e-02, -7.4586e-02, -1.7082e-01,  9.8913e-02, -7.9380e-03,\n                        1.4854e-01,  1.4441e-01,  2.2788e-02, -1.4193e-01,  2.8717e-02,\n                       -1.6416e-01, -2.2896e-01, -1.8093e-01, -1.9170e-01,  8.8762e-02,\n                        1.3665e-01, -3.5837e-02,  5.9547e-02, -1.2933e-01,  1.4183e-01,\n                        1.2364e-01,  5.4303e-02, -4.4833e-02, -1.2378e-01,  4.6024e-02,\n                        8.4425e-03, -1.2865e-01,  7.4320e-02, -2.7043e-02,  1.8803e-02,\n                        6.5229e-02, -2.3451e-01,  1.0175e-01,  3.7337e-03, -1.8129e-01,\n                        3.2933e-02, -9.0617e-03, -7.6152e-02, -2.3792e-01, -8.1932e-03,\n                       -1.7977e-02, -7.1518e-02,  1.5406e-01,  5.1207e-02,  2.8214e-02,\n                       -1.5866e-01,  9.8089e-02,  2.3390e-01,  6.8941e-02, -1.8699e-01,\n                       -9.9971e-02,  6.0192e-02,  5.9622e-02, -2.5837e-01, -1.2951e-01,\n                       -8.7542e-02,  3.7319e-02, -5.5160e-02, -2.8364e-02, -7.0644e-02,\n                        7.7933e-02,  1.1814e-01, -2.8475e-01, -5.2028e-02, -2.8211e-01,\n                        2.1482e-01,  1.8668e-01, -1.2339e-01, -4.2692e-02,  1.2982e-01,\n                       -2.1332e-01, -8.1872e-02,  2.0662e-01, -2.7686e-01,  2.3122e-01,\n                        1.0810e-02,  4.0499e-01,  6.0571e-02, -4.4037e-02, -1.0646e-02,\n                       -1.2878e-01, -9.1147e-03, -7.0818e-02,  1.9411e-03,  5.6143e-02,\n                       -2.3844e-01, -1.0933e-01,  9.3470e-02,  1.0308e-01,  1.1963e-01,\n                       -5.3959e-02, -8.5608e-02, -1.2519e-01, -1.6534e-01, -2.8097e-02,\n                        5.9600e-02,  1.2866e-01, -2.0793e-01,  4.4937e-02, -9.5097e-02],\n                      [ 1.2643e-01, -2.7486e-01,  3.0592e-01,  1.2810e-01, -5.0835e-02,\n                        1.0382e-01, -7.6354e-02, -1.3081e-01,  2.6244e-02,  2.2772e-01,\n                        1.0712e-01,  2.5884e-01, -4.5134e-02, -1.1122e-01,  3.0050e-01,\n                        2.3688e-01,  2.6064e-01, -3.4477e-01, -7.3778e-02, -2.0006e-01,\n                       -6.8991e-02,  2.2319e-01,  4.2672e-02,  9.8415e-02,  1.6050e-01,\n                       -4.0037e-01, -1.9001e-01,  3.2780e-01,  5.7348e-02, -2.6142e-02,\n                       -5.6877e-02,  2.0172e-01, -2.7382e-01,  3.0593e-01,  1.0830e-01,\n                        5.7260e-03,  8.0294e-02, -1.1085e-01,  1.9375e-01, -9.1988e-03,\n                       -1.8856e-01,  4.4624e-02,  3.0063e-01, -5.8669e-02,  6.2222e-02,\n                        7.7724e-02,  1.3117e-01,  3.4088e-03, -3.9504e-02, -1.4381e-01,\n                       -9.2496e-02, -5.5014e-01, -1.1928e-01, -1.9692e-01,  1.2130e-01,\n                        1.1566e-01,  3.4871e-02,  3.2717e-02,  9.9448e-02,  6.7424e-02,\n                        2.3302e-01, -2.0174e-01,  3.0335e-01, -5.6975e-02, -1.6491e-02,\n                        3.2884e-01, -8.5089e-02, -4.9421e-02, -2.1418e-02,  3.3054e-01,\n                       -9.0111e-02,  7.4031e-02,  1.2108e-01, -3.9754e-01,  2.7687e-01,\n                       -6.8710e-03,  2.8559e-01, -4.7473e-03,  1.9466e-02, -8.3899e-02,\n                        1.0095e-01,  2.2328e-01, -3.5025e-01,  1.6334e-02,  1.1020e-01,\n                       -3.4407e-02,  2.0278e-01,  4.4624e-01,  1.7205e-01,  3.0305e-02,\n                       -1.7005e-01, -9.5500e-02, -4.7838e-02, -1.6934e-01,  5.7658e-02,\n                       -3.4090e-01,  4.6857e-02, -8.9570e-02,  2.0416e-01,  3.7522e-02],\n                      [-3.7510e-02,  6.7192e-02,  2.0714e-01,  3.8004e-01,  1.8016e-01,\n                        5.3071e-02,  7.7660e-02,  1.3273e-01,  2.6390e-01,  3.6739e-02,\n                        3.5675e-01, -1.0002e-01,  1.3550e-01,  6.9643e-02, -6.6661e-02,\n                       -2.8691e-02, -9.0191e-02,  3.5754e-01,  3.7254e-01,  2.0169e-01,\n                        2.3724e-01, -3.8264e-01, -1.0121e-01,  3.3716e-02,  3.0136e-03,\n                        3.4864e-01,  2.2422e-01, -1.3468e-01, -3.7757e-02,  6.9048e-02,\n                        4.6931e-02, -8.9450e-02,  2.2732e-01,  1.8781e-01, -3.0987e-01,\n                        3.6516e-02,  3.5346e-01, -7.4005e-03, -1.0490e-01,  2.1899e-01,\n                       -2.4347e-01,  9.3548e-02, -3.2132e-02,  1.1814e-01,  3.1762e-01,\n                        2.1364e-01, -2.3197e-02, -1.0947e-02, -6.7353e-02,  2.2182e-01,\n                        8.3450e-03,  5.3887e-01,  9.1436e-03,  6.1321e-02,  2.2825e-01,\n                       -2.0918e-03,  2.1448e-02, -6.1758e-02, -1.8823e-01, -1.0054e-01,\n                       -1.6139e-01, -4.8450e-03, -9.3416e-02,  1.4632e-01,  1.3827e-01,\n                       -1.0868e-01,  2.3747e-01,  2.1122e-02,  3.5020e-01, -2.8134e-01,\n                       -6.6071e-03, -2.3601e-01,  3.4675e-01,  3.1806e-01, -2.4895e-01,\n                        3.6754e-02,  1.5299e-01,  1.5664e-01,  5.6438e-03, -1.1858e-01,\n                       -1.0344e-01,  9.1672e-02,  1.6263e-01,  2.3419e-01,  1.4553e-01,\n                        9.4468e-02, -2.2068e-02, -1.4159e-01, -1.9904e-02,  2.6884e-01,\n                       -7.1058e-02, -7.9595e-02, -1.6181e-01,  2.5669e-01, -8.7510e-02,\n                        2.7139e-01, -8.9376e-02,  9.2322e-02,  1.6156e-01, -9.7974e-02],\n                      [-2.9452e-01,  8.8471e-02,  1.4824e-01,  1.4515e-01,  2.3314e-01,\n                        8.3839e-02, -1.1737e-01,  6.5731e-02,  7.2293e-02,  1.9331e-02,\n                       -1.5044e-01,  4.0481e-02, -6.1109e-02, -6.0910e-03, -2.8105e-01,\n                        3.3061e-02,  1.5000e-01,  7.7271e-02,  7.4959e-02, -1.1307e-01,\n                       -1.1852e-01, -7.2740e-02,  2.1762e-01, -7.3461e-03, -5.2636e-02,\n                       -7.1320e-03,  1.5355e-01,  3.5558e-01, -2.9532e-01, -6.1899e-02,\n                       -2.1510e-01, -5.7018e-02,  9.8869e-02, -2.1206e-01, -3.7143e-01,\n                        2.0016e-01,  1.0822e-01, -3.0473e-02, -6.2935e-02, -4.0528e-01,\n                        1.7702e-01,  3.7761e-01, -2.2558e-01,  1.6362e-01,  1.2080e-01,\n                       -3.1367e-01, -1.5066e-01, -9.6780e-02, -6.6388e-02,  2.1803e-01,\n                       -1.7980e-02,  4.5584e-02,  1.9165e-01,  3.5924e-01,  5.0285e-02,\n                       -2.4465e-01, -1.2535e-01,  2.3433e-01,  5.9061e-02,  2.0849e-01,\n                       -1.0748e-01,  2.5719e-01,  2.6259e-02,  5.2845e-02, -4.9157e-01,\n                       -3.3364e-02,  1.1445e-01,  9.6540e-02, -7.3634e-02, -3.0939e-01,\n                        2.4243e-02, -2.4756e-01,  5.0653e-01,  1.3746e-01, -3.0453e-02,\n                        1.1680e-01, -5.8667e-02,  1.5190e-01, -1.9091e-01, -9.0984e-02,\n                        3.7196e-01, -1.4705e-01,  1.6781e-01, -9.7498e-02,  2.9369e-02,\n                        1.7519e-01, -2.5553e-01, -7.7991e-02,  1.4148e-01, -1.1987e-01,\n                        6.1188e-02, -3.2958e-01, -9.2265e-02,  2.0826e-01, -6.6608e-02,\n                        9.7843e-02,  2.3546e-02, -3.8662e-02, -2.4248e-01, -8.4746e-02],\n                      [-1.6991e-01,  3.2931e-01, -1.9900e-03,  1.3217e-01, -3.5498e-01,\n                        9.9825e-02,  2.5879e-01, -6.6562e-02, -2.5961e-01,  1.3655e-01,\n                       -1.7107e-01, -5.7598e-03, -2.5278e-01,  3.1768e-02,  5.5215e-03,\n                        2.7446e-02, -9.8042e-02, -3.8650e-02,  1.8073e-01,  2.7054e-02,\n                       -5.8285e-02, -1.3179e-01,  1.1388e-01,  4.8618e-02,  2.5493e-02,\n                        2.7964e-01, -5.1906e-02, -1.0910e-01,  1.3630e-02,  1.1046e-01,\n                        1.3160e-02,  2.0090e-01,  3.9801e-01, -1.4134e-01, -8.7800e-02,\n                       -5.4624e-02,  1.1415e-01, -3.2433e-01,  3.5924e-02,  2.9982e-02,\n                       -1.0660e-02, -7.5643e-02,  5.2777e-02, -3.9568e-02, -9.6582e-02,\n                        5.3162e-02, -3.2421e-01, -9.1934e-02, -2.0204e-01,  9.3976e-02,\n                        1.3328e-02,  4.0233e-01, -3.6421e-02, -2.1370e-01,  4.1077e-02,\n                        1.8980e-01,  1.4551e-01, -1.7203e-01,  1.2542e-01, -5.7972e-02,\n                        2.3449e-01, -2.1871e-01, -1.5198e-01,  1.1905e-01, -5.4988e-02,\n                        1.9235e-01,  1.2806e-01, -8.4000e-02, -4.8866e-02,  5.8809e-02,\n                       -1.6268e-01,  8.7152e-02, -6.2066e-02, -3.1320e-01,  6.0710e-02,\n                        8.5256e-02, -3.8484e-01, -1.0612e-01, -2.1746e-01, -2.7511e-01,\n                        1.3650e-01,  4.5235e-01,  5.9598e-02, -6.8507e-02, -1.1712e-04,\n                       -2.1919e-01,  9.1546e-02, -1.1669e-01, -2.2832e-01,  1.0209e-01,\n                        5.3565e-02, -9.1038e-02,  7.1134e-02, -3.5578e-01,  5.7336e-02,\n                        4.0966e-02,  2.7767e-01,  3.1707e-01,  2.3471e-01, -1.6714e-01],\n                      [ 8.2408e-02,  1.6518e-02,  1.6871e-01,  6.4950e-02,  1.7196e-01,\n                       -3.0396e-01,  2.6287e-01, -6.1001e-02, -1.1984e-02,  1.3143e-01,\n                        1.7324e-01, -4.5686e-01,  1.6675e-01,  3.4173e-02, -1.4879e-01,\n                        2.9737e-01,  8.1329e-02,  4.2126e-02, -2.3803e-03, -1.5886e-01,\n                        8.6089e-02, -1.4892e-01, -1.6319e-01, -5.9637e-02, -2.8755e-02,\n                        2.0621e-01, -4.8663e-02, -7.6177e-02, -1.1679e-01, -5.3323e-02,\n                       -1.8181e-01, -2.4179e-02, -7.6385e-02,  6.6492e-02, -1.2676e-01,\n                        1.5048e-02,  2.1848e-01, -8.1854e-02, -8.1092e-02, -7.4000e-02,\n                        5.5828e-02, -7.7685e-02, -1.2075e-01, -6.4455e-02, -1.4238e-01,\n                       -1.8846e-01, -2.1345e-01,  5.0093e-02,  3.6912e-01,  2.7886e-01,\n                        4.9431e-02,  2.0893e-01, -9.8509e-02,  1.6102e-02,  3.9842e-02,\n                       -3.1439e-02,  1.6476e-01, -2.6872e-01,  5.1102e-03,  1.9734e-01,\n                       -1.7906e-01,  4.3215e-02, -1.6163e-01,  8.1358e-03,  4.1474e-02,\n                        1.9438e-01,  1.5654e-01, -1.5838e-01, -5.5080e-02, -2.5271e-02,\n                        8.6001e-02, -3.5047e-02, -1.1322e-01,  2.3776e-01,  2.8274e-02,\n                       -7.4142e-02, -1.6671e-01, -7.1939e-02, -3.8351e-02,  5.4226e-02,\n                        1.3344e-01, -7.9360e-02,  4.7231e-02,  2.8129e-01, -7.5549e-02,\n                        9.9797e-02, -1.2036e-03,  4.4002e-02, -3.4357e-02, -9.7194e-02,\n                        1.7391e-01, -1.1990e-01,  4.9857e-02,  1.2293e-01,  1.0337e-01,\n                       -1.7005e-02, -1.4509e-02, -8.5131e-02, -2.5227e-01, -1.3228e-01],\n                      [-1.1758e-02, -8.0743e-03,  1.1958e-02, -2.1478e-01, -8.3118e-02,\n                       -2.1228e-01, -2.5210e-02,  1.1208e-01,  7.4574e-03, -2.1339e-01,\n                        1.0517e-01,  3.4820e-02, -3.5830e-02,  2.7086e-02,  1.1851e-01,\n                       -1.2196e-01,  6.1117e-02, -2.7772e-01, -9.5566e-02, -1.5257e-01,\n                        3.0010e-02, -6.5617e-02, -1.8785e-02, -4.6290e-02, -8.7168e-02,\n                        1.0679e-01, -5.2242e-02,  1.0092e-01, -6.2423e-02,  7.4984e-02,\n                        3.0043e-02, -8.4796e-03, -2.5195e-01, -1.3823e-01, -1.8598e-01,\n                        1.1063e-01,  2.2220e-01, -1.2016e-01, -9.3779e-03,  1.7115e-01,\n                        2.5895e-02, -2.5657e-02, -2.6867e-01,  1.8862e-02, -2.2315e-01,\n                        2.3115e-02,  6.0823e-02,  5.0376e-02,  1.9861e-01,  1.9757e-01,\n                       -1.4130e-02, -6.2927e-02, -2.5675e-01,  1.8205e-01, -8.0386e-02,\n                       -2.0685e-01, -1.1817e-01, -4.5051e-02, -6.9960e-03, -7.3051e-02,\n                       -6.3354e-02,  3.6053e-01, -1.4247e-01,  3.2044e-02,  1.2820e-01,\n                       -1.1398e-01, -3.6083e-01, -2.2459e-01, -6.6851e-02,  1.1873e-01,\n                        3.2645e-02,  1.2314e-01, -4.5785e-02,  5.6155e-02,  1.7142e-01,\n                       -1.8857e-03,  1.7697e-01,  6.6708e-02, -4.1822e-02, -2.5728e-03,\n                       -1.4452e-01, -2.0847e-01,  1.5201e-01, -8.7113e-02, -5.6378e-02,\n                       -1.2520e-01, -2.0521e-01,  4.8156e-02,  8.4807e-02,  2.4999e-01,\n                       -1.9160e-01,  1.4897e-01, -2.5617e-01, -1.4005e-01, -1.1681e-01,\n                       -2.6293e-01,  7.5423e-02,  2.9500e-01,  1.5658e-01, -9.7028e-02],\n                      [ 2.0406e-01,  1.1804e-01,  6.4887e-02,  5.5274e-01,  6.4076e-02,\n                       -3.3145e-01,  3.3827e-01,  7.3247e-02,  6.5804e-02, -9.7144e-02,\n                        3.3703e-01,  1.1028e-01, -4.4084e-01,  1.9194e-01,  1.6530e-02,\n                        1.9269e-02, -5.3248e-01, -3.4935e-02,  6.9175e-02,  5.2224e-02,\n                        5.4241e-02, -1.8832e-01, -8.3348e-02,  2.5988e-01, -1.1762e-01,\n                       -3.4212e-02, -1.3591e-02, -6.7767e-02,  1.5525e-01,  1.0178e-01,\n                        1.5579e-01,  8.9269e-02,  3.1683e-01,  9.4167e-02,  1.7474e-02,\n                       -1.3385e-01,  2.4121e-02, -5.0492e-01,  2.0673e-01,  1.3338e-01,\n                        8.1311e-03, -1.3540e-01,  1.5036e-02, -1.3696e-01,  1.4950e-01,\n                       -1.4689e-01, -2.7015e-02, -1.0547e-01, -1.9928e-01, -7.7819e-02,\n                        1.2049e-01,  1.5381e-01, -5.6279e-02, -6.4329e-02, -3.7601e-02,\n                       -7.9748e-03, -7.8263e-02, -2.0393e-01, -1.9142e-01, -6.4837e-02,\n                        9.1607e-02,  8.2888e-02, -6.0896e-02,  1.7466e-01, -1.6909e-01,\n                       -7.0352e-02, -2.1711e-01, -3.1277e-01, -1.3920e-02, -1.1557e-03,\n                       -1.5732e-02,  4.3286e-02,  2.1928e-01, -5.1577e-02,  6.2371e-02,\n                        1.4339e-01, -1.2052e-01, -1.8677e-01,  2.0770e-01, -1.2995e-02,\n                        1.6234e-01, -1.5436e-02,  3.2669e-01, -1.0592e-01, -1.0045e-01,\n                       -6.0853e-03, -1.3782e-01, -1.0357e-01, -1.9253e-01, -1.2810e-01,\n                        1.3025e-01,  7.6058e-02,  8.3174e-02, -9.6250e-02,  5.7686e-02,\n                       -2.0327e-03,  1.7218e-02, -1.9789e-01,  7.0186e-03, -1.8505e-01],\n                      [-2.1930e-02, -1.5573e-01, -2.0302e-01, -1.5400e-01, -7.8294e-02,\n                        2.1835e-01,  2.5455e-01,  3.8413e-02,  3.9658e-02,  1.5119e-01,\n                        2.7577e-01, -3.6651e-01,  1.8362e-02,  1.4316e-01, -1.0717e-01,\n                       -3.2222e-01, -1.1731e-01,  4.7643e-02,  8.3960e-02,  2.6893e-01,\n                       -2.9191e-01, -1.0473e-01,  1.8680e-01,  3.0536e-01, -1.3674e-01,\n                       -2.2603e-01,  3.5233e-01, -4.3057e-01, -1.9307e-01,  9.9426e-02,\n                        2.5636e-01,  1.1814e-01,  4.4236e-01,  1.7999e-01, -1.1647e-01,\n                       -3.1761e-02, -1.7764e-01,  2.8857e-02, -1.7611e-01, -5.6934e-02,\n                       -1.3925e-02,  2.2529e-01, -1.6065e-01,  2.0172e-01,  2.8525e-01,\n                        2.0749e-01,  1.2235e-01, -5.8524e-02, -4.5007e-02,  1.4403e-01,\n                       -8.6190e-03,  4.6589e-01, -1.7470e-01, -3.2511e-01, -1.4612e-01,\n                       -2.0397e-01,  1.9012e-01, -5.6208e-02, -1.4264e-01, -3.8919e-02,\n                       -1.2868e-01, -5.7151e-02, -7.6301e-02,  1.6240e-01,  1.4376e-01,\n                       -2.0931e-01, -1.5756e-02, -3.1523e-02,  4.8138e-02, -9.5691e-02,\n                       -1.3735e-01, -2.4246e-02,  1.1835e-01,  1.1945e-01,  1.5187e-02,\n                       -3.0138e-01, -2.9214e-01, -1.1425e-01,  2.5286e-01, -2.4921e-01,\n                        3.1950e-01,  2.6719e-01,  3.3039e-01,  1.4238e-01, -1.3325e-01,\n                       -2.8540e-02,  1.6044e-01, -4.7187e-02, -2.2291e-01,  1.4142e-01,\n                        7.0679e-03, -1.7793e-01, -1.7170e-01, -5.5920e-02, -4.9641e-02,\n                        4.0759e-01,  2.7034e-01,  1.5774e-01,  2.3540e-01,  2.8385e-02],\n                      [ 9.1894e-02,  3.9643e-01, -2.1655e-01,  1.6274e-01,  1.9437e-01,\n                        6.8797e-02, -9.5933e-02,  1.3714e-01,  3.3116e-01,  1.1056e-02,\n                        1.3181e-01,  2.3981e-01, -1.9596e-01,  3.5031e-02,  3.6894e-01,\n                        1.4711e-01,  3.4391e-04,  2.4129e-02,  5.4555e-02, -1.3778e-01,\n                       -6.4774e-02, -1.3517e-01, -3.2277e-02,  8.1764e-02, -4.7956e-02,\n                       -9.0535e-02,  2.5373e-01,  9.9905e-02, -1.5890e-01, -1.5390e-01,\n                        9.6180e-02, -8.9667e-02,  2.5503e-01,  7.9534e-02,  1.0385e-01,\n                       -5.7970e-02,  1.2846e-01, -3.4382e-01, -1.7404e-01, -1.3958e-01,\n                        1.9410e-01,  2.5368e-01, -9.0243e-02,  3.1634e-02,  1.7575e-02,\n                        1.1066e-02, -8.0499e-02, -2.5612e-02,  1.6545e-01,  1.6518e-01,\n                       -5.3437e-02,  1.6966e-01,  1.7169e-01,  2.6530e-02,  4.6044e-02,\n                       -2.2224e-01,  8.5405e-02, -9.2861e-02, -5.8077e-02, -5.7685e-04,\n                       -3.3069e-02,  1.2349e-01, -1.6619e-01,  1.3675e-01, -7.0543e-02,\n                       -2.3002e-02,  3.7153e-01, -6.8221e-02,  3.0046e-01, -7.3152e-03,\n                        1.3244e-01,  2.8324e-03,  4.9042e-02,  2.1675e-01, -1.2335e-01,\n                       -2.5915e-02, -2.0376e-01, -1.6245e-01, -1.3938e-02,  1.5385e-01,\n                        3.0481e-01,  1.5920e-01, -5.8544e-02,  3.0591e-01,  1.7054e-01,\n                        2.3500e-01, -3.5820e-02,  2.1263e-01,  3.9081e-02, -2.5058e-01,\n                       -1.7524e-01, -1.6406e-01,  1.9967e-01,  2.0562e-01, -1.0649e-01,\n                        1.5200e-01,  1.9179e-01, -6.6412e-02, -1.4249e-01, -1.5342e-01]])),\n             ('layer.2.bias',\n              tensor([-0.0343, -0.1173,  0.2757, -0.0461,  0.1934, -0.0365, -0.0331,  0.1934,\n                       0.2791,  0.0852])),\n             ('layer.4.weight',\n              tensor([[-0.9689, -0.1349,  0.2729, -0.0308,  0.4683,  0.1351,  0.0566,  1.0291,\n                        0.7078,  0.4387],\n                      [-0.1764,  0.9428, -0.2147, -0.1972,  0.1552,  0.2584,  0.6551,  0.3356,\n                       -0.5010, -1.0027],\n                      [-0.4318, -1.0695,  0.3574,  0.3380, -0.4647,  0.9985, -0.3088,  0.2965,\n                       -0.6205,  0.9364]])),\n             ('layer.4.bias', tensor([ 0.2322, -0.1886, -0.2096]))])"
     },
     "metadata": {}
    }
   ],
   "source": [
    "## パラメータ初期化関数\n",
    "def initializer(layer):\n",
    "    if type(layer) == nn.Linear:\n",
    "        nn.init.kaiming_normal_(layer.weight)\n",
    "        layer.bias.data.fill_(0.0)\n",
    "\n",
    "## パラメータ初期化する\n",
    "model.apply(initializer)\n",
    "\n",
    "n_epoch = 10\n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "# 1エポック内1データあたりの損失・正解率の平均\n",
    "avg_loss = 0.0\n",
    "avg_acc = 0.0\n",
    "avg_val_loss = 0.0\n",
    "avg_val_acc = 0.0\n",
    "\n",
    "## 学習する\n",
    "for epoch in range(n_epoch):\n",
    "    ## epoch内のloss, accの保存\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    total_val_loss = 0.0\n",
    "    total_val_acc = 0.0\n",
    "    ## 累計データ数の保存\n",
    "    total_train = 0\n",
    "    total_val = 0\n",
    "\n",
    "    ## 訓練\n",
    "    for X_train, y_train in loader_train:\n",
    "       loss, acc = train_step(X_train, y_train)\n",
    "       total_loss += loss\n",
    "       total_acc += acc\n",
    "       total_train += len(y_train)\n",
    "\n",
    "    ## 評価\n",
    "    for X_val, y_val in loader_valid:\n",
    "        val_loss, val_acc = valid_step(X_val, y_val)\n",
    "        total_val_loss += val_loss\n",
    "        total_val_acc += val_acc\n",
    "        total_val += len(y_val)\n",
    "\n",
    "    # 平均計算\n",
    "    avg_loss = total_loss/total_train\n",
    "    avg_val_loss = total_val_loss/total_val\n",
    "    avg_acc = total_acc/total_train\n",
    "    avg_val_acc = total_val_acc/total_val\n",
    "\n",
    "    print(f'epoch{epoch+1}/{n_epoch}: [loss:{avg_loss:.4f}, acc:{avg_acc:.4f}, val_loss:{avg_val_loss:.4f}, val_acc:{avg_val_acc:.4f}]')\n",
    "\n",
    "    # リストに追加\n",
    "    train_history.append(avg_loss)\n",
    "    val_history.append(avg_val_loss)\n",
    "\n",
    "print('Done!')\n",
    "display(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "y_pred_proba tensor([[ -0.3904,   7.3431,  -9.0218],\n        [  7.8291,  -2.5207,  -1.4116],\n        [  7.3435, -10.6116,  20.2460],\n        [  1.6503,   3.9108,  -8.2561],\n        [  7.5264,  -9.9605,  17.7605],\n        [ -0.7256,   7.5182,  -9.0381],\n        [  7.3173,  -9.9677,  18.5435],\n        [  2.5831,  -0.5240,  -3.1900],\n        [  3.7133,  -1.2928,  -3.7870],\n        [  4.0105,  -2.1087,  -0.7720],\n        [  2.0266,   2.6975,  -5.1263],\n        [  1.9204,  -0.0820,  -2.5927],\n        [  3.4710,  -0.9106,  -1.6805],\n        [  2.7598,  -0.5410,  -3.4566],\n        [  1.9932,   0.2545,  -2.9226],\n        [  8.5846, -10.5887,  17.4398],\n        [  1.8253,   0.7625,  -3.0284],\n        [  4.7890,  -0.1446,  -2.2932],\n        [  8.4944, -10.1296,  17.0702],\n        [  6.7175,  -9.5578,  18.3562],\n        [  0.4830,   5.5593,  -6.8502],\n        [  3.1123,   0.5944,  -2.3812],\n        [  7.8721,  -9.8401,  17.1046],\n        [  9.7098, -11.4161,  18.6198],\n        [  0.5327,   2.9287,  -5.1414],\n        [  8.7194, -11.3714,  21.9752],\n        [  7.2687,  -9.5482,  16.7515],\n        [  3.0528,  -1.8304,  -1.5338],\n        [  7.9625,  -3.1809,   0.0984],\n        [  7.0609,  -9.0629,  16.6294]], grad_fn=<AddmmBackward>)\ny_pred [1 0 2 1 2 1 2 0 0 0 1 0 0 0 0 2 0 0 2 2 1 0 2 2 1 2 2 0 0 2]\ny_test [1 0 2 1 2 1 2 0 0 0 1 0 0 0 0 2 0 0 2 2 1 0 2 2 1 2 2 0 0 2]\n"
    }
   ],
   "source": [
    "# 予測\n",
    "## Tensor変換\n",
    "t_X_test = torch.from_numpy(X_test).float()\n",
    "t_y_test = torch.from_numpy(y_test).float()\n",
    "y_pred_proba = model(t_X_test)\n",
    "y_pred = torch.argmax(y_pred_proba, axis=1)\n",
    "print(\"y_pred_proba\", y_pred_proba)\n",
    "print(\"y_pred\", y_pred.flatten().int().detach().numpy())\n",
    "print(\"y_test\", y_test.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 【問題5】House PricesをKerasで学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 【問題6】MNISTをKerasで学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題8】\n",
    "#### （アドバンス課題）フレームワークの比較\n",
    "それぞれのフレームワークにはどのような違いがあるかをまとめてください。\n",
    "\n",
    "\n",
    "**《視点例》**\n",
    "\n",
    "\n",
    "- 計算速度\n",
    "- コードの行数・可読性\n",
    "- 用意されている機能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bitdicconda58dbae13a5ad45af92cdb395e5ca7493",
   "display_name": "Python 3.7.7 64-bit ('dic': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}